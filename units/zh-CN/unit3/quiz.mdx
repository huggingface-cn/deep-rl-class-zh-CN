# 测试 [[quiz]]

学习和[避免自以为是](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf)**的最好方法是对自己进行测试。**这将帮助您找到需要**加强知识的地方**。

### Q1: 我们提到的 Q-learning 是一种表格型方法，所以什么是表格型方法?

<details>
<summary>参考答案</summary>

*表格型方法*是针对状态和动作空间足够小的问题类型，去用数组或表格近似价值函数的方法。**Q-learning ** 是一个表格型方法的例子，其表被用于表示不同的状态-动作对的值。


</details>

### Q2: 为什么我们不能用经典的 Q-learning 处理雅达利游戏?

<Question
	choices={[
		{
			text: "雅达利的环境对于 Q-learning 太快了",
			explain: ""
		},
		{
			text: "雅达利环境拥有巨大的观测空间。所以创建 Q 表格会失效 ",
			explain: "",
      correct: true
		}
	]}
/>


### Q3: 当我们用帧去输入深度 Q学习时，为什么要把四帧堆叠在一起?

<details>
<summary>参考答案</summary>

我们堆叠在一起因为他帮助我们**解决了时序限制**的问题：一帧没有足够的时序信息。举个例子，在 pong 中，我们的智能体**在一帧中无法判断球的方向**。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/temporal-limitation.jpg" alt="Temporal limitation"/>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/temporal-limitation-2.jpg" alt="Temporal limitation"/>


</details>


### Q4: 深度 Q-learning 的两部分是啥?

<Question
	choices={[
		{
			text: "采样",
			explain: "我们执行动作并储存观测到的经验在重演内存中。",
      correct: true,
		},
		{
			text: "打乱",
			explain: "",
		},
    {
      text: "重新排序",
      explain: "",
    },
    {
			text: "训练",
			explain: "我们随机从元组选择一小批并用梯度下降更新每一步。",
      correct: true,
		}
	]}
/>

### Q5: 为什么我们在深度 Q-learning 中要创建回放内存?

<details>
   <summary>参考答案</summary>

1. **在训练中制造更多更有效的经验**。
通常，在线强化学习中，智能体会与环境交互，获得经验（状态，动作，奖励和下一个状态），学习这些（更新神经网络），抛弃他们这种做法并不高效。经验回放帮助训练中使用**经验更高效**。我们使用一个回放缓冲在**训练中重复使用**来节省经验采样。这使得智能体在**同样的经验中学习多次**。

2. **避免忘记之前所学并且减少经验间的相关性**.
- 现在的问题是如果我们给了一个序列的经验采样在我们的神经网络中，它会倾向于在得到**新经验后忘记之前的经验**。
例如，如果智能体在第一个关卡中，然后再第二个关卡中，这两个关卡是不同的，这就使得智能体忘记了如何在第一个关卡中表现、玩。


</details>

### Q6: 我们怎么使用双 DQN?


<details>
  <summary>参考答案</summary>
当我们计算 Q 目标时，我们使用两个网络将动作选择与目标**值生成**解耦。我们：

- 使用 **DQN 网络** 去为下一个状态挑选最佳动作（有最高 Q 值的动作）。
- 使用 **目标网络** 去计算下一个状态的目标 Q 值的采取动作


</details>


恭喜你完成测试 🥳, 如果你忘了某些部分，可以回顾相关章节去强化一下相关知识点 (😏) 。
