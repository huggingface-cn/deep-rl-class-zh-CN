# 从 Q-Learning 到深度 Q-Learning [[from-q-to-dqn]]

我们已知 Q-Learning 是我们用来训练我们的 Q-Function （一个决定在特定状态并在该状态采取特定动作的**动作-价值函数**）的一种算法。


<figure>
  <img src="https://raw.githubusercontent.com/innovation64/Picimg/main/Q-function.png" alt="Q-function"/>
</figure>

**Q 代表那个状态下的动作的质量**。

在内部，我们的 Q-Function 有一个 **Q-table (一张每个格子对应状态-动作对的值的表)**。把这个 Q-table 当作 Q-Function 的**记忆或者备忘录**。

这里的问题是 Q-learning 是*打表法*。这导致了状态和动作空间必须**足够小以列表的方式呈现在表里来适应合适的价值函数**。同样，他也不可**扩展**。Q-Learning 在小状态空间环境效果很好，比如：

- FrozenLake, 我们有 16 个状态.
- Taxi-v3, 我们有 500 个状态.

但是想一下我们今天要干啥：我们将要用帧作为输入训练一个智能体学习空间入侵这种更复杂的游戏。

**[Nikita Melkozerov 曾说](https://twitter.com/meln1k), Atari 环境** 有一个大小为 (210, 160, 3)* 的观测空间, 包含值的范围从 0 到 255 也就是大致给了我们 \\(256^{210 \times 160 \times 3} = 256^{100800}\\) (作为对比，在可观测宇宙中我们有\\(10^{80}\\) 原子).

* Atari 中的单一帧由一张 210x160 像素的图片组成。给定的图片是彩色的，有 3 个通道。这也是为什么形状大小为 (210, 160, 3)。对于每一个像素，取值在0 到 255 间。

<img src="https://raw.githubusercontent.com/innovation64/Picimg/main/atari.png" alt="Atari State Space"/>

因此，状态空间是巨大的。所以在这种环境下创建和更新 Q-table 会失效。针对这种情况，最好的方法是用参数化 Q-function \\(Q_{\theta}(s,a)\\) 去估计 Q-value 而不是用 Q-table。

神经网络会进行估计，对于给定状态下每个可能的动作给出不同的 Q-value。这就是深度 Q-Learning干的事情。

<img src="https://raw.githubusercontent.com/innovation64/Picimg/main/deep.png" alt="Deep Q Learning"/>

现在我们理解了深度 Q-Learning,下面让我们深入深度 Q-Network。