# 深度 Q-learning [[deep-q-learning]]

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/thumbnail.jpg" alt="Unit 3 thumbnail" width="100%">

在上一单元，我们学习了我们的第一个强化学习算法：Q-learning，**从头开始实现**，并将其在两个环境（FrozenLake-v1 ☃️ and Taxi-v3 🚕.）中训练。

我们通过简单的算法就取得了极佳的结果，但是由于**状态空间离散并且较小**（FrozenLake-v1 ☃️ 有16种不同状态，Taxi-v3 🚕 有500种）所以导致这些环境相对简单。相比之下， 雅达利（Atari）的状态空间包含 **$\\(10^{9}\\) 到 \\(10^{11}\\)$ 个状态**。

但是据目前所知，当环境的状态空间较大时，**产生和更新 Q 表格会失效**。

所以在本单元，我们将要学习我们**第一个深度强化学习智能体**：深度 Q-learning。相比于用 Q 表格，深度 Q-learning 使用一个神经网络，该神经网络输入一个状态并估计基于该状态每个动作的 Q 值。

我们将**使用 [RL-Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)**（一个使用稳定基线且提供训练脚本，评估智能体，微调超参数，绘制结果并记录视频的强化学习训练框架） 训练以演示太空侵略者（Space Invaders）和其他雅达利游戏环境。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif" alt="Environments"/>

所以让我们开始吧！🚀