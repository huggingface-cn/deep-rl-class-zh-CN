# 词汇表

这是一个社区创建的词汇表。欢迎投稿！

- **表格型方法:** 针对状态和动作空间足够小的问题类型，去用数组或表格近似价值函数的方法。
**Q-learning** 是一个表格型方法的例子，其表格被用于表示不同的状态-动作对的值。
- **深度 Q-learning :** 是一种给定一个状态使用神经网络去估计该状态下每个可能动作的不同 Q 值的方法。其通常用来解决不适用表格型方法 Q-learning 的观测空间过大的情况。

- **时序限制:** 当环境状态以帧的形式表示其实很困难。一个帧自己不提供时序信息。为了获取时序信息我们需要一堆帧**堆**在一起。
- **深度 Q-learning 的部分:**
  - **采样:** 执行动作并且把观测到的经验储存在**回放内存**中。
  - **训练:** 元组中的一小批会被随机选出丢入神经网络用梯度下降去更新网络。
  
- **稳定深度 Q-learning  的方法:**
  - **经验回放:** 创建一个回放内存来保存可以在训练期间重复使用的经验样本。
  这使得智能体可以从同样的经验中多次学习。并且避免了在新环境交互下忘记之前的行为。在回放缓冲区中的**随机采样**移除了观测序列的相关性并且阻止了动作价值的震荡或爆炸。

  - **固定 Q 目标:** 为了计算 **Q 目标** 我们需要通过贝尔曼方程估计下一个状态 **Q 值** 。这里有一个问题，同样的网络权重计算 **Q 目标** 和 **Q 值**。这意味着每一个时间步我们确定 **Q 值**,  **Q 目标** 同样也会跟着移动。为了避免这种事情发生，一个单独的固定参数的网络去估计时序差分目标。目标网络通过我们的在每 **C 步**复制深度 Q 网络的参数进行更新。
  
  - **双 DQN:** 一种用于解决 Q 值高估的方法。该方法使用两个网络将动作选择与目标**值生成**解耦：
     -**DQN 网络** 为下一个状态挑选最佳动作(有着最高 **Q 值** 的动作)
     -**目标网络** 计算下一个状态采用该动作的 目标 **Q 值**。
     这种方法减少了 **Q 值** 的高估，它帮助我们更快训练和更稳定学习。 

如果您想改进课程，可以[打开一个 Pull Request](https://github.com/huggingface/deep-rl-class/pulls)

感谢以下人员对于词汇表的贡献:

- [Dario Paez](https://github.com/dario248)
