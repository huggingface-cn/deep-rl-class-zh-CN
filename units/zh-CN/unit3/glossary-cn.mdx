# 词汇表

这是一个社区创建的词汇表。欢迎投稿！

- **表格型方法:** 针对状态和动作空间足够小去用数组或元组表现估计价值函数的问题类型的方法。
**Q 学习** 是一个表格型方法的例子，其表被用于表示不同的状态-动作对的值。
- **深度 Q 学习:** 是一种给定一个状态使用神经网络去估计该状态下每个可能动作的不同 Q 值的方法。他通常用来解决不适用表格型方法 Q 学习的观测空间过大的情况。

- **时序限制:** 当环境状态以帧的形式呈现其实很困难。单独帧自己不提供时序信息。为了获取时序信息我们需要一堆帧**堆**在一起。
- **深度 Q 学习的部分:**
  - **采样:** 执行动作并且把观测到的经验储存在**回放内存**中。
  - **训练:** 元组中的一小批会被随机选出丢入神经网络用梯度下降去更新网络。
  
- **稳定深度 Q 学习 的方法:**
  - **经验回放:** 回放内存是用来在训练中重复使用来节省经验采样的。
  这使得智能体可以从同样的经验中多次学习。并且避免了在新环境交互下忘记之前的行为。在回放缓冲区中的**随机采样**移除了观测序列的相关性并且阻止了动作价值的震荡或爆炸。

  - **固定 Q 目标:** 为了计算 **Q 目标** 我们需要通过贝尔曼方程估计下一个状态 **Q 值** 。这里有一个问题，同样的网络权重计算 **Q 目标** 和 **Q 值**。这意味着每一个时间步我们确定 **Q 值**,  **Q 目标** 同样也会跟着移动。为了避免这种事情发生，一个分离的固定参数的网络去估计时序差分目标。目标网络通过我们的在每 **C 步**复制深度 Q 网络的参数进行更新。
  
  - **双深度Q网络:** 一种用于解决 Q 值高估的方法。这个方法用两个网络从目标**价值生成**中去解除动作选择：
     -**DQN 网络** 为下一个状态筛选最佳动作(有着最高 **Q 值** 的动作)
     -**目标网络** 计算下一个状态采用该动作的 目标 **Q 值**。
     这种方法减少了 **Q 值** 的高估，他帮助我们更快训练和更稳定学习。 

如果您想改进课程，可以[打开一个 Pull Request](https://github.com/huggingface/deep-rl-class/pulls)

感谢以下人员对于词汇表的贡献:

- [Dario Paez](https://github.com/dario248)
