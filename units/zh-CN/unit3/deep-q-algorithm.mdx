# 深度 Q-learning 算法 [[deep-q-algorithm]]

我们了解到深度 Q-learning **使用深度神经网络来近似每个状态下可能动作的不同 Q 值** (价值函数估计)。

区别在于，在训练阶段，不像我们在Q-learning中所做的那样，直接更新状态-动作对的 Q 值：

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-5.jpg" alt="Q Loss"/>

在深度 Q-learning 中，我们创建了一个**损失函数来对比 Q 值的预测和 Q 目标，并且使用梯度下降更新我们深度 Q 网络的权重来使 Q 值预测得更精准**。


<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/Q-target.jpg" alt="Q-target"/>

深度 Q-learning 训练算法有**两个阶段**：

- **采样**：我们执行动作并且**储存观测到的经验元组在回放内存中**。
- **训练**：随机选择**一个小批量并从中学习用梯度下降更新每一步**。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/sampling-training.jpg" alt="Sampling Training"/>

相比于 Q-learning 这不是唯一的不同。深度 Q-learning 训练**可能会不稳定**，主要因为结合了非线性 Q 值函数（神经网络）和 自举法（当我们用现有的估计而不是实际的完整回报更新目标时）。

为了稳定训练，我们使用以下三种不同的解决方案：
1. *经验回放* 去更**高效地使用经验**。
2. *固定 Q 目标*来**稳定训练**。
3. *双深度 Q-learning *, 解决 Q 值**高估**问题。

让我们继续！
 
## 经验回放去更高效地使用经验[[exp-replay]]

为什么我们要制造一个回放内存？

经验回放在深度 Q-learning  里面有两个作用：

1. **在训练期间更有效地利用经验**。
通常，在线强化学习中，智能体会与环境交互，获得经验（状态，动作，奖励和下一个状态），从经验中学习（更新神经网络），然后抛弃它们。这种做法并不高效。

经验回放能够帮助我们**更高效地使用训练的经验**。我们使用一个回放缓冲区来保存经验的样例，**从而在训练中重复使用**。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/experience-replay.jpg" alt="Experience Replay"/>

⇒ 这使得智能体**多次从同样的经验中学习**。

2. **避免忘记之前所学并且减少经验间的相关性**.
- 现在的问题是如果我们给了一个序列的经验采样在我们的神经网络中，他会倾向于在得到**新经验后忘记之前的经验**。例如，如果智能体在第一个关卡中，然后再第二个关卡中，这两个关卡是不同的，这就使得智能体忘记了如何在第一个关卡中表现、玩。

现在的解决办法是创建一个回放缓冲区，在与环境交互时存储经验元组，然后采样一个小批量的元组。这可以防止**网络只学习其以前所做的事情**。 

经验回放同样还有其他好处。通过随机采样经验，我们移除了观测序列的相关性同时避免了动作函数值**陷入震荡或发生爆炸**。

在深度 Q-learning 的伪代码中，我们**从容器 N 中初始化一个回放内存缓冲区 D**（N 是你可以自定义的超参数）。接下来我们将经验储存在内存中同时采样一小批经验喂给训练阶段的深度 Q 网络。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/experience-replay-pseudocode.jpg" alt="Experience Replay Pseudocode"/>

## 固定 Q 目标来稳定训练 [[fixed-q]]

当我们想要计算时序差分误差（或者叫损失）时，我们计算　**TD　目标（Q 目标）和当前　Q 值（Q的估计）的不同**。

但是我们对于**真实的　TD　目标是啥完全不知道**。我们需要估计它。通过贝尔曼方程，我们了解到　TD　目标仅仅是采取该状态动作的奖励加下一个状态的折扣　Q 值。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/Q-target.jpg" alt="Q-target"/>

然而这里有个问题是我们使用相同的参数（权重）去估计　TD　目标和　Q 值。因此，TD　目标与我们正在更改的参数之间存在显著相关性。

这意味着训练的每一步，我们的　**Q 值改变目标值同样也会改变** 。我们离目标越近，同样目标也会移动。就像在追逐目标一样！这会导致在训练中有显著的震荡。

就像假如你是个牛仔（Q　估计）并且你想要抓住奶牛（Q 目标）。你的目标是不断靠近他（减少错误）。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/qtarget-1.jpg" alt="Q-target"/>

在每一个时间步中，你都尽可能的尝试靠近奶牛，但同样奶牛也在每个时间步中移动（因为你们使用相同参数）。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/qtarget-2.jpg" alt="Q-target"/>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/qtarget-3.jpg" alt="Q-target"/>

这导致了追逐的奇怪路径（训练中的显著振荡）。
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/qtarget-4.jpg" alt="Q-target"/>

我们在伪代码中看到如下：
- 使用一个**单独的固定参数的网络去估计　TD　目标**。
- 通过在**深度　Q 网络中每　C　步复制参数**去更新目标网络。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/fixed-q-target-pseudocode.jpg" alt="Fixed Q-target Pseudocode"/>



## 双 DQN [[double-dqn]]

双DQN, 或者叫双深度 Q-learning,  [ Hado van Hasselt](https://papers.nips.cc/paper/3964-double-q-learning)所介绍。这个方法主要用来**解决　Q 值的高估问题**。

为了理解这个问题，先看一下我们怎么计算　TD　目标的：

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1.jpg" alt="TD target"/>

我们在计算　TD 目标中面临着一个简单的问题：我们怎么确保下一个状态的**最佳动作是有着最高 Q 值的动作**。

我们知道 Q 值的精度依赖于我们尝试过的动作**和**我们探索过的周边状态。

因此，在训练之初我们对于最佳动作的采取没有足够的信息。采取最大 Q 值（噪声）作为最佳动作可能导致负向效果。如果**相比于最佳动作给非最优动作更高的 Q 值，学习将会变得非常复杂。**

解决这个问题的方法是：当我们计算 Q 目标时，我们使用两个网络将动作选择与目标 Q 值生成解耦。我们：
- 使用 **DQN 网络** 去为下一个状态选择最佳动作（有最高 Q 值的动作）。
- 使用 **目标网络** 去计算下一个状态的目标 Q 值的采取动作

因此，双DQN帮助我们减少 Q 值高估并且帮助我们更快训练和更稳定学习。

在这三项提升深度 Q-learning 的措施之后，还出现了像 优先经验回放，竞争深度 Q-learning 等许多方法。但这些并不在本课程范围内，如果你有兴趣，可以查看我们提供的阅读链接。
