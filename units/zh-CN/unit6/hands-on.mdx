# 利用 PyBullet 和 Panda-Gym 进行机器人模拟的优势演员--评论员方法 (A2C) 🤖[[hands-on]]

```
  <CourseFloatingBanner classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb"}
    ]}
    askForHelpUrl="http://hf.co/join/discord" />
```

现在你已经研究了优势阶演员--评论员方法 (A2C) 背后的理论，**你已准备好在机器人环境中使用 Stable-Baselines3 训练你的 A2C 智能体**。 并训练两个机器人：

- 一个正在学习移动的蜘蛛。🕷️ 
- 一个正在学习移动到正确位置的机械臂。 🦾

我们将使用两个机器人开发环境：

- [PyBullet](https://github.com/bulletphysics/bullet3)
- [panda-gym](https://github.com/qgallouedec/panda-gym)

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/environments.gif" alt="Environments"/>

要验证认证过程的实际操作，你需要将两个经过训练的模型推送到 Hub 并获得以下结果：

- `AntBulletEnv-v0` 得到的结果需 >= 650.
- `PandaReachDense-v2` 得到的结果需 >= -3.5.

要找到你的结果，[转到排行榜](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) 并找到你的模型，**结果 = 平均奖励 - 奖励标准* *

有关认证过程的更多信息，请查看此部分 👉 https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process

**要开始动手操作，请单击“在 Colab 中打开”按钮** 👇 :

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit6/unit6.ipynb)

# 第 6 单元：利用 PyBullet 和 Panda-Gym 进行机器人模拟的 高阶演员--评论家方法 (A2C) 🤖

### 🎮 开发环境:

- [PyBullet](https://github.com/bulletphysics/bullet3)
- [Panda-Gym](https://github.com/qgallouedec/panda-gym)

### 📚 强化学习库:

- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/)

我们一直在努力改进我们的教程，所以 **如果你在此笔记本中发现一些问题**，请 [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues).

## 本notebook的目标 🏆

在本notebook的最后，你将：

- 有能力能够使用环境库 **PyBullet** 和 **Panda-Gym**。
- 能够**使用 A2C** 训练机器人。
- 理解为什么**我们需要规范化输入**。
- 能够**将你训练有素的智能体和代码推送到 Hub**，并附带有漂亮的视频回放和评估分数 🔥。

## 先决条件🏗️

在深入研究notebook之前，你需要：

🔲 📚 学习 [阅读并学习Unit6中的演员--评论家方法](https://huggingface.co/deep-rl-course/unit6/introduction) 🤗

# 让我们一起训练我们的第一个机器人吧 🤖

## 设置好GPU 💪

- 为了**加速智能体的训练，我们将使用 GPU**。 为此，请转到 `Runtime > Change Runtime type`

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg" alt="GPU Step 1">

- `Hardware Accelerator > GPU`

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg" alt="GPU Step 2">

## 创建一个虚拟屏幕 🔽

在notebook中，我们需要生成重播视频。 为此，使用 colab，**我们需要一个虚拟屏幕来渲染环境**（从而记录帧）。

因此，以下单元将安装库并创建和运行虚拟屏幕 🖥

```python
%%capture
!apt install python-opengl
!apt install ffmpeg
!apt install xvfb
!pip3 install pyvirtualdisplay
```

```python
# Virtual display
from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()
```

### 安装依赖 🔽

第一步是安装依赖项，我们将安装多个依赖：

- `pybullet`: 包含步行机器人环境。
- `panda-gym`: 包含机械臂环境。
- `stable-baselines3[extra]`: SB3 深度强化学习库。
- `huggingface_sb3`: Stable-baselines3 的附加代码，用于从 Hugging Face 🤗 Hub 加载和上传模型。
- `huggingface_hub`: Library允许任何人使用 Hub 的仓库。

```bash
!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit6/requirements-unit6.txt
```

## 导入相关包 📦

```python
import pybullet_envs
import panda_gym
import gym

import os

from huggingface_sb3 import load_from_hub, package_to_hub

from stable_baselines3 import A2C
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.env_util import make_vec_env

from huggingface_hub import notebook_login
```

## 环境 1: AntBulletEnv-v0 🕸

### 创建 AntBulletEnv-v0环境

#### 环境依赖 🎮

在这种环境中，智能体需要正确使用其不同的关节才能正确行走。
你可以在此处找到此环境的详细说明： https://hackmd.io/@jeffreymo/SJJrSJh5_#PyBullet

```python
env_id = "AntBulletEnv-v0"
# Create the env
env = gym.make(env_id)

# Get the state space and action space
s_size = env.observation_space.shape[0]
a_size = env.action_space
```

```python
print("_____OBSERVATION SPACE_____ \n")
print("The State Space is: ", s_size)
print("Sample observation", env.observation_space.sample())  # Get a random observation
```

observation space (来自 [Jeffrey Y Mo](https://hackmd.io/@jeffreymo/SJJrSJh5_#PyBullet)):
区别在于我们的 observation space 是 28 而不是 29.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/obs_space.png" alt="PyBullet Ant Obs space"/>

```python
print("\n _____ACTION SPACE_____ \n")
print("The Action Space is: ", a_size)
print("Action Space Sample", env.action_space.sample())  # Take a random action
```

The action Space (来自 [Jeffrey Y Mo](https://hackmd.io/@jeffreymo/SJJrSJh5_#PyBullet)):

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/action_space.png" alt="PyBullet Ant Obs space"/>

### 归一化观测和奖励

在强化学习中的一个较好的做法是 [归一化输入特征](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html).

为此，wrapper可以计算输入特征的运行平均值和标准差。

我们还通过添加相同的 wrapper 来规范化奖励参数 `norm_reward = True`

[You should check the documentation to fill this cell](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)

```python
env = make_vec_env(env_id, n_envs=4)

# Adding this wrapper to normalize the observation and the reward
env = # TODO: Add the wrapper
```

#### 解决方法

```python
env = make_vec_env(env_id, n_envs=4)

env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.0)
```

### 创建 A2C 模型 🤖

在这种情况下，因为我们有一个包含 28 个值的向量作为输入，所以我们将使用多层感知机（MLP）作为策略。

有关使用 StableBaselines3 实现 A2C 的更多信息，请查看： https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes

为了找到最佳参数，我检查了 [official trained agents by Stable-Baselines3 team](https://huggingface.co/sb3).

```python
model = # Create the A2C model and try to find the best parameters
```

#### 解决方案

```python
model = A2C(
    policy="MlpPolicy",
    env=env,
    gae_lambda=0.9,
    gamma=0.99,
    learning_rate=0.00096,
    max_grad_norm=0.5,
    n_steps=8,
    vf_coef=0.4,
    ent_coef=0.0,
    policy_kwargs=dict(log_std_init=-2, ortho_init=False),
    normalize_advantage=False,
    use_rms_prop=True,
    use_sde=True,
    verbose=1,
)
```

### 训练 A2C 智能体 🏃

- 让我们用 2,000,000 个时间步训练我们的智能体，不要忘记在 Colab 上使用 GPU。 大约需要 25-40 分钟

```python
model.learn(2_000_000)
```

```python
# Save the model and  VecNormalize statistics when saving the agent
model.save("a2c-AntBulletEnv-v0")
env.save("vec_normalize.pkl")
```

### 评估智能体 📈

- 现在我们的智能体已经过了训练，我们需要**检查其性能**。
- Stable-Baselines3 提供了一种方法来做到这一点: `evaluate_policy`
- 就我而言，我得到的平均奖励是 `2371.90 +/- 16.50`

```python
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize

# Load the saved statistics
eval_env = DummyVecEnv([lambda: gym.make("AntBulletEnv-v0")])
eval_env = VecNormalize.load("vec_normalize.pkl", eval_env)

#  do not update them at test time
eval_env.training = False
# reward normalization is not needed at test time
eval_env.norm_reward = False

# Load the agent
model = A2C.load("a2c-AntBulletEnv-v0")

mean_reward, std_reward = evaluate_policy(model, env)

print(f"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}")
```

### 在 Hub 上发布你的训练模型 🔥

现在我们看到训练后取得了不错的效果，我们可以通过一行代码将训练好的模型发布到 Hub 上。

📚 libraries 文档👉 https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20

这是模型卡的示例（使用 PyBullet 环境）：

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/modelcardpybullet.png" alt="Model Card Pybullet"/>

通过使用 `package_to_hub`, 正如我们在之前的单元中提到的，**评估、录制回放、生成智能体的模型卡并将其推送到Hub**。

通过如下方法：

- 你可以**展示我们的工作成果**🔥
- 你可以**可视化你的智能体过程** 👀
- 你可以**与社区共享其他人可以使用的智能体**💾
- 你可以**访问排行榜 🏆 以查看你的智能体与你的同学相比的表现**👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard

为了能够与社区共享你的模型，还需要执行三个步骤：

1️⃣ （如果尚未完成）创建一个 HuggingFace 帐户 ➡ https://huggingface.co/join

2️⃣ 登录，然后你需要存储来自 Hugging Face 网站的身份验证令牌。

- 创建一个新令牌 (https://huggingface.co/settings/tokens) **通过以下描述的方法**

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg" alt="Create HF Token">

- 复制令牌
- 运行下面的单元格并粘贴令牌

```python
notebook_login()
!git config --global credential.helper store
```

如果你不想使用 Google Colab 或 Jupyter Notebook，则需要改用此命令： `huggingface-cli login`

3️⃣ 我们现在准备好将训练有素的智能体推送到 🤗 Hub 🔥 使用 `package_to_hub()` 函数

```python
package_to_hub(
    model=model,
    model_name=f"a2c-{env_id}",
    model_architecture="A2C",
    env_id=env_id,
    eval_env=eval_env,
    repo_id=f"ThomasSimonini/a2c-{env_id}",  # Change the username
    commit_message="Initial commit",
)
```

## 喝杯咖啡休息一下吧！ ☕

- 恭喜你已经训练了第一个学会移动的机器人 🥳!
- **该休息了**。 不要犹豫，**保存此笔记本**“文件 > 将副本保存到云端硬盘”，以便稍后处理第二部分。

## 开发环境 2：PandaReachDense-v2 🦾

我们要训练的智能体是一个需要进行控制（移动手臂和使用末端执行器）的机械臂。

在机器人技术中，*末端执行器*是位于机械臂末端的装置，旨在与环境进行交互。

在 `PandaReach`中, 机器人必须将其末端执行器放置在目标位置（绿色球）。

我们将使用此环境的密集版本。 这意味着我们将获得一个*密集的奖励函数*，**将在每个时间步提供奖励**（智能体越接近完成任务，奖励越高）。 与*稀疏奖励函数*相反，其中环境**当且仅当任务完成时才返回奖励**。

此外，我们将使用*末端执行器位移控制*，这意味着**动作对应于末端执行器的位移**。 我们不控制每个关节的单独运动（关节控制）。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/robotics.jpg"  alt="Robotics"/>

这种方法会使得**训练会更容易**。



在 `PandaReachDense-v2`中, 机械臂必须将其末端执行器放置在目标位置（绿色球）。

```python
import gym

env_id = "PandaPushDense-v2"

# Create the env
env = gym.make(env_id)

# Get the state space and action space
s_size = env.observation_space.shape
a_size = env.action_space
```

```python
print("_____OBSERVATION SPACE_____ \n")
print("The State Space is: ", s_size)
print("Sample observation", env.observation_space.sample())  # Get a random observation
```

The observation space **是一个包含 3 个不同元素的字典**:

- `achieved_goal`: (x,y,z) 目标的位置。
- `desired_goal`: (x,y,z) 目标位置和当前对象位置之间的距离。
- `observation`: 末端执行器的位置 (x,y,z) 和速度 (vx, vy, vz)。

鉴于它是一个作为观测的字典，**我们将需要使用 MultiInputPolicy 而不是 MlpPolicy**。

```python
print("\n _____ACTION SPACE_____ \n")
print("The Action Space is: ", a_size)
print("Action Space Sample", env.action_space.sample())  # Take a random action
```

动作空间是一个具有 3 个值的向量：

- 控制 x, y, z 移动

现在轮到你了：

1. 定义名为“PandaReachDense-v2”的环境
2. 制作矢量化环境
3. 添加 wrapper 以规范化观察和奖励。 [Check the documentation](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)
4. 创建 A2C 模型（不要忘记使 verbose=1 来打印训练日志）。
5. 以 1M 时间步对其进行训练
6. 保存智能体时保存模型和 VecNormalize 统计信息
7. 评估你的智能体
8. 在 Hub 上发布你的训练模型 🔥通过使用 `package_to_hub`函数

### 解决方案(完成待办事项)

```python
# 1 - 2
env_id = "PandaReachDense-v2"
env = make_vec_env(env_id, n_envs=4)

# 3
env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.0)

# 4
model = A2C(policy="MultiInputPolicy", env=env, verbose=1)
# 5
model.learn(1_000_000)
```

```python
# 6
model_name = "a2c-PandaReachDense-v2"
model.save(model_name)
env.save("vec_normalize.pkl")

# 7
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize

# Load the saved statistics
eval_env = DummyVecEnv([lambda: gym.make("PandaReachDense-v2")])
eval_env = VecNormalize.load("vec_normalize.pkl", eval_env)

#  do not update them at test time
eval_env.training = False
# reward normalization is not needed at test time
eval_env.norm_reward = False

# Load the agent
model = A2C.load(model_name)

mean_reward, std_reward = evaluate_policy(model, env)

print(f"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}")

# 8
package_to_hub(
    model=model,
    model_name=f"a2c-{env_id}",
    model_architecture="A2C",
    env_id=env_id,
    eval_env=eval_env,
    repo_id=f"ThomasSimonini/a2c-{env_id}",  # TODO: Change the username
    commit_message="Initial commit",
)
```

## 一些额外的挑战 🏆

学习**的最佳方法是自己尝试**！ 为什么不为 PyBullet 尝试“HalfCheetah Bullet Env-v0”，为 Panda-Gym 尝试“PandaPick Place-v1”？

如果你想为 panda-gym 尝试更高级的任务，你需要检查使用 **TQC 或 SAC**（一种更适合机器人任务的样本效率更高的算法）完成了什么。 在真实的机器人技术中，出于一个简单的原因，你将使用样本效率更高的算法：与模拟过程相反**如果你将机械臂移动太多，则有损坏它的风险**。

PandaPickAndPlace-v1: https://huggingface.co/sb3/tqc-PandaPickAndPlace-v1

不要犹豫，在这里查看 panda-gym 文档: https://panda-gym.readthedocs.io/en/latest/usage/train_with_sb3.html

以下是实现此目标的一些想法：

- 训练更多步数
- 通过查看其他同学所做的工作，尝试不同的超参数 👉 https://huggingface.co/models?other=https://huggingface.co/models?other=AntBulletEnv-v0
- **在 Hub 上推送你新训练的模型** 🔥

第7单元见! 🔥

## 保持热爱，奔赴山海 🤗