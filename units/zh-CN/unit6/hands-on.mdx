# åˆ©ç”¨ PyBullet å’Œ Panda-Gym è¿›è¡Œæœºå™¨äººæ¨¡æ‹Ÿçš„ä¼˜åŠ¿æ¼”å‘˜--è¯„è®ºå‘˜æ–¹æ³• (A2C) ğŸ¤–[[hands-on]]

```
  <CourseFloatingBanner classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb"}
    ]}
    askForHelpUrl="http://hf.co/join/discord" />
```

ç°åœ¨ä½ å·²ç»ç ”ç©¶äº†ä¼˜åŠ¿é˜¶æ¼”å‘˜--è¯„è®ºå‘˜æ–¹æ³• (A2C) èƒŒåçš„ç†è®ºï¼Œ**ä½ å·²å‡†å¤‡å¥½åœ¨æœºå™¨äººç¯å¢ƒä¸­ä½¿ç”¨ Stable-Baselines3 è®­ç»ƒä½ çš„ A2C æ™ºèƒ½ä½“**ã€‚ å¹¶è®­ç»ƒä¸¤ä¸ªæœºå™¨äººï¼š

- ä¸€ä¸ªæ­£åœ¨å­¦ä¹ ç§»åŠ¨çš„èœ˜è››ã€‚ğŸ•·ï¸ 
- ä¸€ä¸ªæ­£åœ¨å­¦ä¹ ç§»åŠ¨åˆ°æ­£ç¡®ä½ç½®çš„æœºæ¢°è‡‚ã€‚ ğŸ¦¾

æˆ‘ä»¬å°†ä½¿ç”¨ä¸¤ä¸ªæœºå™¨äººå¼€å‘ç¯å¢ƒï¼š

- [PyBullet](https://github.com/bulletphysics/bullet3)
- [panda-gym](https://github.com/qgallouedec/panda-gym)

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/environments.gif" alt="Environments"/>

è¦éªŒè¯è®¤è¯è¿‡ç¨‹çš„å®é™…æ“ä½œï¼Œä½ éœ€è¦å°†ä¸¤ä¸ªç»è¿‡è®­ç»ƒçš„æ¨¡å‹æ¨é€åˆ° Hub å¹¶è·å¾—ä»¥ä¸‹ç»“æœï¼š

- `AntBulletEnv-v0` å¾—åˆ°çš„ç»“æœéœ€ >= 650.
- `PandaReachDense-v2` å¾—åˆ°çš„ç»“æœéœ€ >= -3.5.

è¦æ‰¾åˆ°ä½ çš„ç»“æœï¼Œ[è½¬åˆ°æ’è¡Œæ¦œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) å¹¶æ‰¾åˆ°ä½ çš„æ¨¡å‹ï¼Œ**ç»“æœ = å¹³å‡å¥–åŠ± - å¥–åŠ±æ ‡å‡†* *

æœ‰å…³è®¤è¯è¿‡ç¨‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æ­¤éƒ¨åˆ† ğŸ‘‰ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process

**è¦å¼€å§‹åŠ¨æ‰‹æ“ä½œï¼Œè¯·å•å‡»â€œåœ¨ Colab ä¸­æ‰“å¼€â€æŒ‰é’®** ğŸ‘‡ :

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit6/unit6.ipynb)

# ç¬¬ 6 å•å…ƒï¼šåˆ©ç”¨ PyBullet å’Œ Panda-Gym è¿›è¡Œæœºå™¨äººæ¨¡æ‹Ÿçš„ é«˜é˜¶æ¼”å‘˜--è¯„è®ºå®¶æ–¹æ³• (A2C) ğŸ¤–

### ğŸ® å¼€å‘ç¯å¢ƒ:

- [PyBullet](https://github.com/bulletphysics/bullet3)
- [Panda-Gym](https://github.com/qgallouedec/panda-gym)

### ğŸ“š å¼ºåŒ–å­¦ä¹ åº“:

- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/)

æˆ‘ä»¬ä¸€ç›´åœ¨åŠªåŠ›æ”¹è¿›æˆ‘ä»¬çš„æ•™ç¨‹ï¼Œæ‰€ä»¥ **å¦‚æœä½ åœ¨æ­¤ç¬”è®°æœ¬ä¸­å‘ç°ä¸€äº›é—®é¢˜**ï¼Œè¯· [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues).

## æœ¬notebookçš„ç›®æ ‡ ğŸ†

åœ¨æœ¬notebookçš„æœ€åï¼Œä½ å°†ï¼š

- æœ‰èƒ½åŠ›èƒ½å¤Ÿä½¿ç”¨ç¯å¢ƒåº“ **PyBullet** å’Œ **Panda-Gym**ã€‚
- èƒ½å¤Ÿ**ä½¿ç”¨ A2C** è®­ç»ƒæœºå™¨äººã€‚
- ç†è§£ä¸ºä»€ä¹ˆ**æˆ‘ä»¬éœ€è¦è§„èŒƒåŒ–è¾“å…¥**ã€‚
- èƒ½å¤Ÿ**å°†ä½ è®­ç»ƒæœ‰ç´ çš„æ™ºèƒ½ä½“å’Œä»£ç æ¨é€åˆ° Hub**ï¼Œå¹¶é™„å¸¦æœ‰æ¼‚äº®çš„è§†é¢‘å›æ”¾å’Œè¯„ä¼°åˆ†æ•° ğŸ”¥ã€‚

## å…ˆå†³æ¡ä»¶ğŸ—ï¸

åœ¨æ·±å…¥ç ”ç©¶notebookä¹‹å‰ï¼Œä½ éœ€è¦ï¼š

ğŸ”² ğŸ“š å­¦ä¹  [é˜…è¯»å¹¶å­¦ä¹ Unit6ä¸­çš„æ¼”å‘˜--è¯„è®ºå®¶æ–¹æ³•](https://huggingface.co/deep-rl-course/unit6/introduction) ğŸ¤—

# è®©æˆ‘ä»¬ä¸€èµ·è®­ç»ƒæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªæœºå™¨äººå§ ğŸ¤–

## è®¾ç½®å¥½GPU ğŸ’ª

- ä¸ºäº†**åŠ é€Ÿæ™ºèƒ½ä½“çš„è®­ç»ƒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ GPU**ã€‚ ä¸ºæ­¤ï¼Œè¯·è½¬åˆ° `Runtime > Change Runtime type`

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg" alt="GPU Step 1">

- `Hardware Accelerator > GPU`

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg" alt="GPU Step 2">

## åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿå±å¹• ğŸ”½

åœ¨notebookä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆé‡æ’­è§†é¢‘ã€‚ ä¸ºæ­¤ï¼Œä½¿ç”¨ colabï¼Œ**æˆ‘ä»¬éœ€è¦ä¸€ä¸ªè™šæ‹Ÿå±å¹•æ¥æ¸²æŸ“ç¯å¢ƒ**ï¼ˆä»è€Œè®°å½•å¸§ï¼‰ã€‚

å› æ­¤ï¼Œä»¥ä¸‹å•å…ƒå°†å®‰è£…åº“å¹¶åˆ›å»ºå’Œè¿è¡Œè™šæ‹Ÿå±å¹• ğŸ–¥

```python
%%capture
!apt install python-opengl
!apt install ffmpeg
!apt install xvfb
!pip3 install pyvirtualdisplay
```

```python
# Virtual display
from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()
```

### å®‰è£…ä¾èµ– ğŸ”½

ç¬¬ä¸€æ­¥æ˜¯å®‰è£…ä¾èµ–é¡¹ï¼Œæˆ‘ä»¬å°†å®‰è£…å¤šä¸ªä¾èµ–ï¼š

- `pybullet`: åŒ…å«æ­¥è¡Œæœºå™¨äººç¯å¢ƒã€‚
- `panda-gym`: åŒ…å«æœºæ¢°è‡‚ç¯å¢ƒã€‚
- `stable-baselines3[extra]`: SB3 æ·±åº¦å¼ºåŒ–å­¦ä¹ åº“ã€‚
- `huggingface_sb3`: Stable-baselines3 çš„é™„åŠ ä»£ç ï¼Œç”¨äºä» Hugging Face ğŸ¤— Hub åŠ è½½å’Œä¸Šä¼ æ¨¡å‹ã€‚
- `huggingface_hub`: Libraryå…è®¸ä»»ä½•äººä½¿ç”¨ Hub çš„ä»“åº“ã€‚

```bash
!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit6/requirements-unit6.txt
```

## å¯¼å…¥ç›¸å…³åŒ… ğŸ“¦

```python
import pybullet_envs
import panda_gym
import gym

import os

from huggingface_sb3 import load_from_hub, package_to_hub

from stable_baselines3 import A2C
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.env_util import make_vec_env

from huggingface_hub import notebook_login
```

## ç¯å¢ƒ 1: AntBulletEnv-v0 ğŸ•¸

### åˆ›å»º AntBulletEnv-v0ç¯å¢ƒ

#### ç¯å¢ƒä¾èµ– ğŸ®

åœ¨è¿™ç§ç¯å¢ƒä¸­ï¼Œæ™ºèƒ½ä½“éœ€è¦æ­£ç¡®ä½¿ç”¨å…¶ä¸åŒçš„å…³èŠ‚æ‰èƒ½æ­£ç¡®è¡Œèµ°ã€‚
ä½ å¯ä»¥åœ¨æ­¤å¤„æ‰¾åˆ°æ­¤ç¯å¢ƒçš„è¯¦ç»†è¯´æ˜ï¼š https://hackmd.io/@jeffreymo/SJJrSJh5_#PyBullet

```python
env_id = "AntBulletEnv-v0"
# Create the env
env = gym.make(env_id)

# Get the state space and action space
s_size = env.observation_space.shape[0]
a_size = env.action_space
```

```python
print("_____OBSERVATION SPACE_____ \n")
print("The State Space is: ", s_size)
print("Sample observation", env.observation_space.sample())  # Get a random observation
```

observation space (æ¥è‡ª [Jeffrey Y Mo](https://hackmd.io/@jeffreymo/SJJrSJh5_#PyBullet)):
åŒºåˆ«åœ¨äºæˆ‘ä»¬çš„ observation space æ˜¯ 28 è€Œä¸æ˜¯ 29.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/obs_space.png" alt="PyBullet Ant Obs space"/>

```python
print("\n _____ACTION SPACE_____ \n")
print("The Action Space is: ", a_size)
print("Action Space Sample", env.action_space.sample())  # Take a random action
```

The action Space (æ¥è‡ª [Jeffrey Y Mo](https://hackmd.io/@jeffreymo/SJJrSJh5_#PyBullet)):

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/action_space.png" alt="PyBullet Ant Obs space"/>

### å½’ä¸€åŒ–è§‚æµ‹å’Œå¥–åŠ±

åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¸€ä¸ªè¾ƒå¥½çš„åšæ³•æ˜¯ [å½’ä¸€åŒ–è¾“å…¥ç‰¹å¾](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html).

ä¸ºæ­¤ï¼Œwrapperå¯ä»¥è®¡ç®—è¾“å…¥ç‰¹å¾çš„è¿è¡Œå¹³å‡å€¼å’Œæ ‡å‡†å·®ã€‚

æˆ‘ä»¬è¿˜é€šè¿‡æ·»åŠ ç›¸åŒçš„ wrapper æ¥è§„èŒƒåŒ–å¥–åŠ±å‚æ•° `norm_reward = True`

[You should check the documentation to fill this cell](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)

```python
env = make_vec_env(env_id, n_envs=4)

# Adding this wrapper to normalize the observation and the reward
env = # TODO: Add the wrapper
```

#### è§£å†³æ–¹æ³•

```python
env = make_vec_env(env_id, n_envs=4)

env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.0)
```

### åˆ›å»º A2C æ¨¡å‹ ğŸ¤–

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå› ä¸ºæˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å« 28 ä¸ªå€¼çš„å‘é‡ä½œä¸ºè¾“å…¥ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ä½œä¸ºç­–ç•¥ã€‚

æœ‰å…³ä½¿ç”¨ StableBaselines3 å®ç° A2C çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ï¼š https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes

ä¸ºäº†æ‰¾åˆ°æœ€ä½³å‚æ•°ï¼Œæˆ‘æ£€æŸ¥äº† [official trained agents by Stable-Baselines3 team](https://huggingface.co/sb3).

```python
model = # Create the A2C model and try to find the best parameters
```

#### è§£å†³æ–¹æ¡ˆ

```python
model = A2C(
    policy="MlpPolicy",
    env=env,
    gae_lambda=0.9,
    gamma=0.99,
    learning_rate=0.00096,
    max_grad_norm=0.5,
    n_steps=8,
    vf_coef=0.4,
    ent_coef=0.0,
    policy_kwargs=dict(log_std_init=-2, ortho_init=False),
    normalize_advantage=False,
    use_rms_prop=True,
    use_sde=True,
    verbose=1,
)
```

### è®­ç»ƒ A2C æ™ºèƒ½ä½“ ğŸƒ

- è®©æˆ‘ä»¬ç”¨ 2,000,000 ä¸ªæ—¶é—´æ­¥è®­ç»ƒæˆ‘ä»¬çš„æ™ºèƒ½ä½“ï¼Œä¸è¦å¿˜è®°åœ¨ Colab ä¸Šä½¿ç”¨ GPUã€‚ å¤§çº¦éœ€è¦ 25-40 åˆ†é’Ÿ

```python
model.learn(2_000_000)
```

```python
# Save the model and  VecNormalize statistics when saving the agent
model.save("a2c-AntBulletEnv-v0")
env.save("vec_normalize.pkl")
```

### è¯„ä¼°æ™ºèƒ½ä½“ ğŸ“ˆ

- ç°åœ¨æˆ‘ä»¬çš„æ™ºèƒ½ä½“å·²ç»è¿‡äº†è®­ç»ƒï¼Œæˆ‘ä»¬éœ€è¦**æ£€æŸ¥å…¶æ€§èƒ½**ã€‚
- Stable-Baselines3 æä¾›äº†ä¸€ç§æ–¹æ³•æ¥åšåˆ°è¿™ä¸€ç‚¹: `evaluate_policy`
- å°±æˆ‘è€Œè¨€ï¼Œæˆ‘å¾—åˆ°çš„å¹³å‡å¥–åŠ±æ˜¯ `2371.90 +/- 16.50`

```python
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize

# Load the saved statistics
eval_env = DummyVecEnv([lambda: gym.make("AntBulletEnv-v0")])
eval_env = VecNormalize.load("vec_normalize.pkl", eval_env)

#  do not update them at test time
eval_env.training = False
# reward normalization is not needed at test time
eval_env.norm_reward = False

# Load the agent
model = A2C.load("a2c-AntBulletEnv-v0")

mean_reward, std_reward = evaluate_policy(model, env)

print(f"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}")
```

### åœ¨ Hub ä¸Šå‘å¸ƒä½ çš„è®­ç»ƒæ¨¡å‹ ğŸ”¥

ç°åœ¨æˆ‘ä»¬çœ‹åˆ°è®­ç»ƒåå–å¾—äº†ä¸é”™çš„æ•ˆæœï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€è¡Œä»£ç å°†è®­ç»ƒå¥½çš„æ¨¡å‹å‘å¸ƒåˆ° Hub ä¸Šã€‚

ğŸ“š libraries æ–‡æ¡£ğŸ‘‰ https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20

è¿™æ˜¯æ¨¡å‹å¡çš„ç¤ºä¾‹ï¼ˆä½¿ç”¨ PyBullet ç¯å¢ƒï¼‰ï¼š

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/modelcardpybullet.png" alt="Model Card Pybullet"/>

é€šè¿‡ä½¿ç”¨ `package_to_hub`, æ­£å¦‚æˆ‘ä»¬åœ¨ä¹‹å‰çš„å•å…ƒä¸­æåˆ°çš„ï¼Œ**è¯„ä¼°ã€å½•åˆ¶å›æ”¾ã€ç”Ÿæˆæ™ºèƒ½ä½“çš„æ¨¡å‹å¡å¹¶å°†å…¶æ¨é€åˆ°Hub**ã€‚

é€šè¿‡å¦‚ä¸‹æ–¹æ³•ï¼š

- ä½ å¯ä»¥**å±•ç¤ºæˆ‘ä»¬çš„å·¥ä½œæˆæœ**ğŸ”¥
- ä½ å¯ä»¥**å¯è§†åŒ–ä½ çš„æ™ºèƒ½ä½“è¿‡ç¨‹** ğŸ‘€
- ä½ å¯ä»¥**ä¸ç¤¾åŒºå…±äº«å…¶ä»–äººå¯ä»¥ä½¿ç”¨çš„æ™ºèƒ½ä½“**ğŸ’¾
- ä½ å¯ä»¥**è®¿é—®æ’è¡Œæ¦œ ğŸ† ä»¥æŸ¥çœ‹ä½ çš„æ™ºèƒ½ä½“ä¸ä½ çš„åŒå­¦ç›¸æ¯”çš„è¡¨ç°**ğŸ‘‰ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard

ä¸ºäº†èƒ½å¤Ÿä¸ç¤¾åŒºå…±äº«ä½ çš„æ¨¡å‹ï¼Œè¿˜éœ€è¦æ‰§è¡Œä¸‰ä¸ªæ­¥éª¤ï¼š

1ï¸âƒ£ ï¼ˆå¦‚æœå°šæœªå®Œæˆï¼‰åˆ›å»ºä¸€ä¸ª HuggingFace å¸æˆ· â¡ https://huggingface.co/join

2ï¸âƒ£ ç™»å½•ï¼Œç„¶åä½ éœ€è¦å­˜å‚¨æ¥è‡ª Hugging Face ç½‘ç«™çš„èº«ä»½éªŒè¯ä»¤ç‰Œã€‚

- åˆ›å»ºä¸€ä¸ªæ–°ä»¤ç‰Œ (https://huggingface.co/settings/tokens) **é€šè¿‡ä»¥ä¸‹æè¿°çš„æ–¹æ³•**

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg" alt="Create HF Token">

- å¤åˆ¶ä»¤ç‰Œ
- è¿è¡Œä¸‹é¢çš„å•å…ƒæ ¼å¹¶ç²˜è´´ä»¤ç‰Œ

```python
notebook_login()
!git config --global credential.helper store
```

å¦‚æœä½ ä¸æƒ³ä½¿ç”¨ Google Colab æˆ– Jupyter Notebookï¼Œåˆ™éœ€è¦æ”¹ç”¨æ­¤å‘½ä»¤ï¼š `huggingface-cli login`

3ï¸âƒ£ æˆ‘ä»¬ç°åœ¨å‡†å¤‡å¥½å°†è®­ç»ƒæœ‰ç´ çš„æ™ºèƒ½ä½“æ¨é€åˆ° ğŸ¤— Hub ğŸ”¥ ä½¿ç”¨ `package_to_hub()` å‡½æ•°

```python
package_to_hub(
    model=model,
    model_name=f"a2c-{env_id}",
    model_architecture="A2C",
    env_id=env_id,
    eval_env=eval_env,
    repo_id=f"ThomasSimonini/a2c-{env_id}",  # Change the username
    commit_message="Initial commit",
)
```

## å–æ¯å’–å•¡ä¼‘æ¯ä¸€ä¸‹å§ï¼ â˜•

- æ­å–œä½ å·²ç»è®­ç»ƒäº†ç¬¬ä¸€ä¸ªå­¦ä¼šç§»åŠ¨çš„æœºå™¨äºº ğŸ¥³!
- **è¯¥ä¼‘æ¯äº†**ã€‚ ä¸è¦çŠ¹è±«ï¼Œ**ä¿å­˜æ­¤ç¬”è®°æœ¬**â€œæ–‡ä»¶ > å°†å‰¯æœ¬ä¿å­˜åˆ°äº‘ç«¯ç¡¬ç›˜â€ï¼Œä»¥ä¾¿ç¨åå¤„ç†ç¬¬äºŒéƒ¨åˆ†ã€‚

## å¼€å‘ç¯å¢ƒ 2ï¼šPandaReachDense-v2 ğŸ¦¾

æˆ‘ä»¬è¦è®­ç»ƒçš„æ™ºèƒ½ä½“æ˜¯ä¸€ä¸ªéœ€è¦è¿›è¡Œæ§åˆ¶ï¼ˆç§»åŠ¨æ‰‹è‡‚å’Œä½¿ç”¨æœ«ç«¯æ‰§è¡Œå™¨ï¼‰çš„æœºæ¢°è‡‚ã€‚

åœ¨æœºå™¨äººæŠ€æœ¯ä¸­ï¼Œ*æœ«ç«¯æ‰§è¡Œå™¨*æ˜¯ä½äºæœºæ¢°è‡‚æœ«ç«¯çš„è£…ç½®ï¼Œæ—¨åœ¨ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚

åœ¨ `PandaReach`ä¸­, æœºå™¨äººå¿…é¡»å°†å…¶æœ«ç«¯æ‰§è¡Œå™¨æ”¾ç½®åœ¨ç›®æ ‡ä½ç½®ï¼ˆç»¿è‰²çƒï¼‰ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨æ­¤ç¯å¢ƒçš„å¯†é›†ç‰ˆæœ¬ã€‚ è¿™æ„å‘³ç€æˆ‘ä»¬å°†è·å¾—ä¸€ä¸ª*å¯†é›†çš„å¥–åŠ±å‡½æ•°*ï¼Œ**å°†åœ¨æ¯ä¸ªæ—¶é—´æ­¥æä¾›å¥–åŠ±**ï¼ˆæ™ºèƒ½ä½“è¶Šæ¥è¿‘å®Œæˆä»»åŠ¡ï¼Œå¥–åŠ±è¶Šé«˜ï¼‰ã€‚ ä¸*ç¨€ç–å¥–åŠ±å‡½æ•°*ç›¸åï¼Œå…¶ä¸­ç¯å¢ƒ**å½“ä¸”ä»…å½“ä»»åŠ¡å®Œæˆæ—¶æ‰è¿”å›å¥–åŠ±**ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨*æœ«ç«¯æ‰§è¡Œå™¨ä½ç§»æ§åˆ¶*ï¼Œè¿™æ„å‘³ç€**åŠ¨ä½œå¯¹åº”äºæœ«ç«¯æ‰§è¡Œå™¨çš„ä½ç§»**ã€‚ æˆ‘ä»¬ä¸æ§åˆ¶æ¯ä¸ªå…³èŠ‚çš„å•ç‹¬è¿åŠ¨ï¼ˆå…³èŠ‚æ§åˆ¶ï¼‰ã€‚

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/robotics.jpg"  alt="Robotics"/>

è¿™ç§æ–¹æ³•ä¼šä½¿å¾—**è®­ç»ƒä¼šæ›´å®¹æ˜“**ã€‚



åœ¨ `PandaReachDense-v2`ä¸­, æœºæ¢°è‡‚å¿…é¡»å°†å…¶æœ«ç«¯æ‰§è¡Œå™¨æ”¾ç½®åœ¨ç›®æ ‡ä½ç½®ï¼ˆç»¿è‰²çƒï¼‰ã€‚

```python
import gym

env_id = "PandaPushDense-v2"

# Create the env
env = gym.make(env_id)

# Get the state space and action space
s_size = env.observation_space.shape
a_size = env.action_space
```

```python
print("_____OBSERVATION SPACE_____ \n")
print("The State Space is: ", s_size)
print("Sample observation", env.observation_space.sample())  # Get a random observation
```

The observation space **æ˜¯ä¸€ä¸ªåŒ…å« 3 ä¸ªä¸åŒå…ƒç´ çš„å­—å…¸**:

- `achieved_goal`: (x,y,z) ç›®æ ‡çš„ä½ç½®ã€‚
- `desired_goal`: (x,y,z) ç›®æ ‡ä½ç½®å’Œå½“å‰å¯¹è±¡ä½ç½®ä¹‹é—´çš„è·ç¦»ã€‚
- `observation`: æœ«ç«¯æ‰§è¡Œå™¨çš„ä½ç½® (x,y,z) å’Œé€Ÿåº¦ (vx, vy, vz)ã€‚

é‰´äºå®ƒæ˜¯ä¸€ä¸ªä½œä¸ºè§‚æµ‹çš„å­—å…¸ï¼Œ**æˆ‘ä»¬å°†éœ€è¦ä½¿ç”¨ MultiInputPolicy è€Œä¸æ˜¯ MlpPolicy**ã€‚

```python
print("\n _____ACTION SPACE_____ \n")
print("The Action Space is: ", a_size)
print("Action Space Sample", env.action_space.sample())  # Take a random action
```

åŠ¨ä½œç©ºé—´æ˜¯ä¸€ä¸ªå…·æœ‰ 3 ä¸ªå€¼çš„å‘é‡ï¼š

- æ§åˆ¶ x, y, z ç§»åŠ¨

ç°åœ¨è½®åˆ°ä½ äº†ï¼š

1. å®šä¹‰åä¸ºâ€œPandaReachDense-v2â€çš„ç¯å¢ƒ
2. åˆ¶ä½œçŸ¢é‡åŒ–ç¯å¢ƒ
3. æ·»åŠ  wrapper ä»¥è§„èŒƒåŒ–è§‚å¯Ÿå’Œå¥–åŠ±ã€‚ [Check the documentation](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)
4. åˆ›å»º A2C æ¨¡å‹ï¼ˆä¸è¦å¿˜è®°ä½¿ verbose=1 æ¥æ‰“å°è®­ç»ƒæ—¥å¿—ï¼‰ã€‚
5. ä»¥ 1M æ—¶é—´æ­¥å¯¹å…¶è¿›è¡Œè®­ç»ƒ
6. ä¿å­˜æ™ºèƒ½ä½“æ—¶ä¿å­˜æ¨¡å‹å’Œ VecNormalize ç»Ÿè®¡ä¿¡æ¯
7. è¯„ä¼°ä½ çš„æ™ºèƒ½ä½“
8. åœ¨ Hub ä¸Šå‘å¸ƒä½ çš„è®­ç»ƒæ¨¡å‹ ğŸ”¥é€šè¿‡ä½¿ç”¨ `package_to_hub`å‡½æ•°

### è§£å†³æ–¹æ¡ˆ(å®Œæˆå¾…åŠäº‹é¡¹)

```python
# 1 - 2
env_id = "PandaReachDense-v2"
env = make_vec_env(env_id, n_envs=4)

# 3
env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.0)

# 4
model = A2C(policy="MultiInputPolicy", env=env, verbose=1)
# 5
model.learn(1_000_000)
```

```python
# 6
model_name = "a2c-PandaReachDense-v2"
model.save(model_name)
env.save("vec_normalize.pkl")

# 7
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize

# Load the saved statistics
eval_env = DummyVecEnv([lambda: gym.make("PandaReachDense-v2")])
eval_env = VecNormalize.load("vec_normalize.pkl", eval_env)

#  do not update them at test time
eval_env.training = False
# reward normalization is not needed at test time
eval_env.norm_reward = False

# Load the agent
model = A2C.load(model_name)

mean_reward, std_reward = evaluate_policy(model, env)

print(f"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}")

# 8
package_to_hub(
    model=model,
    model_name=f"a2c-{env_id}",
    model_architecture="A2C",
    env_id=env_id,
    eval_env=eval_env,
    repo_id=f"ThomasSimonini/a2c-{env_id}",  # TODO: Change the username
    commit_message="Initial commit",
)
```

## ä¸€äº›é¢å¤–çš„æŒ‘æˆ˜ ğŸ†

å­¦ä¹ **çš„æœ€ä½³æ–¹æ³•æ˜¯è‡ªå·±å°è¯•**ï¼ ä¸ºä»€ä¹ˆä¸ä¸º PyBullet å°è¯•â€œHalfCheetah Bullet Env-v0â€ï¼Œä¸º Panda-Gym å°è¯•â€œPandaPick Place-v1â€ï¼Ÿ

å¦‚æœä½ æƒ³ä¸º panda-gym å°è¯•æ›´é«˜çº§çš„ä»»åŠ¡ï¼Œä½ éœ€è¦æ£€æŸ¥ä½¿ç”¨ **TQC æˆ– SAC**ï¼ˆä¸€ç§æ›´é€‚åˆæœºå™¨äººä»»åŠ¡çš„æ ·æœ¬æ•ˆç‡æ›´é«˜çš„ç®—æ³•ï¼‰å®Œæˆäº†ä»€ä¹ˆã€‚ åœ¨çœŸå®çš„æœºå™¨äººæŠ€æœ¯ä¸­ï¼Œå‡ºäºä¸€ä¸ªç®€å•çš„åŸå› ï¼Œä½ å°†ä½¿ç”¨æ ·æœ¬æ•ˆç‡æ›´é«˜çš„ç®—æ³•ï¼šä¸æ¨¡æ‹Ÿè¿‡ç¨‹ç›¸å**å¦‚æœä½ å°†æœºæ¢°è‡‚ç§»åŠ¨å¤ªå¤šï¼Œåˆ™æœ‰æŸåå®ƒçš„é£é™©**ã€‚

PandaPickAndPlace-v1: https://huggingface.co/sb3/tqc-PandaPickAndPlace-v1

ä¸è¦çŠ¹è±«ï¼Œåœ¨è¿™é‡ŒæŸ¥çœ‹ panda-gym æ–‡æ¡£: https://panda-gym.readthedocs.io/en/latest/usage/train_with_sb3.html

ä»¥ä¸‹æ˜¯å®ç°æ­¤ç›®æ ‡çš„ä¸€äº›æƒ³æ³•ï¼š

- è®­ç»ƒæ›´å¤šæ­¥æ•°
- é€šè¿‡æŸ¥çœ‹å…¶ä»–åŒå­¦æ‰€åšçš„å·¥ä½œï¼Œå°è¯•ä¸åŒçš„è¶…å‚æ•° ğŸ‘‰ https://huggingface.co/models?other=https://huggingface.co/models?other=AntBulletEnv-v0
- **åœ¨ Hub ä¸Šæ¨é€ä½ æ–°è®­ç»ƒçš„æ¨¡å‹** ğŸ”¥

ç¬¬7å•å…ƒè§! ğŸ”¥

## ä¿æŒçƒ­çˆ±ï¼Œå¥”èµ´å±±æµ· ğŸ¤—