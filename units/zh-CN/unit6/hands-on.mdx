# åˆ©ç”¨ Panda-Gym è¿›è¡Œæœºå™¨äººæ¨¡æ‹Ÿçš„ä¼˜åŠ¿æ¼”å‘˜--è¯„è®ºå‘˜æ–¹æ³• (A2C) ğŸ¤–[[hands-on]]

```
  <CourseFloatingBanner classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb"}
    ]}
    askForHelpUrl="http://hf.co/join/discord" />
```

ç°åœ¨ä½ å·²ç»ç ”ç©¶äº†ä¼˜åŠ¿é˜¶æ¼”å‘˜--è¯„è®ºå‘˜æ–¹æ³• (A2C) èƒŒåçš„ç†è®ºï¼Œ**ä½ å·²å‡†å¤‡å¥½åœ¨æœºå™¨äººç¯å¢ƒä¸­ä½¿ç”¨ Stable-Baselines3 è®­ç»ƒä½ çš„ A2C æ™ºèƒ½ä½“**ã€‚ å¹¶è®­ç»ƒæœºå™¨äººï¼š


- ä¸€ä¸ªæœºå™¨äººæ‰‹è‡‚ğŸ¦¾ï¼Œä½¿å…¶ç§»åŠ¨åˆ°æ­£ç¡®çš„ä½ç½®ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨çš„æœºå™¨äººå¼€å‘ç¯å¢ƒï¼š

- [panda-gym](https://github.com/qgallouedec/panda-gym)


è¦éªŒè¯è®¤è¯è¿‡ç¨‹çš„å®é™…æ“ä½œï¼Œä½ éœ€è¦å°†ç»è¿‡è®­ç»ƒçš„æ¨¡å‹æ¨é€åˆ° Hub å¹¶è·å¾—ä»¥ä¸‹ç»“æœï¼š

- `PandaReachDense-v2` å¾—åˆ°çš„ç»“æœéœ€ >= -3.5.

è¦æ‰¾åˆ°ä½ çš„ç»“æœï¼Œ[è½¬åˆ°æ’è¡Œæ¦œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) å¹¶æ‰¾åˆ°ä½ çš„æ¨¡å‹ï¼Œ**ç»“æœ = å¹³å‡å¥–åŠ± - å¥–åŠ±æ ‡å‡†* *

æœ‰å…³è®¤è¯è¿‡ç¨‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æ­¤éƒ¨åˆ† ğŸ‘‰ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process

**è¦å¼€å§‹åŠ¨æ‰‹æ“ä½œï¼Œè¯·å•å‡»â€œåœ¨ Colab ä¸­æ‰“å¼€â€æŒ‰é’®** ğŸ‘‡ :

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit6/unit6.ipynb)

# ç¬¬ 6 å•å…ƒï¼šåˆ©ç”¨ Panda-Gym è¿›è¡Œæœºå™¨äººæ¨¡æ‹Ÿçš„é«˜é˜¶æ¼”å‘˜--è¯„è®ºå®¶æ–¹æ³• (A2C) ğŸ¤–

### ğŸ® å¼€å‘ç¯å¢ƒ:

- [Panda-Gym](https://github.com/qgallouedec/panda-gym)

### ğŸ“š å¼ºåŒ–å­¦ä¹ åº“:

- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/)

æˆ‘ä»¬ä¸€ç›´åœ¨åŠªåŠ›æ”¹è¿›æˆ‘ä»¬çš„æ•™ç¨‹ï¼Œæ‰€ä»¥ **å¦‚æœä½ åœ¨æ­¤ç¬”è®°æœ¬ä¸­å‘ç°ä¸€äº›é—®é¢˜**ï¼Œè¯· [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues).

## æœ¬notebookçš„ç›®æ ‡ ğŸ†

åœ¨æœ¬notebookçš„æœ€åï¼Œä½ å°†ï¼š

- æœ‰èƒ½åŠ›èƒ½å¤Ÿä½¿ç”¨ç¯å¢ƒåº“ **Panda-Gym**ã€‚
- èƒ½å¤Ÿ**ä½¿ç”¨ A2C** è®­ç»ƒæœºå™¨äººã€‚
- ç†è§£ä¸ºä»€ä¹ˆ**æˆ‘ä»¬éœ€è¦è§„èŒƒåŒ–è¾“å…¥**ã€‚
- èƒ½å¤Ÿ**å°†ä½ è®­ç»ƒæœ‰ç´ çš„æ™ºèƒ½ä½“å’Œä»£ç æ¨é€åˆ° Hub**ï¼Œå¹¶é™„å¸¦æœ‰æ¼‚äº®çš„è§†é¢‘å›æ”¾å’Œè¯„ä¼°åˆ†æ•° ğŸ”¥ã€‚

## å…ˆå†³æ¡ä»¶ğŸ—ï¸

åœ¨æ·±å…¥ç ”ç©¶notebookä¹‹å‰ï¼Œä½ éœ€è¦ï¼š

ğŸ”² ğŸ“š å­¦ä¹  [é˜…è¯»å¹¶å­¦ä¹ Unit6ä¸­çš„æ¼”å‘˜--è¯„è®ºå®¶æ–¹æ³•](https://huggingface.co/deep-rl-course/unit6/introduction) ğŸ¤—

# è®©æˆ‘ä»¬ä¸€èµ·è®­ç»ƒæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªæœºå™¨äººå§ ğŸ¤–

## è®¾ç½®å¥½GPU ğŸ’ª

- ä¸ºäº†**åŠ é€Ÿæ™ºèƒ½ä½“çš„è®­ç»ƒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ GPU**ã€‚ ä¸ºæ­¤ï¼Œè¯·è½¬åˆ° `Runtime > Change Runtime type`

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg" alt="GPU Step 1">

- `Hardware Accelerator > GPU`

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg" alt="GPU Step 2">

## åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿå±å¹• ğŸ”½

åœ¨notebookä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆé‡æ’­è§†é¢‘ã€‚ ä¸ºæ­¤ï¼Œä½¿ç”¨ colabï¼Œ**æˆ‘ä»¬éœ€è¦ä¸€ä¸ªè™šæ‹Ÿå±å¹•æ¥æ¸²æŸ“ç¯å¢ƒ**ï¼ˆä»è€Œè®°å½•å¸§ï¼‰ã€‚

å› æ­¤ï¼Œä»¥ä¸‹å•å…ƒå°†å®‰è£…åº“å¹¶åˆ›å»ºå’Œè¿è¡Œè™šæ‹Ÿå±å¹• ğŸ–¥

```python
%%capture
!apt install python-opengl
!apt install ffmpeg
!apt install xvfb
!pip3 install pyvirtualdisplay
```

```python
# Virtual display
from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()
```

### å®‰è£…ä¾èµ– ğŸ”½

ç¬¬ä¸€æ­¥æ˜¯å®‰è£…ä¾èµ–é¡¹ï¼Œæˆ‘ä»¬å°†å®‰è£…å¤šä¸ªä¾èµ–ï¼š

- `gymnasium`
- `panda-gym`: åŒ…å«æœºæ¢°è‡‚ç¯å¢ƒã€‚
- `stable-baselines3`: SB3 æ·±åº¦å¼ºåŒ–å­¦ä¹ åº“ã€‚
- `huggingface_sb3`: Stable-baselines3 çš„é™„åŠ ä»£ç ï¼Œç”¨äºä» Hugging Face ğŸ¤— Hub åŠ è½½å’Œä¸Šä¼ æ¨¡å‹ã€‚
- `huggingface_hub`: Libraryå…è®¸ä»»ä½•äººä½¿ç”¨ Hub çš„ä»“åº“ã€‚

```bash
!pip install stable-baselines3[extra]
!pip install gymnasium
!pip install huggingface_sb3
!pip install huggingface_hub
!pip install panda_gym
```

## å¯¼å…¥ç›¸å…³åŒ… ğŸ“¦

```python
import gymnasium as gym
import panda_gym

import os

from huggingface_sb3 import load_from_hub, package_to_hub

from stable_baselines3 import A2C
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.env_util import make_vec_env

from huggingface_hub import notebook_login
```

## PandaReachDense-v3 ğŸ¦¾

æˆ‘ä»¬å°†è®­ç»ƒçš„æ™ºèƒ½ä½“æ˜¯ä¸€ä¸ªéœ€è¦è¿›è¡Œæ§åˆ¶çš„æœºå™¨äººæ‰‹è‡‚ï¼ˆç§»åŠ¨æ‰‹è‡‚å¹¶ä½¿ç”¨æœ«ç«¯æ‰§è¡Œå™¨ï¼‰ã€‚
åœ¨æœºå™¨äººå­¦ä¸­ï¼Œ*æœ«ç«¯æ‰§è¡Œå™¨*æ˜¯å®‰è£…åœ¨æœºå™¨äººæ‰‹è‡‚æœ«ç«¯çš„è®¾å¤‡ï¼Œç”¨äºä¸ç¯å¢ƒäº’åŠ¨ã€‚
åœ¨ `PandaReach` ä¸­ï¼Œæœºå™¨äººå¿…é¡»å°†å…¶æœ«ç«¯æ‰§è¡Œå™¨æ”¾ç½®åœ¨ç›®æ ‡ä½ç½®ï¼ˆç»¿è‰²çƒï¼‰ã€‚
æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªç¯å¢ƒçš„å¯†é›†å‹ç‰ˆæœ¬ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å°†è·å¾—ä¸€ä¸ª*å¯†é›†å¥–åŠ±å‡½æ•°*ï¼Œå®ƒ**åœ¨æ¯ä¸ªæ—¶é—´æ­¥éƒ½ä¼šæä¾›å¥–åŠ±**ï¼ˆä»£ç†å®Œæˆä»»åŠ¡è¶Šæ¥è¿‘ï¼Œå¥–åŠ±è¶Šé«˜ï¼‰ã€‚è¿™ä¸*ç¨€ç–å¥–åŠ±å‡½æ•°*ä¸åŒï¼Œåè€…åªæœ‰åœ¨ä»»åŠ¡å®Œæˆæ—¶æ‰**è¿”å›å¥–åŠ±**ã€‚
åŒæ—¶ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨*æœ«ç«¯æ‰§è¡Œå™¨ä½ç§»æ§åˆ¶*ï¼Œè¿™æ„å‘³ç€**åŠ¨ä½œå¯¹åº”äºæœ«ç«¯æ‰§è¡Œå™¨çš„ä½ç§»**ã€‚æˆ‘ä»¬ä¸æ§åˆ¶æ¯ä¸ªå…³èŠ‚çš„å•ç‹¬è¿åŠ¨ï¼ˆå…³èŠ‚æ§åˆ¶ï¼‰ã€‚

![æœºå™¨äººå­¦](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/robotics.jpg)

é€šè¿‡è¿™ç§æ–¹å¼ï¼Œ**è®­ç»ƒå°†ä¼šæ›´åŠ å®¹æ˜“**ã€‚

### åˆ›å»ºç¯å¢ƒ

#### ç¯å¢ƒä¾èµ– ğŸ®

åœ¨ PandaReachDense-v3 ä¸­ï¼Œæœºå™¨äººæ‰‹è‡‚å¿…é¡»å°†å…¶æœ«ç«¯æ‰§è¡Œå™¨æ”¾ç½®åœ¨ç›®æ ‡ä½ç½®ï¼ˆç»¿è‰²çƒï¼‰

```python
env_id = "PandaReachDense-v3"
# Create the env
env = gym.make(env_id)

# Get the state space and action space
s_size = env.observation_space.shape
a_size = env.action_space
```

```python
print("_____OBSERVATION SPACE_____ \n")
print("The State Space is: ", s_size)
print("Sample observation", env.observation_space.sample())  # Get a random observation
```

è§‚å¯Ÿç©ºé—´**æ˜¯ä¸€ä¸ªå…·æœ‰3ä¸ªä¸åŒå…ƒç´ çš„å­—å…¸**ï¼š

- `Achased_goal`: (Xï¼ŒYï¼ŒZ)ç›®æ ‡çš„ä½ç½®ã€‚

- `desired_goal`: (xï¼Œyï¼Œz)ç›®æ ‡ä½ç½®ä¸å½“å‰å¯¹è±¡ä½ç½®ä¹‹é—´çš„è·ç¦»ã€‚

- `observation`: ä½ç½®(xï¼Œyï¼Œz)å’Œæœ€ç»ˆæ•ˆæœçš„é€Ÿåº¦(VXï¼ŒVYï¼ŒVZ)ã€‚

é‰´äºå®ƒæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œ**æˆ‘ä»¬éœ€è¦ä½¿ç”¨ MultiInputpolicy ç­–ç•¥è€Œä¸æ˜¯ Mlppolicy ç­–ç•¥**ã€‚

```python
print("\n _____ACTION SPACE_____ \n")
print("The Action Space is: ", a_size)
print("Action Space Sample", env.action_space.sample())  # Take a random action
```

åŠ¨ä½œç©ºé—´æ˜¯ä¸€ä¸ªå…·æœ‰3ä¸ªå€¼çš„å‘é‡ï¼š

- æ§åˆ¶ Xï¼ŒYï¼ŒZ è¿åŠ¨

### å½’ä¸€åŒ–è§‚æµ‹å’Œå¥–åŠ±

åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¸€ä¸ªè¾ƒå¥½çš„åšæ³•æ˜¯ [å½’ä¸€åŒ–è¾“å…¥ç‰¹å¾](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html).

ä¸ºæ­¤ï¼Œwrapperå¯ä»¥è®¡ç®—è¾“å…¥ç‰¹å¾çš„è¿è¡Œå¹³å‡å€¼å’Œæ ‡å‡†å·®ã€‚

æˆ‘ä»¬è¿˜é€šè¿‡æ·»åŠ ç›¸åŒçš„ wrapper æ¥è§„èŒƒåŒ–å¥–åŠ±å‚æ•° `norm_reward = True`

[You should check the documentation to fill this cell](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)

```python
env = make_vec_env(env_id, n_envs=4)

# Adding this wrapper to normalize the observation and the reward
env = # TODO: Add the wrapper
```

#### å‚è€ƒç­”æ¡ˆ

```python
env = make_vec_env(env_id, n_envs=4)

env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)
```

### åˆ›å»º A2C æ¨¡å‹ ğŸ¤–



æœ‰å…³ä½¿ç”¨ StableBaselines3 å®ç° A2C çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ï¼š https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes

ä¸ºäº†æ‰¾åˆ°æœ€ä½³å‚æ•°ï¼Œæ£€æŸ¥äº† [official trained agents by Stable-Baselines3 team](https://huggingface.co/sb3).

```python
model = # Create the A2C model and try to find the best parameters
```

#### å‚è€ƒç­”æ¡ˆ

```python
model = A2C(policy = "MultiInputPolicy",
            env = env,
            verbose=1)
```

### è®­ç»ƒ A2C æ™ºèƒ½ä½“ ğŸƒ

- è®©æˆ‘ä»¬ç”¨ 1,000,000 ä¸ªæ—¶é—´æ­¥è®­ç»ƒæˆ‘ä»¬çš„æ™ºèƒ½ä½“ï¼Œä¸è¦å¿˜è®°åœ¨ Colab ä¸Šä½¿ç”¨ GPUã€‚ å¤§çº¦éœ€è¦ 25-40 åˆ†é’Ÿ

```python
model.learn(1_000_000)
```

```python
# Save the model and  VecNormalize statistics when saving the agent
model.save("a2c-PandaReachDense-v3")
env.save("vec_normalize.pkl")
```

### è¯„ä¼°æ™ºèƒ½ä½“ ğŸ“ˆ

- ç°åœ¨æˆ‘ä»¬çš„æ™ºèƒ½ä½“å·²ç»è¿‡äº†è®­ç»ƒï¼Œæˆ‘ä»¬éœ€è¦**æ£€æŸ¥å…¶æ€§èƒ½**ã€‚
- Stable-Baselines3 æä¾›äº†ä¸€ç§æ–¹æ³•æ¥åšåˆ°è¿™ä¸€ç‚¹: `evaluate_policy`


```python
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize

# Load the saved statistics
eval_env = DummyVecEnv([lambda: gym.make("PandaReachDense-v3")])
eval_env = VecNormalize.load("vec_normalize.pkl", eval_env)

#  do not update them at test time
eval_env.training = False
# reward normalization is not needed at test time
eval_env.norm_reward = False

# Load the agent
model = A2C.load("a2c-PandaReachDense-v3")

mean_reward, std_reward = evaluate_policy(model, eval_env)

print(f"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}")
```

### åœ¨ Hub ä¸Šå‘å¸ƒä½ çš„è®­ç»ƒæ¨¡å‹ ğŸ”¥

ç°åœ¨æˆ‘ä»¬çœ‹åˆ°è®­ç»ƒåå–å¾—äº†ä¸é”™çš„æ•ˆæœï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€è¡Œä»£ç å°†è®­ç»ƒå¥½çš„æ¨¡å‹å‘å¸ƒåˆ° Hub ä¸Šã€‚

ğŸ“š libraries æ–‡æ¡£ğŸ‘‰ https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20


é€šè¿‡ä½¿ç”¨ `package_to_hub`, æ­£å¦‚æˆ‘ä»¬åœ¨ä¹‹å‰çš„å•å…ƒä¸­æåˆ°çš„ï¼Œ**è¯„ä¼°ã€å½•åˆ¶å›æ”¾ã€ç”Ÿæˆæ™ºèƒ½ä½“çš„æ¨¡å‹å¡å¹¶å°†å…¶æ¨é€åˆ°Hub**ã€‚

é€šè¿‡å¦‚ä¸‹æ–¹æ³•ï¼š

- ä½ å¯ä»¥**å±•ç¤ºæˆ‘ä»¬çš„å·¥ä½œæˆæœ**ğŸ”¥
- ä½ å¯ä»¥**å¯è§†åŒ–ä½ çš„æ™ºèƒ½ä½“è¿‡ç¨‹** ğŸ‘€
- ä½ å¯ä»¥**ä¸ç¤¾åŒºå…±äº«å…¶ä»–äººå¯ä»¥ä½¿ç”¨çš„æ™ºèƒ½ä½“**ğŸ’¾
- ä½ å¯ä»¥**è®¿é—®æ’è¡Œæ¦œ ğŸ† ä»¥æŸ¥çœ‹ä½ çš„æ™ºèƒ½ä½“ä¸ä½ çš„åŒå­¦ç›¸æ¯”çš„è¡¨ç°**ğŸ‘‰ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard

ä¸ºäº†èƒ½å¤Ÿä¸ç¤¾åŒºå…±äº«ä½ çš„æ¨¡å‹ï¼Œè¿˜éœ€è¦æ‰§è¡Œä¸‰ä¸ªæ­¥éª¤ï¼š

1ï¸âƒ£ ï¼ˆå¦‚æœå°šæœªå®Œæˆï¼‰åˆ›å»ºä¸€ä¸ª HuggingFace å¸æˆ· â¡ https://huggingface.co/join

2ï¸âƒ£ ç™»å½•ï¼Œç„¶åä½ éœ€è¦å­˜å‚¨æ¥è‡ª Hugging Face ç½‘ç«™çš„èº«ä»½éªŒè¯ä»¤ç‰Œã€‚

- åˆ›å»ºä¸€ä¸ªæ–°ä»¤ç‰Œ (https://huggingface.co/settings/tokens) **é€šè¿‡ä»¥ä¸‹æè¿°çš„æ–¹æ³•**

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg" alt="Create HF Token">

- å¤åˆ¶ä»¤ç‰Œ
- è¿è¡Œä¸‹é¢çš„å•å…ƒæ ¼å¹¶ç²˜è´´ä»¤ç‰Œ

```python
notebook_login()
!git config --global credential.helper store
```

å¦‚æœä½ ä¸æƒ³ä½¿ç”¨ Google Colab æˆ– Jupyter Notebookï¼Œåˆ™éœ€è¦æ”¹ç”¨æ­¤å‘½ä»¤ï¼š `huggingface-cli login`

3ï¸âƒ£ æˆ‘ä»¬ç°åœ¨å‡†å¤‡å¥½å°†è®­ç»ƒæœ‰ç´ çš„æ™ºèƒ½ä½“æ¨é€åˆ° ğŸ¤— Hub ğŸ”¥ ä½¿ç”¨ `package_to_hub()` å‡½æ•°

```python
package_to_hub(
    model=model,
    model_name=f"a2c-{env_id}",
    model_architecture="A2C",
    env_id=env_id,
    eval_env=eval_env,
    repo_id=f"ThomasSimonini/a2c-{env_id}",  # Change the username
    commit_message="Initial commit",
)
```

## ä¸€äº›é¢å¤–çš„æŒ‘æˆ˜ ğŸ†

å­¦ä¹ **çš„æœ€ä½³æ–¹æ³•æ˜¯è‡ªå·±å°è¯•**ï¼ ä¸ºä»€ä¹ˆä¸å°è¯•â€œ pandapickandplace-v3â€ï¼Ÿ


å¦‚æœä½ æƒ³å°è¯• panda-gym çš„æ›´é«˜çº§ä»»åŠ¡ï¼Œä½ éœ€è¦æŸ¥çœ‹ä½¿ç”¨ **TQC æˆ– SAC**ï¼ˆä¸€ç§æ›´é€‚åˆæœºå™¨äººä»»åŠ¡çš„æ ·æœ¬é«˜æ•ˆç®—æ³•ï¼‰æ‰€åšçš„å·¥ä½œã€‚åœ¨å®é™…çš„æœºå™¨äººå­¦ä¸­ï¼Œå‡ºäºä¸€ä¸ªç®€å•çš„åŸå› ï¼Œä½ ä¼šä½¿ç”¨æ›´é«˜æ•ˆçš„æ ·æœ¬ç®—æ³•ï¼šä¸æ¨¡æ‹Ÿä¸åŒï¼Œ**å¦‚æœä½ è¿‡åº¦ç§»åŠ¨ä½ çš„æœºå™¨äººæ‰‹è‡‚ï¼Œå°±æœ‰å¯èƒ½å¯¼è‡´å®ƒæŸå**ã€‚

PandaPickAndPlace-v1 (è¿™ä¸ªæ¨¡å‹ä½¿ç”¨v1ç‰ˆæœ¬ç¯å¢ƒ): https://huggingface.co/sb3/tqc-PandaPickAndPlace-v1

å¯¹äº†ï¼Œåˆ«å¿˜äº†æŸ¥çœ‹ panda-gym æ–‡æ¡£ï¼šhttps://panda-gym.readthedocs.io/en/latest/usage/train_with_sb3.html

æˆ‘ä»¬æä¾›äº†è®­ç»ƒå¦ä¸€ä¸ªæ™ºèƒ½ä½“çš„æ­¥éª¤ï¼ˆå¯é€‰ï¼‰ï¼š


1. å®šä¹‰åä¸ºâ€œPandaPickAndPlace-v3â€çš„ç¯å¢ƒ
2. åˆ¶ä½œçŸ¢é‡åŒ–ç¯å¢ƒ
3. æ·»åŠ  wrapper ä»¥è§„èŒƒåŒ–è§‚å¯Ÿå’Œå¥–åŠ±ã€‚ [Check the documentation](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)
4. åˆ›å»º A2C æ¨¡å‹ï¼ˆä¸è¦å¿˜è®°ä½¿ verbose=1 æ¥æ‰“å°è®­ç»ƒæ—¥å¿—ï¼‰ã€‚
5. ä»¥ 1M æ—¶é—´æ­¥å¯¹å…¶è¿›è¡Œè®­ç»ƒ
6. ä¿å­˜æ™ºèƒ½ä½“æ—¶ä¿å­˜æ¨¡å‹å’Œ VecNormalize ç»Ÿè®¡ä¿¡æ¯
7. è¯„ä¼°ä½ çš„æ™ºèƒ½ä½“
8. åœ¨ Hub ä¸Šå‘å¸ƒä½ çš„è®­ç»ƒæ¨¡å‹ ğŸ”¥é€šè¿‡ä½¿ç”¨ `package_to_hub`å‡½æ•°

### å‚è€ƒç­”æ¡ˆ(å¯é€‰)

```python
# 1 - 2
env_id = "PandaPickAndPlace-v3"
env = make_vec_env(env_id, n_envs=4)

# 3
env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)

# 4
model = A2C(policy="MultiInputPolicy",
             env=env,
             verbose=1)
# 5
model.learn(1_000_000)
```

```python
# 6
model_name = "a2c-PandaPickAndPlace-v3"
model.save(model_name)
env.save("vec_normalize.pkl")

# 7
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize

# Load the saved statistics
eval_env = DummyVecEnv([lambda: gym.make("PandaPickAndPlace-v3")])
eval_env = VecNormalize.load("vec_normalize.pkl", eval_env)

#  do not update them at test time
eval_env.training = False
# reward normalization is not needed at test time
eval_env.norm_reward = False

# Load the agent
model = A2C.load(model_name)

mean_reward, std_reward = evaluate_policy(model, eval_env)

print(f"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}")

# 8
package_to_hub(
    model=model,
    model_name=f"a2c-{env_id}",
    model_architecture="A2C",
    env_id=env_id,
    eval_env=eval_env,
    repo_id=f"ThomasSimonini/a2c-{env_id}",  # TODO: Change the username
    commit_message="Initial commit",
)
```

ç¬¬7å•å…ƒè§! ğŸ”¥

## ä¿æŒçƒ­çˆ±ï¼Œå¥”èµ´å±±æµ· ğŸ¤—