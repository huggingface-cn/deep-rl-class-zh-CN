# 介绍[[introduction]]

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/thumbnail.png"  alt="Thumbnail"/>

在第 4 单元中，我们了解了我们的第一个基于策略的算法，称为 **强化**。

在基于策略的方法中，**我们的目标是直接优化策略而不使用价值函数**。 更准确地说，Reinforce（强化） 是 *Policy-Based Methods* （基于策略的方法）子类的一部分，称为 *Policy-Gradient methods*（策略梯度方法）。 该子类通过**使用梯度上升**估计最优策略的权重来直接优化策略。

我们看到 Reinforce 运作良好。 然而，因为我们使用蒙特卡洛采样来估计回报（我们使用整个事件来计算回报），**我们在策略梯度估计方面存在显着差异**。

请记住，策略梯度估计是**回报增长最快的方向**。 换句话说，如何更新我们的政策权重，使带来良好回报的行动更有可能被采取。 我们将在本单元中进一步研究的蒙特卡洛方差**导致训练速度变慢，因为我们需要大量样本来减轻它**。

因此，今天我们将研究 **动作评论算法**，这是一种结合了基于价值和基于策略的方法的混合架构，通过减少方差来帮助稳定训练：

- *一个 人物* 控制 **我们的 智能体 的行为方式**（基于策略的方法）
- *评论家* 衡量 **采取的行动的好坏** (基于价值的方法)

我们将研究其中一种混合方法 **优势动作评论算法** (A2C)，**并在机器人环境中使用 Stable-Baselines3 训练我们的代理**。 我们将训练两个机器人：

- 一个蜘蛛🕷️正在学习如何移动。
- 一个机械臂 🦾 正在移动向正确的位置。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/environments.gif" alt="Environments"/>

听起来很令人兴奋？ 让我们开始吧！
