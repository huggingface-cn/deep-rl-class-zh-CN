# 自我对弈：在对抗性游戏中训练竞争智能体的经典技术

现在我们已经学习了多智能体的基础知识，我们准备深入研究。正如在介绍中提到的，我们将**在一个对抗性的游戏中训练智能体，使用 SoccerTwos 这个 2 对 2 的游戏**。

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/soccertwos.gif" alt="SoccerTwos"/>

<figcaption>环境由<a href="https://github.com/Unity-Technologies/ml-agents">Unity MLAgents 团队创建</a></figcaption>

</figure>

## 什么是自我对弈 ?

在对抗性游戏中正确训练智能体可能会变得**相当复杂**。

一方面，我们需要找到一个经过良好训练的对手来与训练中的智能体对战。另一方面，即使你有一个非常好的训练对手，这也不是一个好的解决方案，因为当对手太强时，你的智能体如何改进其策略呢？

想象一下一个刚开始学习足球的孩子。与一位非常优秀的足球运动员比赛是没有意义的，因为赢得比赛或者至少偶尔得到球将会非常困难。所以这个孩子会不断地输掉比赛，没有时间学习一个好的策略。

最好的解决方案是**找一个与智能体水平相当的对手，并且随着智能体提升自己的水平来提升对手的水平**。因为如果对手太强，我们将一无所获；如果对手太弱，我们将学到与更强对手对抗时无用的行为。

这个解决方案被称为*自我对弈*。在自我对弈中，**智能体将其之前的策略副本作为对手**。这样，智能体将与一个相同水平的对手进行对战（具有一定挑战性但不过于困难），有机会逐渐改进其策略，并在自身提升时更新对手。这是一种逐步提升对手复杂性的方法。

这与人类在竞争中学习的方式相同：

- 我们开始与一个水平相近的对手进行训练
- 然后我们从对手那里学习，当我们掌握了一些技巧后，我们可以挑战更强的对手。

我们在自我对弈中也是这样做的：

- 我们**以我们的智能体副本作为对手**开始，这样对手的水平就与我们相当。
- 我们**从对手那里学习**，当我们掌握了一些技巧后，我们**用最新的训练策略副本更新对手**。

自我对弈的理论并不是什么新鲜事物。早在上世纪五十年代，Arthur Samuel 的跳棋程序和 1995 年 Gerald Tesauro 的 TD-Gammon 就已经使用了自我对弈。如果你想了解更多关于自我对弈的历史，请参阅 Andrew Cohen 的这篇[非常好的博客文章](https://blog.unity.com/technology/training-intelligent-adversaries-using-self-play-with-ml-agents)

## MLAgents 中的自我对弈

自我对弈已经整合到了 MLAgents 库中，并由多个超参数进行管理，我们将对这些超参数进行研究。但正如文档中所解释的那样，主要关注点是**在最终策略的技能水平和广泛性以及学习的稳定性之间的权衡**。

与一组变化缓慢或不变的对手进行训练，且多样性较低，**会导致更稳定的训练。但如果变化过慢，存在过拟合的风险。**

因此，我们需要控制：

- 通过 `swap_steps` 和 `team_change` 参数控制**更换对手的频率**。
- 通过 `window` 参数控制**保存的对手数量**。较大的 `window` 值意味着智能体的对手池中包含了更多的行为多样性，因为它包含了训练过程中较早期的策略。
- 通过 `play_against_latest_model_ratio` 参数控制从池中采样时与**当前自身对战和对手对战的概率**。较大的 `play_against_latest_model_ratio` 值表示智能体将更频繁地与当前对手对战。
- 通过 `save_steps` 参数控制**在保存新对手之前的训练步数**。较大的 `save_steps` 值将产生一组覆盖更广泛的技能水平和可能的游戏风格的对手，因为策略接受了更多的训练。

要获取有关这些超参数的更多详细信息，你肯定需要[查看文档的这一部分](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Training-Configuration-File.md#self-play)。


## 用 ELO 分数评估我们的智能体

### 什么是 ELO 分数？

在对抗性游戏中，追踪累积奖励并不总是一个有意义的衡量学习进度的指标：因为这个指标仅仅取决于对手的技能水平。

相反，我们使用了一个名为*** ELO 评分系统***（以 Arpad Elo 命名），它可以计算在零和游戏中给定人群中两个玩家之间的**相对技能水平**。

在零和游戏中：一个智能体获胜，另一个智能体失败。这是一种数学表示，其中每个参与者的效用增益或损失**恰好被其他参与者的效用增益或损失所平衡**。我们之所以谈论零和游戏，是因为效用的总和等于零。

这个 ELO 分数（通常从一个特定的分数开始，比如 1200）可能一开始会降低，但在训练过程中应该逐渐增加。

Elo 系统是通过与其他玩家的失败和平局**进行推断得出的**。这意味着玩家的评级**取决于他们的对手以及对手得分的结果**。

Elo 定义了一个Elo 分数，它表示在零和游戏中的玩家相对技能水平。我们之所以说相对技能，**是因为它取决于对手的表现**。

中心思想是将玩家的表现视为一个**正态分布的随机变量**。

两个玩家之间的评级差异被视为**预测比赛结果的指标**。如果玩家获胜，但获胜的概率很高，那么它只会从对手那里获得少量分数，因为这意味着它比对手强得多。

每场比赛之后：

- 获胜的玩家从失败的玩家**夺取分数**。
- 分数的数量由两个玩家的**评级差异决定（因此是相对的）**。
    - 如果评级较高的玩家获胜→从评级较低的玩家那里获得少量分数。
    - 如果评级较低的玩家获胜→从评级较高的玩家那里获得很多分数。
    - 如果是平局→评级较低的玩家从评级较高的玩家那里获得少量分数。

所以如果 A 和 B 的评级分别为 Ra 和 Rb，那么**预期得分如下**：

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/elo1.png" alt="ELO Score"/>

然后，在比赛结束时，我们需要更新玩家的实际 Elo 分数。我们**使用一个与玩家表现超出或低于预期的程度成比例的线性调整**。

我们还定义了每场比赛的最大调整等级：K 因子。

- 对于高手，K=16。
- 对于较弱的玩家，K=32。

如果玩家 A 拥有 Ea 分，但获得了 Sa 分，那么玩家的评级将使用以下公式进行更新：

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/elo2.png" alt="ELO Score"/>

### 示例

如果我们举个例子：

玩家 A 的评级为 2600

玩家 B 的评级为 2300

- 我们首先计算预期得分：

\\(E_{A} = \frac{1}{1+10^{(2300-2600)/400}} = 0.849 \\)

\\(E_{B} = \frac{1}{1+10^{(2600-2300)/400}} = 0.151 \\)

- 如果组织者确定 K=16，并且 A 获胜，新的评级将为：

\\(ELO_A = 2600 + 16*(1-0.849) = 2602 \\)

\\(ELO_B = 2300 + 16*(0-0.151) = 2298 \\)

- 如果组织者确定 K=16，并且 B 获胜，新的评级将为：

\\(ELO_A = 2600 + 16*(0-0.849) = 2586 \\)

\\(ELO_B = 2300 + 16 *(1-0.151) = 2314 \\)


### 优点

使用 ELO 评分具有多个优点：

- 点数始终保持**平衡**（当出现意外结果时，交换的点数更多，但总和始终保持相同）。
- 这是一个**自我校正的系统**，因为如果一个玩家赢得了与一个弱对手的比赛，你只会赢得一些分数。
- 适用于**团队游戏**：我们计算每个团队的平均分数，并在 Elo 中使用它。

### 缺点

- ELO **不考虑每个人在团队中的个体贡献**。
- 评分通货紧缩：**为了获得相同的评分，需要随着时间的推移不断提升技能**。
- **无法比较历史上的评分**。

