# 基于模型的强化学习 (MBRL)


基于模型的强化学习（Model-based reinforcement learning）与无模型的强化学习方法在学习上的区别在于学习一个*动态模型*，但这对于决策过程产生了重要的影响。

动态模型通常用来建模环境的转换动态，即 \\( s_{t+1} = f_\theta (s_t, a_t) \\)，但在该框架中也可以使用逆动力学模型（从状态到动作的映射）或奖励模型（预测奖励）。

## 简单定义

- 存在一个智能体在不断尝试解决问题，**积累状态和动作数据**。
- 利用这些数据，智能体创建一个结构化学习工具，即*动态模型*，用于推理世界的状态。
- 利用动态模型，智能体通过预测未来**决定如何行动**。
- 通过这些行动，**智能体收集更多数据，改进模型，并希望改善未来的动作**。

## 学术定义

基于模型的强化学习（MBRL）遵循智能体在环境中相互作用、**学习环境模型**，并将模型用于控制（做出决策）的框架。

具体而言，智能体在由转移函数 \\( s_{t+1} = f (s_t , a_t) \\) 控制的马尔可夫决策过程（MDP）中进行行动，并在每一步返回奖励 \\( r(s_t, a_t) \\)。通过收集的数据集 \\( D :={ s_i, a_i, s_{i+1}, r_i} \\)，智能体学习一个模型 \\( s_{t+1} = f_\theta (s_t , a_t) \\) 来**最小化转换的负对数似然**。

我们采用基于样本的模型预测控制（Model-Predictive Control，MPC），利用学习到的动态模型，在从均匀分布 \\( U(a) \\) 中采样的一组动作上，对有限的、递归预测的时间范围 \\( \tau \\) 进行预期奖励的优化（详见[论文](https://arxiv.org/pdf/2002.04523)或[论文](https://arxiv.org/pdf/2012.09156.pdf)或[论文](https://arxiv.org/pdf/2009.01221.pdf)）。

## 补充阅读

如需了解有关基于模型的强化学习（MBRL）的更多信息，我们建议您查阅以下资源：

- 一篇关于[调试 MBRL 的博客](https://www.natolambert.com/writing/debugging-mbrl)。
- 一篇关于 MBRL 的[最新综述论文](https://arxiv.org/abs/2006.16712)。

## 作者

本节由<a href="https://twitter.com/natolambert"> Nathan Lambert </a>撰写。
