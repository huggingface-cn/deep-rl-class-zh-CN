# RLHF


基于人类反馈的强化学习(Reinforcement Learning from Human Feedback，RLHF)是一种**将人类数据标签融入基于强化学习的优化过程的方法论**。它的动机来自于**对建模人类偏好的挑战**。

对于许多问题而言，即使你可以尝试编写一个理想的方程，人们对偏好的看法也会有所不同。

**基于测量数据来更新模型是尝试缓解这些固有的人类机器学习问题的途径之一**。

## 开始学习 RLHF

要开始学习 RLHF，请按照以下步骤进行：

1. 阅读这篇介绍：[阐明从人类反馈中强化学习 (RLHF)](https://huggingface.co/blog/rlhf)。

2. 观看我们几周前的一次直播录像，其中 Nathan 介绍了强化学习从人类反馈中学习（RLHF）的基础知识，以及这项技术如何被用于实现像 ChatGPT 这样的最先进机器学习工具。这次讲座主要是关于相互关联的机器学习模型的概述。它涵盖了自然语言处理和强化学习的基础知识，以及如何在大型语言模型上使用 RLHF。最后，我们讨论了 RLHF 中的开放问题。

<Youtube id="2MBJOuVq380" />

3. 阅读关于这个主题的其他博客文章，例如 [闭源API与开源仍在继续：RLHF，ChatGPT，数据护城河](https://robotic.substack.com/p/rlhf-chatgpt-data-moats)。如果还有其他你喜欢的资源，请告诉我们！

## 补充阅读

*请注意，这是从上面的阐明 RLHF 博客文章中复制的内容。*
以下是迄今为止关于 RLHF 的最重要的论文列表。该领域在 DeepRL 的出现（大约在 2017 年左右）后近年来受到关注，并发展成为许多大型科技公司应用 LLM 的广泛研究。下面是一些早于 LM 关注点的 RLHF 论文：

- [TAMER: 通过评估强化手动训练智能体](https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/ICDL08-knox.pdf)（Knox 和 Stone，2008 年）：提出了一种学习智能体的方法，其中人类迭代地为采取的行动提供评分以学习奖励模型。
- [基于策略依赖的人类反馈的交互式学习](http://proceedings.mlr.press/v70/macglashan17a/macglashan17a.pdf)（MacGlashan 等，2017 年）：提出了一种演员-评论员算法 COACH，其中利用人类反馈（包括积极和消极反馈）来调整优势函数。
- [基于人类偏好的深度强化学习](https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html)（Christiano 等，2017 年）：在 Atari 轨迹偏好之间应用 RLHF 的方法。
- [Deep TAMER：在高维状态空间中进行交互式智能体塑形](https://ojs.aaai.org/index.php/AAAI/article/view/11485)（Warnell 等，2018 年）：扩展了 TAMER 框架，使用深度神经网络来建模奖励预测。


以下是一组不断增长的论文快照，展示了 RLHF 在 LM 领域的表现：
- [从人们偏好中进行微调语言模型](https://arxiv.org/abs/1909.08593) (Zieglar 等人，2019 年)：一篇早期的论文，研究了奖励学习对四个特定任务的影响。
- [通过人类反馈来学习总结](https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html) (Stiennon 等人，2020 年)：将 RLHF 应用于文本摘要任务。此外，[Recursively Summarizing Books with Human Feedback](https://arxiv.org/abs/2109.10862) (OpenAI Alignment Team 2021) 是对书籍进行摘要的后续工作。
- [WebGPT: 通过浏览器辅助的人类反馈进行问答](https://arxiv.org/abs/2112.09332) (OpenAI，2021 年)：使用 RLHF 训练智能体程序浏览网页。
- InstructGPT: [使用人类反馈来训练语言模型遵循指令](https://arxiv.org/abs/2203.02155) (OpenAI Alignment Team 2022)：将 RLHF 应用于通用语言模型[[InstructGPT 的博客文章](https://openai.com/blog/instruction-following/)]。
- GopherCite: [教授语言模型使用经过验证的引用来支持回答](https://www.deepmind.com/publications/gophercite-teaching-language-models-to-support-answers-with-verified-quotes) (Menick等人，2022年)：使用RLHF训练LM以返回带有特定引文的答案。
- Sparrow: [通过有针对性的人类判断改善对话智能体的对齐性](https://arxiv.org/abs/2209.14375) (Glaese 等人，2022 年)：使用 RLHF 对对话智能体进行微调，以提高其对齐性。
- [ChatGPT: 优化用于对话的语言模型](https://openai.com/blog/chatgpt/) (OpenAI，2022 年)：使用 RLHF 训练 LM，以适合作为通用聊天机器人使用。
- [奖励模型过度优化的规模定律](https://arxiv.org/abs/2210.10760) (Gao 等人，2022 年)：研究了 RLHF 中学习的偏好模型的缩放特性。
- [使用来自人类反馈的强化学习训练一个有用且无害的助手](https://arxiv.org/abs/2204.05862) (Anthropic，2022年)：详细记录了使用 RLHF 训练 LM 助手的过程。
- [对语言模型进行红队测试以减少伤害：方法、规模行为和经验教训](https://arxiv.org/abs/2209.07858) (Ganguli 等人，2022 年)：详细记录了努力“发现、衡量和尝试减少[语言模型]潜在有害输出”的过程。
- [使用强化学习进行开放式对话的动态规划](https://arxiv.org/abs/2208.02294) (Cohen 等人，2022 年)：使用 RL 来增强开放式对话智能体的交流能力。
- [强化学习在自然语言处理中的适用性：基准、基线和自然语言策略优化的构建模块](https://arxiv.org/abs/2210.01241) (Ramamurthy 和 Ammanabrolu 等人，2022 年)：讨论了 RLHF 中开源工具的设计空间，并提出了一种新的算法 NLPO（Natural Language Policy Optimization）作为 PPO 的替代方案。

## 作者

本节内容由 [Nathan Lambert](https://twitter.com/natolambert) 撰写。
