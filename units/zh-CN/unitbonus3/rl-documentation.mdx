# RL 文档介绍

在这个高级主题中，我们讨论一个问题：**我们应该如何监控和跟踪我们在现实世界中训练并与人类交互的强化学习智能体？**

随着机器学习系统对现代生活产生越来越大的影响，对这些系统进行文档化的需求也越来越大。

这样的文档可以涵盖诸如使用的训练数据 - 存储位置、收集时间、参与者等方面，或者模型优化框架 - 架构、评估指标、相关论文等方面，等等。

如今，模型卡片和数据表格越来越多地被提供。例如，在 Hub 上（参见文档[这里](https://huggingface.co/docs/hub/model-cards)）。

如果你点击 Hub 上的一个[热门模型](https://huggingface.co/models)，你可以了解它的创建过程。

这些模型和数据特定的日志是在创建模型或数据集时填写的，当这些模型被构建成未来的演进系统时，它们通常不会更新。

## 激励奖励报告的动机

强化学习系统的设计基于奖励和时间的测量进行优化。虽然奖励函数的概念可以很好地映射到许多熟悉的监督学习领域（通过损失函数），但对机器学习系统如何随时间演变的理解是有限的。

为此，作者介绍了[*强化学习的激励奖励报告*](https://www.notion.so/Brief-introduction-to-RL-documentation-b8cbda5a6f5242338e0756e6bef72af4)（这个简洁的命名旨在与流行的论文*模型报告的模型卡*和*数据集的数据表*相呼应）。
目标是提出一种以**奖励的人为因素**和**随时间变化的反馈系统**为重点的文档类型。

在 Mitchell 等人和 Gebru 等人提出的[模型卡](https://arxiv.org/abs/1810.03993)和[数据表](https://arxiv.org/abs/1803.09010)的文档框架基础上，我们认为有必要为人工智能系统提供激励奖励报告。

**激励奖励报告**是针对提议的强化学习部署的动态文档，用于界定设计选择。

然而，关于这一框架在不同强化学习应用中的适用性、系统可解释性的障碍以及部署的监督机器学习系统与强化学习中使用的序贯决策之间的共鸣，还有许多问题有待回答。

至少，激励奖励报告为强化学习从业者提供了一个讨论这些问题并开始解决实践中如何解决这些问题的工作的机会。

## 使用文档捕捉时间行为

针对强化学习和反馈驱动的机器学习系统设计的文档的核心部分是一个*变更日志*（change-log）。变更日志更新来自设计者的信息（如改变的训练参数、数据等），以及用户的反馈变化（如有害行为、意外响应等）。

变更日志配合更新触发器，鼓励监测这些效果。

## 贡献

一些最具影响力的强化学习驱动系统具有多利益相关者的性质，并且位于私营公司的封闭门后。这些公司很大程度上没有受到监管，因此文档的负担落在公众身上。

如果您有兴趣做出贡献，我们正在在[Github](https://github.com/RewardReports/reward-reports)上为流行的机器学习系统构建激励奖励报告的公共记录。

想要了解更多信息，请访问激励奖励报告的[论文](https://arxiv.org/abs/2204.10817)，或查看[示例报告](https://github.com/RewardReports/reward-reports/tree/main/examples)。

## 作者

本节由<a href="https://twitter.com/natolambert"> Nathan Lambert </a>撰写。
