# 词汇表 [[glossary]]

这是一个社区创建的词汇表。欢迎投稿！

### 智能体
智能体学会通过反复试验做出决定，并受到周围环境的奖励和惩罚。

### 环境
环境是一个模拟世界，智能体可以在其中通过交互来学习。

### 马尔可夫性质

这意味着我们的智能体采取的行动**仅以当前状态为条件，与过去的状态和行动无关**。

### 观测/状态

- **状态**：对世界状态的完整描述。
- **观测**：对环境/世界状态的部分描述。

### 动作

- **离散动作**：有限数量的动作，例如向左、向右、向上和向下。
- **连续动作**：动作有无限的可能性；例如，在自动驾驶汽车的情况下，驾驶场景有无限可能的动作发生。

### 奖励和折扣

- **奖励**：RL 中的基本元素。告诉智能体所采取的行动是好是坏。
- RL 算法专注于最大化**累积奖励**。
- **奖励假设**：RL 问题可以表述为（累积）回报的最大化。
- 执行**折扣**是因为在开始时获得的奖励更有可能发生，因为它们比长期奖励更可预测。

### 任务

- **回合制的**：有起点和终点。
- **持续的**：有起点但没有终点。

### 探索与利用的权衡

- **探索**：就是通过尝试随机行动并从环境中接收反馈/回报/奖励来探索环境。
- **利用**：利用我们对环境的了解以获得最大奖励。
- **探索-利用权衡**：它平衡了我们想要**探索**环境的程度和我们想要**利用**我们对环境的了解程度的程度。

### 策略

- **策略**：它被称为智能体的大脑。它告诉我们在给定状态下采取什么行动。
- **最优策略**：当智能体根据最优策略采取动作时，**会最大化期望回报**。它是通过*训练*学习的。

### 基于策略的方法：

- 一种解决 RL 问题的方法。
- 在这种方法中，直接学习策略。
- 将每个状态映射到该状态下的最佳对应动作。或者在该状态下可能的动作集合的概率分布。

### 基于价值的方法：

- 另一种解决强化学习问题的方法。
- 在这里，我们没有训练策略，而是训练了一个**价值函数**，它将每个状态映射到处于该状态的期望值。

欢迎投稿🤗

如果你想改进课程，可以[打开一个 Pull Request](https://github.com/huggingface/deep-rl-class/pulls)

由于以下原因，使该词汇表成为可能：

- [@lucifermorningstar1305](https://github.com/lucifermorningstar1305)
- [@daspartho](https://github.com/daspartho)
- [@misza222](https://github.com/misza222)