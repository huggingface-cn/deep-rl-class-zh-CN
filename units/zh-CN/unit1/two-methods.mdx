# 两种解决RL问题的主要方法[[两种方式]]

<Tip>
现在我们已经学习了 RL 框架，我们怎么解决 RL 问题呢？
</Tip>

换句话说，怎么构建一个 RL 智能体，让其能够**挑选最大化期望累计奖励的动作**

## π策略：智能体的大脑[[策略]]

**π**策略是**智能体的大脑**，它是告诉我们在给定状态下**采取什么行动的函数。**因此它**定义了智能体在给定时间的行为**。

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_1.jpg" alt="Policy" />
<figcaption>思考作为我们智能体大脑的策略，这个函数会告诉我们智能体的动作状态</figcaption>
</figure>

这个策略是**我们想要学习的函数**的，我们的目标是找到最优策略 π\* ，当智能体根据这个策略行动时返回**最大期望回报**。我们通过**训练**找到 π\*

这里有两种方式去训练我们的智能体找到最优策略π\*:

- **直接型** ，通过教授智能体学习**采取哪个动作**，并给出当前状态：**基于策略方法**
- **间接型** ，通过教授智能体学习哪个**状态更有价值**，并且采取动作**向更有价值的状态行进**：基于价值的方法

## 基于策略方法[[基于策略]]

在基于策略的方法中，**我们直接学习一个策略函数** 

这个函数会定义每个状态和其最佳对应动作之间的映射。我们也可以说它定义了**在那个状态下可能动作集合的概率分布。**

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_2.jpg" alt="Policy" />
<figcaption>如图所见, 策略 (确定的) <b>直接指示每一步该怎么走.</b></figcaption>
</figure>


我们有两类策略：


- *确定型*：给定一个状态，策略**返回同样的动作**

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_3.jpg" alt="Policy"/>
<figcaption>动作 = 策略(状态)</figcaption>
</figure>

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_4.jpg" alt="Policy" width="100%"/>

- *随机型*：结果是一个动作的概率分布

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_5.jpg" alt="Policy"/>
<figcaption>策略(动作 | 状态) = 给定当前状态下动作集合的概率分布</figcaption>
</figure>

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy-based.png" alt="Policy Based"/>
<figcaption>给定一个初始状态，我们的随机策略将输出该状态下可能动作的概率分布。</figcaption>
</figure>


回顾一下：

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/pbm_1.jpg" alt="Pbm recap" width="100%" />
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/pbm_2.jpg" alt="Pbm recap" width="100%" />


## 基于价值的方法[[基于价值]]

在基于价值的方法中，相比于训练一个策略函数，我们**学习一个价值函数**，其能将一个状态映射到**该状态**的期望价值。

状态的价值是智能体在**该状态开始，并根据策略行动所能得到的期望折扣回报**。

根据策略执行仅仅代表我们的策略是在那个状态是高价值的

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/value_1.jpg" alt="Value based RL" width="100%" />

这里我们看的我们的价值函数定义了**每个可能状态的价值**

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/value_2.jpg" alt="Value based RL"/>
<figcaption>多亏了我们的价值函数，在每一步，我们的策略都会选择价值函数定义的最大值的状态：-7 ，然后是 -6 ，然后是 -5 （依此类推）以达到目标。</figcaption>
</figure>

多亏了我们的价值函数，在每一步，我们的策略都会选择价值函数定义的最大价值的状态：-7 ，然后是 -6 ，然后是 -5 （以此类推）来达到目标。

回顾一下：

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/vbm_1.jpg" alt="Vbm recap" width="100%" />
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/vbm_2.jpg" alt="Vbm recap" width="100%" />
