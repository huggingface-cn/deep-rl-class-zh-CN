# 摘要 [[summary]]

这里有很多信息！让我们总结一下：

- 强化学习是一种从动作中学习的计算方法。我们构建了一个从环境中学习的智能体**通过试错与他交互**并接收奖励（负面或正面）作为反馈。

- 任何RL智能体的目标都是最大化其预期累积奖励（也称为预期回报），因为 RL 基于**奖励假设**，即**所有目标都可以描述为最大化预期累积奖励。**

- RL 过程是一个循环，该循环输出一个 **状态、动作、奖励 和 下一个状态的序列。**

- 为了计算预期的累积奖励（期望回报），我们对奖励进行折扣：较早出现的奖励（在游戏开始时）**更有可能发生，因为他们比长期的未来奖励更可预测。* *

- 要解决 RL 问题，你需要**找到最优策略**。该策略是你智能体的“大脑”，他将告诉我们**在给定状态下采取什么动作。**最优策略**为你提供了最大化期望回报的动作。**

- 有两种方法可以找到你的最佳策略：
    1. 通过直接训练你的策略：**基于策略的方法。**
    2. 通过训练一个价值函数来告诉我们智能体在每个状态下将获得的预期回报，并使用这个函数来定义我们的策略：**基于价值的方法。**

- 最后，我们谈到深度强化学习，因为我们引入了**深度神经网络来估计要采取的动作（基于策略）或估计状态的价值（基于价值）**因此得名“深度”。