# 词汇表 [[词汇表]]

这是一个社区创建的词汇表。欢迎投稿！

### 马尔可夫属性

这意味着我们的智能体采取的行动**仅以当前状态为条件，独立于过去的状态和行动**。

### 观察/状态

- **状态**：对世界状态的完整描述。
- **观察**：对环境/世界状态的部分描述。

### 动作

- **离散动作**：有限数量的动作，例如向左、向右、向上和向下。
- **连续动作**：动作的无限可能性；例如，在自动驾驶汽车的情况下，驾驶场景有无限可能的动作发生。

### 奖励和折扣

- **奖励**：RL 中的基本因素。告诉代理所采取的行动是好是坏。
- RL 算法专注于最大化**累积奖励**。
- **奖励假设**：RL 问题可以表述为（累积）回报的最大化。
- 执行**折扣**是因为在开始时获得的奖励更有可能发生，因为它们比长期奖励更可预测。

### 任务

- **情节**：有起点和终点。
- **连续**：有起点但没有终点。

### 探索与开发的权衡

- **探索**：就是通过尝试随机行动并从环境中接收反馈/回报/奖励来探索环境。
- **利用**：这是关于利用我们对环境的了解以获得最大回报。
- **探索-利用权衡**：它平衡了我们想要**探索**环境的程度和我们想要**利用**我们对环境的了解程度的程度。

### 策略

- **策略**：它被称为智能体的大脑。它告诉我们在给定状态下采取什么行动。
- **最优策略**：当智能体根据它行事时**最大化****预期回报**的策略。它是通过*培训*学习的。

### 基于策略的方法：

- 一种解决 RL 问题的方法。
- 在这种方法中，直接学习策略。
- 将每个状态映射到该状态下的最佳对应动作。或者在该状态下一组可能的动作的概率分布。

### 基于价值的方法：

- 另一种解决强化学习问题的方法。
- 在这里，我们没有训练策略，而是训练了一个**价值函数**，它将每个状态映射到处于该状态的期望值。

欢迎投稿🤗

如果你想改进课程，可以[打开一个 Pull Request](https://github.com/huggingface/deep-rl-class/pulls)

由于以下原因，使该词汇表成为可能：

- [@lucifermorningstar1305](https://github.com/lucifermorningstar1305)
- [@daspartho](https://github.com/daspartho)
- [@misza222](https://github.com/misza222)