# 训练你的第一个深度强化学习智能体 🤖 [[hands-on]]




      <CourseFloatingBanner classNames="absolute z-10 right-0 top-0"
      notebooks={[
        {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit1/unit1.ipynb"}
        ]}
        askForHelpUrl="http://hf.co/join/discord" />

现在你已经学习了强化学习的基础知识，你已经准备好训练你的第一个智能体并通过 Hub 与社区分享它 🔥：
你将学会正确登陆月球的月球着陆器智能体🌕

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/lunarLander.gif" alt="LunarLander">

最后，你将**将这个训练有素的智能体上传到 Hugging Face Hub 🤗，这是一个免费的开放平台，人们可以在其中共享 ML 模型、数据集和演示。**

感谢 <a href="https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard">leaderboard</a>, 你将能够与其他同学比较你的结果并交流最佳实践以提高智能体的分数。谁将赢得第 1 单元 🏆 的挑战？

为了手动验证[认证过程](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process), 你需要将你训练好的模型推送到Hub，并获得结果 >=200.

找到你的结果 去 [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) 找到你的模型, **结果 = 平均奖励 - 标准差奖励**

**如果你找不到你的模型，请转到页面底部并点击刷新按钮。**

关于更多认证过程点击右边 👉 https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process

同时你可以查看你的过程 👉 https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course

让我们开始吧! 🚀

**点击下方按钮开始** 👇 :

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit1/unit1.ipynb)

我们强烈**建议学生使用 Google Colab 进行动手练习**，而不是在个人电脑上运行它们。

通过使用 Google Colab，**你可以专注于学习和实验，而不用担心设置环境的技术问题**。

# 第 1 单元：训练你的第一个深度强化学习智能体 🤖

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/thumbnail.jpg" alt="Unit 1 thumbnail" width="100%">

在此笔记本中，你将训练你的**第一个深度强化学习智能体**一个月球着陆器智能体，它将学习**正确降落在月球上🌕**。使用 [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/) 深度强化学习库，与社区分享，并尝试不同的配置


### 环境配置 🎮

- [LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/)

### 使用的库 📚

- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)

我们一直在努力改进我们的教程，所以**如果你在此笔记本中发现一些问题**，请[在 Github Repo 上打开一个问题](https://github.com/huggingface/deep-rl-class/issues)

## 本 notebook 的目标 🏆

本章结束你将会得到以下知识：

- 学会使用环境库 **Gymnasium**。
- 学会使用深度强化学习库 **Stable-Baselines3**.
- 学会使用把你的**智能体推送到仓库**并同时展示视频和评估分数 🔥.


## 本 notebook 来源于深度学习教程

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg" alt="Deep RL Course illustration"/>

在本免费教程中你将会得到：

- 📖 通过**理论和练习**学习深度强化学习
- 🧑‍💻 学会使用**经典的强化学习库**像 Stable Baselines3, RL Baselines3 Zoo, CleanRL 和 Sample Factory 2.0.
- 🤖 在特定环境训练**智能体**
- 🎓 完成80%的作业后，**获得完成证书**。

还有更多

请查看 📚 教学大纲 👉 https://simoninithomas.github.io/deep-rl-course

别忘了 **<a href="http://eepurl.com/ic5ZUD">注册本课程</a>** (我们正在收集你的电子邮件，以便能够在发布每个单元时向**你发送链接，并为你提供有关挑战和更新的信息).**

最好的与我们交了保持联系互动的方式是加入我们的discord社区 👉🏻 https://discord.gg/ydHrjt3WP5

## 前置准备 🏗️

在开始前你需要:

🔲 📝 **[阅读单元 0](https://huggingface.co/deep-rl-course/unit0/introduction)** 给你了所有关于课程的信息相关的须知并帮助你轻松开始🤗

🔲 📚 通过阅读[单元 1](https://huggingface.co/deep-rl-course/unit1/introduction)来**熟悉对强化学习基础（ MC ， TD ，奖励假设...）的理解**.

## 深度强化学习的回顾 📚
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg" alt="The RL process" width="100%">

让我们对我们在第一个单元中学到的知识进行小回顾：

- 强化学习是一种**从动作学习**的计算方法。我们建立了一个智能体，通过**通过试错**与环境进行互动，并获得奖励（负或正面）作为反馈。

- 任何 RL 智能体的目标是**最大化其期望的累积奖励**（也称为期望回报），因为 RL 基于 _奖励假设_，即所有目标都可以描述为最大化期望累积奖励。

- RL 过程是一个**循环，该循环输出一个 **状态、动作、奖励 和 下一个状态的序列。**


- 为了计算预期的累积奖励（期望回报），我们对奖励进行折扣：较早出现的奖励（在游戏开始时）**更有可能发生，因为它们比长期的未来奖励更可预测。* *

- 要解决 RL 问题，你需要**找到最优策略**。该策略是你智能体的“大脑”，它将告诉我们**在给定状态下采取什么动作。**最优策略**为你提供了最大化期望回报的动作。**

有两种方法可以找到你的最佳策略：

- 通过直接训练你的策略：**基于策略的方法。**
- 通过训练一个价值函数来告诉我们智能体在每个状态下将获得的预期回报，并使用这个函数来定义我们的策略：**基于价值的方法。**

- 最后，我们谈到深度强化学习，因为我们引入了**深度神经网络来估计要采取的动作（基于策略）或估计状态的价值（基于价值），因此得名“深度”。**

# 让我们训练我们的第一个深度强化学习智能体，然后将其上传到 hub 🚀

## 获得证书 🎓

为了验证这个实践练习并通过[认证流程](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)，你需要将你训练好的模型推送到 Hub 并且**获得大于等于 200 的结果**。

要查看你的结果，请前往[排行榜](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)并找到你的模型，**结果 = 平均奖励 - 奖励的标准差**

有关认证流程的更多信息，请查看此部分👉 https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process


## 设置 GPU 💪

- 为了**加速智能体训练,我们将会用到 GPU **。这里设置 `Runtime > Change Runtime type`

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg" alt="GPU Step 1">

- `Hardware Accelerator > GPU`

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg" alt="GPU Step 2">

## 安装依赖并创建虚拟屏幕 🔽
第一步是安装相关依赖，我们将会安装多项

- `gymnasium[box2d]`: 包含 Lunarlander-V2 环境🌛。
- `stable-baselines3[extra]`: 深度强化学习库。
- `huggingface_sb3`: 稳定的 baselines3 的其他代码，可从 huggingface🤗 仓库上加载和上传模型。

为了简化操作，我们创建了一个脚本安装所有依赖


```bash
apt install swig cmake
```


```bash
pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt
```

在 notebook 上，我们需要生成一个重播视频。为此，使用 Colab ，**我们需要有一个虚拟屏幕才能渲染环境**（从而记录帧）。

因此，以下单元格将安装虚拟屏幕库，并创建并运行虚拟屏幕🖥


```bash
sudo apt-get update
apt install python-opengl
apt install ffmpeg
apt install xvfb
pip3 install pyvirtualdisplay
```

为了确保使用新的已安装库，**有时需要重新启动笔记本运行时**。下一个单元将迫使**运行时崩溃，因此你需要再次连接并从这里开始运行代码。感谢这个技巧，**我们将能够运行虚拟屏幕。**


```python
import os

os.kill(os.getpid(), 9)
```


```python
# Virtual display
from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()
```

## 导入相关包 📦

我们导入的另一个库是 huggingface_hub **，能够从仓库上传和下载训练有素的模型**。


huggingface 仓库🤗是任何人都可以共享和探索模型和数据集的中心地点。它具有版本控制，指标，可视化和其他功能，可让你轻松与他人合作

这里可以看到所有可用的深度强化学习模型 👉 https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads




```python
import gymnasium

from huggingface_sb3 import load_from_hub, package_to_hub
from huggingface_hub import (
    notebook_login,
)  # To log to our Hugging Face account to be able to upload models to the Hub.

from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.monitor import Monitor

```

## 理解什么是 Gymnasium 并且他怎么工作的🤖

🏋 包含我们运行环境的库叫 Gymnasium.
**你将在深度强化学习领域多次用到该库**

Gymnasium 库提供了两个事情：
- 一个交互界面允许你**创建强化学习环境**.
- 一个**环境集合** ( gym-control, atari, box2D...).

让我们看一个例子，首先我们了解一下什么是 RL 循环.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg" alt="The RL process" width="100%">

每一步:
- 我们的智能体从**环境**接收**状态 S0** - 我们将收到游戏的第一帧（环境）。
- 基于**状态 S0，**智能体采取 **动作 A0** - 我们的智能体将移动到右边
- 环境将变成一个**新**的**状态 S1**（新的一帧）.
- 环境给智能体提供了一些**奖励 R1 ** - 我们还没有死*（正奖励 +1）*。.


通过 Gymnasium:

1️⃣ 我们使用  `gymnasium.make()`  创建环境

2️⃣ 我们使用  `observation = env.reset()` 重设环境初始状态

在每一步:

3️⃣ 使用模型获取动作(我们的例子里使用随机动作)

4️⃣ 使用  `env.step(action)` , 我们展示了动作在环境中怎么获得
- `observation`: 新状态  (st+1)
- `reward`: 执行动作后的奖励
- `terminated`：表示该回合是否终止（智能体到达最终状态）
- `truncated`：在这个新版本中引入，它表示时间限制或者代理是否超出环境的边界。
- `info`：提供额外信息的字典（取决于环境）。

有关更多解释，请查看这里👉 https://gymnasium.farama.org/api/env/#gymnasium.Env.step

如果这个回合结束:
- 我们重设环境初始状态  `observation = env.reset()`

**让我们看一个例子!** 确保你读了代码

```python
import gymnasium as gym

# 首先，我们创建一个叫 LunarLander-v2 的环境
env = gym.make("LunarLander-v2")

# 然后我们重置一下环境
observation, info = env.reset()

for _ in range(20):
    # Take a random action
    action = env.action_space.sample()
    print("Action taken:", action)

    # Do this action in the environment and get
    # next_state, reward, terminated, truncated and info
    observation, reward, terminated, truncated, info = env.step(action)

    # If the game is terminated (in our case we land, crashed) or truncated (timeout)
    if terminated or truncated:
        # Reset the environment
        print("Environment is reset")
        observation, info = env.reset()

env.close()
```

## 创建月球发射器环境🌛并了解其工作原理

### [环境 🎮](https://www.gymlibrary.dev/environments/box2d/lunar_lander/)
在本教程中, 我们要训练一个智能体, 一个**正确着陆在月球**的 [月球发射器](https://gymnasium.farama.org/environments/box2d/lunar_lander/) . 为此，智能体需要学习**以适应其速度和位置（水平，垂直和角度）才能正确降落。**

---


💡 启动环境并检查相关文档是一个好习惯 

👉 https://gymnasium.farama.org/environments/box2d/lunar_lander/

---


让我们看看环境啥样


```python
# We create our environment with gym.make("<name_of_the_environment>")
env = gym.make("LunarLander-v2")
env.reset()
print("_____OBSERVATION SPACE_____ \n")
print("Observation Space Shape", env.observation_space.shape)
print("Sample observation", env.observation_space.sample()) # Get a random observation
```

我们看到 `Observation Space Shape (8,)`是一个尺寸 8 的向量，其中每个值包含有关着陆器的不同信息：
- 水平垫坐标（ x ）
- 垂直垫坐标（ y ）
- 水平速度（ x ）
- 垂直速度（ y ）
- 角度
- 角速度
- 如果左腿有接触点触及土地
- 如果右腿有接触点触及土地



```python
print("\n _____ACTION SPACE_____ \n")
print("Action Space Shape", env.action_space.n)
print("Action Space Sample", env.action_space.sample()) # Take a random action
```

动作空间（智能体可以采取的可能的操作集）是离散的，有 4 个可用的操作🎮：

- 动作 0：没做什么，
- 动作 1：点火左定向引擎，
- 动作 2：发射主引擎，
- 动作 3：点火右向定向引擎。。

奖励函数（在每个时间步中都会获得奖励的函数）💰：

每一步都会获得奖励。一个回合的总奖励是**该回合内所有步骤的奖励之和**。

对于每一步，奖励：

- 随着着陆器离着陆垫越近/越远而增加/减少。
- 随着着陆器移动得越慢/越快而增加/减少。
- 随着着陆器倾斜得越多（角度不水平）而减少。
- 每个与地面接触的腿增加 10 分。
- 每帧侧面发动机点火减少 0.03 分。
- 每帧主发动机点火减少 0.3 分。

回合**因坠毁或安全着陆分别获得额外的 -100 或 +100 分的奖励**。

如果一个回合**得分至少 200 分，则认为它是一个解决方案**。

#### 向量化环境

- 我们创建了一个由 16 个环境组成的向量化环境（一种将多个独立环境堆叠到一个环境中的方法），这样，在训练期间我们将拥有更多样的经验。


```python
# 创建环境
env = make_vec_env('LunarLander-v2', n_envs=16)
```

## 创建模型🤖

- 现在我们研究了我们的环境，并了解了问题：**能够通过控制左，右和主要方向引擎**将月球着陆器正确地降落到着陆垫上。让我们构建我们将用来解决此问题的算法。

- 为此，我们将使用第一个深度 RL 库[stable Baselines3（sb3）]（https://stable-baselines3.readthedocs.io/en/master/）。

- SB3 是 PyTorch **中强化学习算法的一组可靠的实现**。

---
💡使用库时，一个好习惯是首先查看文档： https：//stable-baselines3.readthedocs.io/en/master/ ，然后尝试一些教程。

---

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sb3.png" alt="Stable Baselines3">

为了解决这个问题，我们将使用 SB3 ** PPO **。[PPO（又称近端策略优化）是你在本课程中你将研究的深度强化学习 SOTA 算法之一](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D)。

PPO 结合了下面：

- *基于价值的强化学习方法*：学习一个动作价值函数，该功能将告诉我们**给定一个状态和动作时，最有价值的行动**。
- *基于策略的强化学习方法*：学习一个策略，该策略能够给我们动作的概率分布**。

Stable-Baselines3 简易设置:

1️⃣  **创建环境** (上文已完成)

2️⃣ 定义 **你想用的模型并且实例化** `model = PPO("MlpPolicy")`

3️⃣ 通过 `model.learn` 训练你的智能体并且定义训练步数

```python
# 创建环境
env = gym.make('LunarLander-v2')

# 初始化智能体
model = PPO('MlpPolicy', env, verbose=1)
# 训练智能体
model.learn(total_timesteps=int(2e5))
```


```python
# 目标: 定义一个 PPO MLP 策略结构
# 因为输入是一个向量我们使用 MLP
# 如果我们输入是帧，我们将使用 CNN 策略
model = 
```

#### 解决方案


```python
# 解决方案
# 我们添加了一些参数去加速训练
model = PPO(
    policy = 'MlpPolicy',
    env = env,
    n_steps = 1024,
    batch_size = 64,
    n_epochs = 4,
    gamma = 0.999,
    gae_lambda = 0.98,
    ent_coef = 0.01,
    verbose=1)
```

##训练 PPO 智能体🏃

- 让我们训练我们的智能体以1,000,000个时间步长训练，不要忘记在 COLAB 上使用 GPU 。大约需要约 20 分钟，但是如果你只想尝试一下，就可以使用更少的时间步数。

- 在训练期间，休息一下🤗


```python
# 目标: 训练 1,000,000 时间步长

# 目标: 改名字并将模型存入文件
model_name = ""

```

#### 解决方案


```python
# 解决方案
# 训练 1,000,000 时间步长
model.learn(total_timesteps=1000000)
# 保存模型
model_name = "ppo-LunarLander-v2"
model.save(model_name)
```

## 评估智能体📈

- 现在我们的月球发射器智能体接受了训练🚀，我们需要**检查其性能**。

- 稳定的 baselines3 提供了一种方法：`evaluate_policy`。

- 要完成该部分，你需要[查阅文档]（https://stable-baselines3.readthedocs.io/en/master/guide/guide/guide/examples.html#basic-usage-usage-usage-usage-training-saving-loading）

- 在下一步中，我们将看到**如何自动评估和分享你的智能体在排行榜中竞争，但现在让我们自己做**

💡当你评估智能体时，你不应使用训练环境，而应创建评估环境。


```python
# 目标: 评估智能体
# 为评估创建一个新环境
eval_env =

# 用 10 个评估回合和 deterministic=True 的条件去评估模型
mean_reward, std_reward = 

# 打印结果


```

#### 解决方案


```python
#@title
eval_env = Monitor(gym.make("LunarLander-v2"))
mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)
print(f"mean_reward={mean_reward:.2f} +/- {std_reward}")
```

- 就我而言，在训练 100 万步之后，我获得了平均奖励是 `200.20 +/- 20.80` ，这意味着我们的月球着陆器（Lunar Lander）智能体准备在月球上降落。

## 在仓库上发布我们训练的模型🔥

现在，我们看到训练后我们获得了良好的成绩，我们可以用一条代码在仓库上发布已经训练的模型。

📚库文档👉https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face-face--x-stable-baselines3-v20

这是模型卡片的示例（带有太空入侵者）：

通过使用 `package_to_hub` **你评估，记录一个重演，生成智能体的模型卡片并将其推到仓库**。

这边：

- 你可以**展示我们的作品**🔥

- 你可以**可视化你的智能体**👀

- 你可以**与社区分享其他人可以使用**💾的智能体

- 你可以**在 leaderboard 🏆查看与同学相比你的智能体的表现如何** 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard

为了能够与社区分享你的模型，还有三个步骤可以遵循：

1️⃣（如果尚未完成）创建一个帐户到 HF https://huggingface.co/join

2️⃣ 登录，然后你需要从 huggingface 网站存储身份验证令牌。

- 创建一个新的令牌（https://huggingface.co/settings/tokens）**带有写权限**

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg" alt="Create HF Token">

- 复制令牌
- 在下面运行单元格并粘贴令牌


```python
notebook_login()
!git config --global credential.helper store
```

如果你不想使用 Google Colab 或 Jupyter 笔记本，则需要使用此命令：`huggingface-cli login`

3️⃣我们现在使用 `package_to_hub（）` 函数将训练的智能体推到 hunggingface🤗 仓库🔥

让我们填写 `package_to_hub` 函数：

- `model`：我们训练有素的模型。

- `model_name`：我们在``model_save''中定义的训练模型的名称

- `model_architecture`：我们使用的模型体系结构：在我们的情况下的PPO

- `env_id`：环境的名称，在我们的情况下`lunarlander-v2`

- `eval_env`：eval_env中定义的评估环境

- `repo_id`：将创建/更新的huggingface存储库的名称`（repo_id = {username}/{repo_name}）`

**良好起名规范{ username }/{ model_architecture } - {env_id}**

- `commit_message`：提交的消息


```python
import gymnasium as gym
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.env_util import make_vec_env

from huggingface_sb3 import package_to_hub

## 目标: 定义一个 repo_id
## repo_id 是 Hugging Face Hub 中模型仓库的 id（例如，repo_id = {organization}/{repo_name}，例如 ThomasSimonini/ppo-LunarLander-v2）repo_id = 

# 目标: 定义环境名字
env_id = 

# 创建一个评估环境并设置 render_mode="rgb_array"
eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode="rgb_array")])


# 目标: 定义我们使用的模型架构
model_architecture = ""

## 目标: 定义提交信息
commit_message = ""

# 在将仓库推送到 Hub 之前，使用 save、evaluate 方法，生成模型卡片并记录你的智能体的重播视频
package_to_hub(model=model, # Our trained model
               model_name=model_name, # The name of our trained model 
               model_architecture=model_architecture, # The model architecture we used: in our case PPO
               env_id=env_id, # Name of the environment
               eval_env=eval_env, # Evaluation Environment
               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2
               commit_message=commit_message)

# 注意：如果在运行package_to_hub函数后出现重定位问题，请运行以下代码
# cd <path_to_repo> && git add . && git commit -m "Add message" && git pull 
# 最后不要忘记执行“git push”将更改推送到Hub。
```

#### 解决方法


```python
import gymnasium as gym

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.env_util import make_vec_env

from huggingface_sb3 import package_to_hub

# 将你刚才在上面两个单元格中定义的变量放在这里
# 定义环境的名称
env_id = "LunarLander-v2"

# 目标: 定义我们使用的模型架构
model_architecture = "PPO"

## 定义一个 repo_id
## repo_id 是 Hugging Face Hub 中模型仓库的 id（例如，repo_id = {organization}/{repo_name}，例如 ThomasSimonini/ppo-LunarLander-v2）
## 请更改为你的 REPO ID
repo_id = "ThomasSimonini/ppo-LunarLander-v2" # Change with your repo id, you can't push with mine 😄

## 目标: 定义提交信息
commit_message = "Upload PPO LunarLander-v2 trained agent"

# 创建一个评估环境并设置 render_mode="rgb_array"
eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode="rgb_array")])

# 将你刚才填写的 package_to_hub 函数放在这里

package_to_hub(model=model, # Our trained model
               model_name=model_name, # The name of our trained model 
               model_architecture=model_architecture, # The model architecture we used: in our case PPO
               env_id=env_id, # Name of the environment
               eval_env=eval_env, # Evaluation Environment
               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2
               commit_message=commit_message)

```

恭喜🥳你刚刚训练并上传了第一个深度强化学习智能体。上面的脚本应显示指向模型存储库的链接，例如 https://huggingface.co/osanseviero/test_sb3。当你转到此链接时，你可以：

- 在右侧查看你的智能体的视频预览。

- 单击“文件和版本”以查看存储库中的所有文件。

- 单击“在稳定的 baselines3 中使用”以获取显示如何加载模型的代码段。

- 模型卡（`readme.md`文件），该卡给出了模型的描述

在引擎下，仓库使用基于GIT的存储库（如果你不知道 Git 是什么），请不要担心），这意味着你可以在实验和改进智能体时使用新版本更新模型。

使用排行榜将 Lunarlander-V2 与同学进行比较🏆👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard

## 从仓库加载并保存你的登月着陆器模型🤗
感谢 [ironbar](https://github.com/ironbar) 的贡献

从仓库加载已保存的模型非常简单

去 https://huggingface.co/models?library=stable-baselines3 找所有的 Stable-baselines3 保存模型.
1. 选择一个复制一下 repo_id

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit1/copy-id.png" alt="Copy-id"/>

然后，我们只需要使用 load_from_hub 与：

- repo_id

- 文件名：存储库中的保存模型及其扩展名（*.zip）

由于我从 Hub 下载的模型是用 Gym（Gymnasium 的前一个版本）训练的，我们需要安装 shimmy，这是一个 API 转换工具，它将帮助我们正确运行环境。

Shimmy 文档：https://github.com/Farama-Foundation/Shimmy

```python
!pip install shimmy
```

```python
from huggingface_sb3 import load_from_hub
repo_id = "Classroom-workshop/assignment2-omar" # The repo_id
filename = "ppo-LunarLander-v2.zip" # The model filename.zip

# When the model was trained on Python 3.8 the pickle protocol is 5
# But Python 3.6, 3.7 use protocol 4
# In order to get compatibility we need to:
# 1. Install pickle5 (we done it at the beginning of the colab)
# 2. Create a custom empty object we pass as parameter to PPO.load()
custom_objects = {
            "learning_rate": 0.0,
            "lr_schedule": lambda _: 0.0,
            "clip_range": lambda _: 0.0,
}

checkpoint = load_from_hub(repo_id, filename)
model = PPO.load(checkpoint, custom_objects=custom_objects, print_system_info=True)
```

让我们评估一下智能体


```python
#@title
eval_env = Monitor(gym.make("LunarLander-v2"))
mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)
print(f"mean_reward={mean_reward:.2f} +/- {std_reward}")
```

## 一些其他挑战🏆

学习**的最好方法是自己尝试**！如你所见，当前的智能体不佳。作为第一个建议，你可以训练更多步骤。有了 1,000,000 步，我们看到了一些不错的结果！
在 [排行榜](https://huggingface.co/spaces/huggingface-projects/deep-reinforcement-learning-leaderboard)中，你会找到你的智能体。你能刷榜吗？

这里有一些技巧帮助你刷榜：

* 训练更多步骤
* 尝试 `ppo' 的其他超参数。详细参考 https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters.
* 查看 [Stable-Baselines3 文档](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html)并且尝试一下其他模型比如 DQN
* 将**新训练的模型推到**仓库上

使用 [排行榜](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) 🏆比较你和你同学的 LunarLander-v2结果

月亮着陆任务太无聊了吗？尝试**改变环境**，使用 MountainCar-V0，Cartpole-V1 或 Carracing-V0？查看他们的工作原理[使用Gym文档]（https://www.gymlibrary.dev/），祝你玩的开心 🎉。

恭喜完成本章！这是重要的一步

如果你仍然对所有这些元素某些感到困惑……那是完全正常的！**这对我和所有研究 RL 的人都是一样的。**

花点时间才能真正掌握信息，然后再尝试其他挑战**。掌握这些信息并拥有坚实的基础很重要。

自然，在课程中，我们将使用并更深入地解释这些术语，但是**最好在深入下一章之前对它们有很好的了解。**



下次，在奖励单元1中，你将训练 Huggy 狗去叼棍子。


<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit1/huggy.jpg" alt="Huggy"/>

## 保持热爱，奔赴山海 🤗


