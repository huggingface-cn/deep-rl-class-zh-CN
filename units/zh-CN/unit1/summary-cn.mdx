# 摘要 [[summary]]

这里有很多信息！让我们总结一下：

- 强化学习是一种从行动中学习的计算方法。我们构建了一个从环境中学习的代理**通过反复试验与它交互**并接收奖励（负面或正面）作为反馈。

- 任何RL智能体的目标都是最大化其预期累积奖励（也称为预期回报），因为 RL 基于**奖励假设**，即**所有目标都可以描述为预期累积奖励的最大化报酬。**

- RL 过程是一个循环，输出一系列 **state、action、reward 和 next state。**

- 为了计算预期的累积奖励（预期回报），我们对奖励进行折扣：较早出现的奖励（在游戏开始时）**更有可能发生，因为它们比长期的未来奖励更可预测。* *

- 要解决 RL 问题，你需要**找到最优策略**。该政策是你智能体的“大脑”，它将告诉我们**在给定状态下采取什么行动。**最优政策是**为你提供最大化预期回报的行动。**

- 有两种方法可以找到你的最佳策略：
    1. 通过直接训练你的策略：**基于策略的方法。**
    2. 通过训练一个价值函数来告诉我们智能体在每个状态下将获得的预期回报，并使用这个函数来定义我们的策略：**基于价值的方法。**

- 最后，我们谈到深度强化学习，因为我们引入了**深度神经网络来估计要采取的行动（基于策略）或估计状态的价值（基于价值）**因此得名“深度”。