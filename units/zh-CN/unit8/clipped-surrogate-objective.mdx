# 裁剪替代目标函数简介
## 回顾：策略目标函数

让我们回忆一下什么是强化学习中的目标优化：
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/lpg.jpg" alt="Reinforce"/>

这个想法是，通过在此函数上采取梯度上升这一步（相当于将该函数负向梯度下降），我们**推动我们的智能体采取动作，从而导致更高的奖励并避免有害动作。**

但是，问题在步长上：
- 太小，**训练过程会十分缓慢**
- 太大，**训练中就会有很多变量**


在PPO中，我们的想法是使用一个名为*裁剪替代目标函数*的新目标函数来约束我们的策略更新，**这将使用一个裁剪（clip）将策略变化约束在一个小范围内。**

这个新的函数**旨在避免破坏性的大权重更新**：

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/ppo-surrogate.jpg" alt="PPO surrogate function"/>

让我们学习其每一个部分并去理解他的运行原理。

## 比率函数
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/ratio1.jpg" alt="Ratio"/>

这个比率的计算方式如下：

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/ratio2.jpg" alt="Ratio"/>

它是当前策略在状态 \\( s_t \\) 下选择动作 \\( a_t \\) 的概率与先前策略在相同状态下选择动作的概率之比。

正如我们所看到的，\\( r_t(\theta) \\) 表示当前策略和旧策略之间的概率比率：


- 如果 \\( r_t(\theta) > 1 \\)，**则当前策略下状态 \\( s_t \\) 下选择动作 \\( a_t \\) 的概率比旧策略更高。**
- 如果 \\( r_t(\theta) \\) 在 0 和 1 之间，**则当前策略下选择动作的概率比旧策略更低。**

所以概率比率是一个**非常简单的方式去估计当下策略与旧策略的不同。**


## 裁剪替代目标函数中的未裁剪部分
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/unclipped1.jpg" alt="PPO"/>

这个比率**可以替代我们在策略目标函数中的概率对数。**这给了我们新目标函数的剩下部分：将比率乘以优势。

<figure class="image table text-center m-0 w-full">
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/unclipped2.jpg" alt="PPO"/>
  <figcaption><a href="https://arxiv.org/pdf/1707.06347.pdf">近端优先策略优化算法</a></figcaption>
</figure>

然而，如果没有约束条件，如果我们当前策略下选择的动作在概率上比旧策略更可能，**这将导致一个重要的策略梯度步骤**，从而导致**过度的策略更新。**

## 裁剪替代目标函数的裁剪部分

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/clipped.jpg" alt="PPO"/>

因此，我们需要通过惩罚那些导致比率偏离 1 的变化来限制这个目标函数（在论文中，比率只能从 0.8 到 1.2 变化）。

**通过剪辑比率，我们确保策略更新不会太大，因为当前策略不能与旧策略相差太大。**

为了做到这一点，我们有两个解决方案：

- TRPO（信任区域策略优化）使用 KL 散度约束来限制策略更新，但这种方法实现起来比较复杂，需要更多的计算时间。
- PPO 直接在目标函数中使用剪辑概率比率的**裁剪替代目标函数**来限制策略更新。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/clipped.jpg" alt="PPO"/>

这个裁剪部分是 rt(theta) 在 \\( [1 - \epsilon, 1 + \epsilon] \\) 范围内被裁剪的版本。

使用裁剪替代目标函数，我们有两个概率比率，一个非裁剪的，一个在范围内裁剪的（在 \\( [1 - \epsilon, 1 + \epsilon] \\)之间，epsilon 是一个超参数，帮助我们定义这个裁剪范围（在论文中 \\( \epsilon = 0.2 \\)）。

然后，我们取裁剪和非裁剪目标的最小值，**因此最终目标是非裁剪目标的一个下限（悲观下限）。**

取裁剪和非裁剪目标的最小值意味着**我们将根据比率和优势情况选择裁剪或非裁剪目标。**

