# 使用 Sample-Factory 实现 PPO 的简介

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/thumbnail2.png" alt="thumbnail"/>

在本教程的第二部分中，我们将深入研究 PPO 算法的优化，并使用 [Sample-Factory](https://samplefactory.dev/) 来训练我们的智能体玩 [doom](https://vizdoom.cs.put.edu.pl/)（Doom的开源版本）。Sample-Factory 是一个**异步实现的 PPO 算法**。

在这个 notebook 中，**你将训练你的智能体玩单人任务简单级别**，智能体必须收集健康包以避免死亡。之后，你可以**训练你的智能体玩更复杂的级别，例如多人模式中的死亡模式**。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/environments.png" alt="Environment"/>

听起来不错，让我们开始吧！ 🚀

本次动手尝试由 [Edward Beeching](https://twitter.com/edwardbeeching) 编写, 他是一个在 Hugging Face 的机器学习研究员。他参与开发了 Godot 强化学习智能体，这是一个用于在 Godot 游戏引擎中开发环境和智能体的开源接口。