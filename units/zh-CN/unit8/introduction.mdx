# 介绍 [[introduction]]

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/thumbnail.png" alt="Unit 8"/>

在第六单元，我们学习了优势演员评论员(A2C),这是一种结合了基于价值和基于策略的方法的混合架构，通过以下方法减少方差来帮助稳定训练：

- *一个演员*控制你**智能体的怎样行动**(基于策略方法)。
- *一个评论员*衡量**本次采取的动作的好坏**(基于价值方法)。

今天，我们将要学习近端策略优化(PPO)算法，一种**通过回避大量策略更新来提升我们智能体训练的稳定性**的架构。为了做到此目的，我们使用一个比率来指示我们当前和旧策略之间的差异，并将该比率剪切到特定范围 \\( [1 - \epsilon, 1 + \epsilon] \\) 。

这样做可以保证**我们的策略更新不会太大，训练更稳定。**

本单元分为两个部分：
- 在第一部分，你将会学到 PPO 背后的原理以及从头开始用 [CleanRL](https://github.com/vwxyzjn/cleanrl) 执行你的 PPO 智能体。我们用 LunarLander-v2 来测试他的鲁棒性。LunarLander-v2 **就是你第一次开始本课程时的环境**。那个时候，你还不知道 PPO 是怎样运行的，但现在，**你可以从头训练他。真是太不可思议了 🤩**。
- 在第二部分，我们将会用 [Sample-Factory](https://samplefactory.dev/) 更深入 PPO 的优化并且训练一个智能体玩 vizdoom(一个 Doom 的开源版本)。

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/environments.png" alt="Environment"/>
<figcaption>这是你将要训练你智能体的环境： VizDoom 环境</figcaption>
</figure>

听起来不错？那让我们开始把！🚀
