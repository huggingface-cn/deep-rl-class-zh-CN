# 补充阅读 [[additional-readings]]

如果你想更深入了解的话，这里有一些**可选读物**。

## 关于 PPO 解释

- [由 Daniel Bick 所撰写的关于近端策略优化的连贯、自包含解释](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf)
- [理解强化学习中近端优化策略算法的方法](https://stackoverflow.com/questions/46422845/what-is-the-way-to-understand-proximal-policy-optimization-algorithm-in-rl)
- [深度强化学习基础系列， Pieter Abbeel 写的关于 L4 TRPO 和 PPO](https://youtu.be/KjWF8VIMGiY)
- [OpenAI PPO 博文](https://openai.com/blog/openai-baselines-ppo/)
- [Spinning Up RL 平台  PPO 的应用](https://spinningup.openai.com/en/latest/algorithms/ppo.html)
- [近端优化策略算法的论文](https://arxiv.org/abs/1707.06347)

## PPO 操作细节

- [PPO 的 37 个实现细节](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)
- [第一部分，共三部分—— PPO 实现：11 个核心实现细节](https://www.youtube.com/watch?v=MEt6rrxH8W4)

## 重要采样

- [重要采样解释](https://youtu.be/C3p2wI4RAi8)
