# 动手尝试：进阶深度强化学习。使用 Sample Factory 框架输入像素玩 Doom

<CourseFloatingBanner classNames="absolute z-10 right-0 top-0"
notebooks={[
  {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part2.ipynb"}
  ]}
  askForHelpUrl="http://hf.co/join/discord" />

colab notebook:
[![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit8/unit8_part2.ipynb)

# 第 8 单元第 2 部分：高级深度强化学习。使用 Sample Factory 框架输入像素玩 Doom

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/thumbnail2.png" alt="Thumbnail"/>

本 notebook 将介绍如何使用 Doom 游戏的三维环境训练深度神经网络以收集物品，下面是生成的策略的视频。我们使用异步实现的 PPO 算法 [Sample Factory](https://www.samplefactory.dev/) 训练该策略。

请注意以下几点：

* [Sample Factory](https://www.samplefactory.dev/) 是一个高级的 RL 框架，**仅在 Linux 和 Mac 上运行**（不支持 Windows）。

* 该框架在**具有许多 CPU 核心的 GPU 计算机上表现最佳**，在这种设置下它可以达到每秒 10 万个交互的速度。标准的 Colab 笔记本可用的资源**限制了该库的性能**。因此，在这种设置下的速度**不反映实际的性能**。

* 如果您想了解更多信息，请查看[示例](https://github.com/alex-petrenko/sample-factory/tree/master/sf_examples)中的 Sample Factory 基准测试。


```python
from IPython.display import HTML

HTML(
    """<video width="640" height="480" controls>
  <source src="https://huggingface.co/edbeeching/doom_health_gathering_supreme_3333/resolve/main/replay.mp4"
  type="video/mp4">Your browser does not support the video tag.</video>"""
)
```

为了验证本次动手尝试的[认证过程](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)，你需要推送一个模型

- `doom_health_gathering_supreme` 获得结果 >= 5.

找到你的结果 去 [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) 找到你的模型,**结果 = 平均奖励 - 标准差奖励**

如果你找不到你的模型，**请转到页面底部并点击刷新按钮**

有关认证过程的更多信息，请查看此部分 👉 https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process

## 设置 GPU  💪

- 为了 **加速智能体的训练，我们将使用 GPU**. 点击 `Runtime > Change Runtime type`

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg" alt="GPU Step 1">

- `Hardware Accelerator > GPU`

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg" alt="GPU Step 2">

在开始训练我们的智能体之前，让我**学习一下我们将要使用的库与环境**。

## Sample Factory

[Sample Factory](https://www.samplefactory.dev/) 是**最快的强化学习库之一，专注于非常高效的同步和异步策略梯度（ PPO ）实现。**

Sample Factory 经过了充分**的测试，被许多研究人员和从业者使用**，并且正在积极维护。我们的实现在**各种领域已知达到了 SOTA 性能，并尽量减少了强化学习实验训练时间和硬件需求。**

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/samplefactoryenvs.png" alt="Sample factory"/>

### 关键特征

- 高度优化的算法[架构](https://www.samplefactory.dev/06-architecture/overview/) ，最大程度地提高学习吞吐量 
- [同步和异步](https://www.samplefactory.dev/07-advanced-topics/sync-async/)的训练模式
- [串行（单进程）模式](https://www.samplefactory.dev/07-advanced-topics/serial-mode/)，方便调试
- 在基于 CPU 和[ GPU 加速的环境](https://www.samplefactory.dev/09-environment-integrations/isaacgym/)中均能实现最佳性能
- 单智能体和多智能体训练、自我对弈，支持在一个或多个 GPU 上同时训练[多个策略](https://www.samplefactory.dev/07-advanced-topics/multi-policy-training/)
- 基于种群的训练（[PBT](https://www.samplefactory.dev/07-advanced-topics/pbt/)）
- 离散、连续、混合动作空间
- 向量、图像、字典观测空间
- 通过解析动作/观测空间规范自动创建模型架构。支持[自定义模型架构](https://www.samplefactory.dev/03-customization/custom-models/)
- 设计成可导入其他项目，[自定义环境](https://www.samplefactory.dev/03-customization/custom-environments/)是一等公民
- 详细的[ WandB 和 Tensorboard 摘要](https://www.samplefactory.dev/05-monitoring/metrics-reference/)、[自定义指标](https://www.samplefactory.dev/05-monitoring/custom-metrics/)
- [HuggingFace 🤗 集成](https://www.samplefactory.dev/10-huggingface/huggingface/)（上传训练模型和指标到 Hub）
- 与多个范例环境集成，带有经过调整的参数和训练模型，包括[Mujoco](https://www.samplefactory.dev/09-environment-integrations/mujoco/)、[Atari](https://www.samplefactory.dev/09-environment-integrations/atari/)、[VizDoom](https://www.samplefactory.dev/09-environment-integrations/vizdoom/)、[DMlab](https://www.samplefactory.dev/09-environment-integrations/dmlab/)

以上所有策略都可以在🤗 hub上获得。搜索标签[sample-factory](https://huggingface.co/models?library=sample-factory&sort=downloads)。

### sample-factory 怎么运行

Sample-factory 是社区中**最高度优化的强化学习实现之一**。

它通过**生成多个进程来运行 rollout worker、inference worker 和 learner worker**。

*Workers* 通过共享内存进行通信，这降低了进程之间的通信成本。

*Rollout workers* 与环境交互，并将观测结果发送给 *inference workers*。

*Inferences workers* 查询一个固定版本的策略，并**将动作发送回 rollout worker**。

在 *k* 步之后，rollout workers 将经验轨迹发送给 learner worker，**它使用该轨迹来更新智能体的策略网络**。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/samplefactory.png" alt="Sample factory"/>

### Sample-factory 中的演员-评论员模型

在 Sample Factory 中，演员-评论员模型由三个部分组成：

- **编码器** - 处理输入观测（图像、向量）并将它们映射到一个向量。这是您最有可能想要自定义的模型部分。
- **核心** - 将一个或多个编码器的向量集成，可以选择在基于记忆的智能体中包括单层或多层 LSTM/GRU。
- **解码器** - 在计算策略和价值输出之前，对模型核心的输出应用额外的层。

该库已经被设计成自动支持任何观测和动作空间。用户可以轻松添加他们自己的自定义模型。你可以在[文档](https://www.samplefactory.dev/03-customization/custom-models/#actor-critic-models-in-sample-factory)中了解更多信息。

## ViZDoom

[ViZDoom](https://vizdoom.cs.put.edu.pl/) 是一个**开源的 Doom 引擎的 python 接口**。

这个库是由 Marek Wydmuch 和 Michal Kempka 于 2016 年在波兰波兹南理工大学的计算科学研究所创建的。

该库使得在多种场景下可以**直接从屏幕像素中训练智能体**，包括以下场景，例如在下面的视频中展示的多人模式中的死亡模式。由于 ViZDoom 环境是基于 90 年代创建的游戏，所以它可以在现代硬件上以加速的速度运行，**从而允许我们相当快地学习复杂的 AI 行为。**

该库的功能包括：

- 多平台支持（Linux，macOS，Windows）
- Python 和 C++ 的 API
- OpenAI Gym 环境封装器
- 易于创建自定义场景（提供了可视化编辑器、脚本语言和示例）
- 异步和同步的单人和多人模式
- 轻量级（仅几 MB）和快速（同步模式下单线程可达到 7000fps）
- 可自定义的分辨率和渲染参数
- 可访问深度缓冲区（3D 视觉）
- 自动标记帧中可见的游戏对象
- 可访问音频缓冲区
- 可访问角色/对象和地图几何形状的列表
- 屏幕外渲染和场景记录
- 异步模式中的时间缩放。

## 我们首先需要安装一些依赖来适配 ViZDoom 环境

现在我们的 Colab 运行环境已经设置好了，我们可以开始安装在 Linux 上运行 ViZDoom 所需的依赖项。

如果你正在 Mac 机器上进行操作，你需要按照 [github 页面](https://github.com/Farama-Foundation/ViZDoom/blob/master/doc/Quickstart.md#-quickstart-for-macos-and-anaconda3-python-36)上的安装说明进行安装。

```python
# Install ViZDoom deps from
# https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#-linux

apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \
nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \
libopenal-dev timidity libwildmidi-dev unzip ffmpeg

# Boost libraries
apt-get install libboost-all-dev

# Lua binding dependencies
apt-get install liblua5.1-dev
```

## 然后我们安装 Sample Factory 和 ViZDoom

- 这个耗时 7 分钟左右

```bash
pip install sample-factory
pip install vizdoom
```

## 在 sample-factory 中设置 Doom 环境

```python
import functools

from sample_factory.algo.utils.context import global_model_factory
from sample_factory.cfg.arguments import parse_full_cfg, parse_sf_args
from sample_factory.envs.env_utils import register_env
from sample_factory.train import run_rl

from sf_examples.vizdoom.doom.doom_model import make_vizdoom_encoder
from sf_examples.vizdoom.doom.doom_params import add_doom_env_args, doom_override_defaults
from sf_examples.vizdoom.doom.doom_utils import DOOM_ENVS, make_doom_env_from_spec


# Registers all the ViZDoom environments
def register_vizdoom_envs():
    for env_spec in DOOM_ENVS:
        make_env_func = functools.partial(make_doom_env_from_spec, env_spec)
        register_env(env_spec.name, make_env_func)


# Sample Factory allows the registration of a custom Neural Network architecture
# See https://github.com/alex-petrenko/sample-factory/blob/master/sf_examples/vizdoom/doom/doom_model.py for more details
def register_vizdoom_models():
    global_model_factory().register_encoder_factory(make_vizdoom_encoder)


def register_vizdoom_components():
    register_vizdoom_envs()
    register_vizdoom_models()


# parse the command line args and create a config
def parse_vizdoom_cfg(argv=None, evaluation=False):
    parser, _ = parse_sf_args(argv=argv, evaluation=evaluation)
    # parameters specific to Doom envs
    add_doom_env_args(parser)
    # override Doom default values for algo parameters
    doom_override_defaults(parser)
    # second parsing pass yields the final configuration
    final_cfg = parse_full_cfg(parser, argv)
    return final_cfg
```

现在设置已经完成，我们可以开始训练智能体。我们选择学习一个名为 `Health Gathering Supreme` 的 ViZDoom 任务。

### 场景: Health Gathering Supreme

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/Health-Gathering-Supreme.png" alt="Health-Gathering-Supreme"/>



该场景的目标是**在不知道什么使它存活下来的条件下教会智能体如何生存**。智能体仅知道**生命宝贵**和死亡是不好的，因此**它必须学习什么可以延长它的生存时间，以及和它的健康息息相关的东西**。

地图是一个包含墙壁并有绿色酸性地板的矩形，地板会**定期伤害玩家**。初始时，地图上均匀分布着一些医疗包。每隔一段时间，会有一个新的医疗包从天而降。**医疗包可以恢复玩家的一些健康值**- 为了生存，智能体需要捡起它们。玩家死亡或超时后，该轮游戏结束。

更多配置信息：
- 存活奖励 = 1
- 3个可用按钮：向左转，向右转，向前移动
- 1个可用游戏变量：HEALTH
- 死亡惩罚 = 100

您可以在 ViZDoom 的 [这里](https://github.com/Farama-Foundation/ViZDoom/tree/master/scenarios) 找到更多可用场景的信息。

还有一些更复杂的场景已经在 ViZDoom 创建，如[此 GitHub 页面](https://github.com/edbeeching/3d_control_deep_rl)所述。

## 训练智能体

- 我们将要训练这个智能体大约 4000000 步，这大概会耗费 20 分钟 

```python
## Start the training, this should take around 15 minutes
register_vizdoom_components()

# The scenario we train on today is health gathering
# other scenarios include "doom_basic", "doom_two_colors_easy", "doom_dm", "doom_dwango5", "doom_my_way_home", "doom_deadly_corridor", "doom_defend_the_center", "doom_defend_the_line"
env = "doom_health_gathering_supreme"
cfg = parse_vizdoom_cfg(
    argv=[f"--env={env}", "--num_workers=8", "--num_envs_per_worker=4", "--train_for_env_steps=4000000"]
)

status = run_rl(cfg)
```

## 让我们看一下训练过的策略表现和智能体的视频输出。

```python
from sample_factory.enjoy import enjoy

cfg = parse_vizdoom_cfg(
    argv=[f"--env={env}", "--num_workers=1", "--save_video", "--no_render", "--max_num_episodes=10"], evaluation=True
)
status = enjoy(cfg)
```

## 现在让我们可视化一下智能体的表现

```python
from base64 import b64encode
from IPython.display import HTML

mp4 = open("/content/train_dir/default_experiment/replay.mp4", "rb").read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML(
    """
<video width=640 controls>
      <source src="%s" type="video/mp4">
</video>
"""
    % data_url
)
```

这个智能体目前已经学习了一些东西，但是他的表现可以更好。我们明显需要训练更久，但让我们先把这个模型上传到 Hub。

## 现在让我们上传你的存档点和视频到 Hugging Face Hub




为了能够分享你的模型到社区，你需要遵循以下三个步骤：

1️⃣ (如果没有) 创建一个 HF 账号 ➡ https://huggingface.co/join

2️⃣ 登陆, 从网站中获取认证 token。
- 创建一个具有**写权限**的新 token (https://huggingface.co/settings/tokens)

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg" alt="Create HF Token">

- 复制 token
- 运行下面单元格代码并粘贴 token

如果你不想使用 Google Colab 或者 Jupyter Notebook，你需要用这行命令替代：`huggingface-cli login` (or `login`)

```python
from huggingface_hub import notebook_login
notebook_login()
!git config --global credential.helper store
```

```python
from sample_factory.enjoy import enjoy

hf_username = "ThomasSimonini"  # insert your HuggingFace username here

cfg = parse_vizdoom_cfg(
    argv=[
        f"--env={env}",
        "--num_workers=1",
        "--save_video",
        "--no_render",
        "--max_num_episodes=10",
        "--max_num_frames=100000",
        "--push_to_hub",
        f"--hf_repository={hf_username}/rl_course_vizdoom_health_gathering_supreme",
    ],
    evaluation=True,
)
status = enjoy(cfg)
```

## 让我们加载模型


这个智能体的表现不错，但还有提升的空间！让我们从 Hub 下载并可视化一个训练了 10B 个时间步的智能体。

```bash
#download the agent from the hub
python -m sample_factory.huggingface.load_from_hub -r edbeeching/doom_health_gathering_supreme_2222 -d ./train_dir
```

```bash
ls train_dir/doom_health_gathering_supreme_2222
```

```python
env = "doom_health_gathering_supreme"
cfg = parse_vizdoom_cfg(
    argv=[
        f"--env={env}",
        "--num_workers=1",
        "--save_video",
        "--no_render",
        "--max_num_episodes=10",
        "--experiment=doom_health_gathering_supreme_2222",
        "--train_dir=train_dir",
    ],
    evaluation=True,
)
status = enjoy(cfg)
```

```python
mp4 = open("/content/train_dir/doom_health_gathering_supreme_2222/replay.mp4", "rb").read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML(
    """
<video width=640 controls>
      <source src="%s" type="video/mp4">
</video>
"""
    % data_url
)
```

## 一些额外的挑战 🏆: Doom 多人死亡模式

训练一个在 Doom 的多人死亡模式中玩游戏的智能体，需要使用**比 Colab 更高配置的计算机，需要很多小时**。

幸运的是，我们已经在这个场景中训练过一个智能体，并且它已经可以在 🤗 Hub 上获取！ **让我们下载这个模型并可视化智能体的表现。**

```python
# Download the agent from the hub
python -m sample_factory.huggingface.load_from_hub -r edbeeching/doom_deathmatch_bots_2222 -d ./train_dir
```

鉴于智能体长时间进行游戏，视频生成可能需要**10分钟**。

```python
from sample_factory.enjoy import enjoy

register_vizdoom_components()
env = "doom_deathmatch_bots"
cfg = parse_vizdoom_cfg(
    argv=[
        f"--env={env}",
        "--num_workers=1",
        "--save_video",
        "--no_render",
        "--max_num_episodes=1",
        "--experiment=doom_deathmatch_bots_2222",
        "--train_dir=train_dir",
    ],
    evaluation=True,
)
status = enjoy(cfg)
mp4 = open("/content/train_dir/doom_deathmatch_bots_2222/replay.mp4", "rb").read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML(
    """
<video width=640 controls>
      <source src="%s" type="video/mp4">
</video>
"""
    % data_url
)
```


你**可以尝试使用上面的代码在这个环境中训练你的智能体**，但不能在 Colab 上训练。
**祝你好运 🤞**

如果你喜欢一个更简单的场景，**为什么不尝试在另一个 ViZDoom 场景中训练，例如 `doom_deadly_corridor` 或 `doom_defend_the_center`。**

---

这就结束了最后一个单元。但我们还没有结束！🤗 下面的 **额外章节包括一些最有趣、最先进和最前沿的深度强化学习工作**。

## 保持热爱，奔赴山海 🤗
