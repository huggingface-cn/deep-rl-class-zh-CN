# (选修) 什么是深度强化学习的好奇心机制?

这是关于好奇心机制的介绍(选修)，如果你想要了解更多，你可以阅读下面两篇补充文章，这里面会探讨其中的数学细节：

- [通过下一状态预测进行好奇心驱动的学习](https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa)
- [随机网络蒸馏：好奇心驱动学习的新方法](https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938)

## 现代强化学习的两大问题

要了解什么是 Curiosity ，我们首先需要了解 RL 的两个主要问题：

第一个问题是，*稀疏奖励问题：*即**大多数奖励不包含信息，因此被设置为零**。

请记住，RL 基于*奖励假设*，即每个目标都可以描述为奖励的最大化。因此，奖励作为 RL 智能体的反馈；**如果他们没有收到任何信息，那他们对哪些动作是适合的（或不适的）的了解就不会改变**。


<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit5/curiosity1.png" alt="Curiosity"/>
<figcaption>来源: 感谢奖励机制，我们的智能体才知道在那个状态下这个动作是好的</figcaption>
</figure>

例如，在 [Vizdoom](https://vizdoom.cs.put.edu.pl/) ，一组基于游戏 Doom “DoomMyWayHome” 的环境，你的智能体只有在**找到背心**时才会得到奖励。但是，背心离你的出发点很远，所以你的大部分奖励都会是零。因此，如果我们的智能体没有收到有用的反馈(密集奖励)，学习最优策略将花费更长的时间，并且**他可能会花时间转来转去而找不到目标。**

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit5/curiosity2.png" alt="Curiosity"/>

第二个大问题是**外部奖励函数是人工的；即在每个环境中，一个人都必须执行一个奖励函数**。但是我们如何在大而复杂的环境中扩展他呢？

## 所以什么是好奇心机制呢?

一个针对这些问题的解决方案是**开发智能体内部的奖励函数，即由智能体本身生成的奖励函数**。智能体将充当自学者，因为他将是学生和他自己的反馈主人。

**这种内在的奖励机制被称为好奇心**，因为这种奖励会促使智能体探索新奇/不熟悉的状态。为实现这一目标，我们的智能体将在探索新轨迹时获得高额奖励。

这种奖励机制的灵感来自于人类的行为方式。**我们天生就有探索环境和发现新事物的内在欲望**。

这里有不同的方法来计算这种内在奖励。经典方法(通过下一个状态的预测来计算好奇心)是将好奇心定义为智能体根据**当前状态和采取的动作预测下一个状态时的误差。**

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit5/curiosity3.png" alt="Curiosity"/>

由于好奇心的概念是**鼓励智能体执行可以减少智能体预测其动作后果的不确定性的动作**(在智能体花费较少时间或动态复杂的区域不确定性会更高)。

如果智能体在这些状态上花费了大量时间，预测下一个状态将是很容易的(好奇心低)。另一方面，如果是一个未探索的新状态，预测接下来的状态将会很困难(好奇心高)。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit5/curiosity4.png" alt="Curiosity"/>

使用好奇心将推动智能体倾向于选择具有高预测误差的转换(在智能体花费较少时间或动态复杂的区域误差会更高)，并**因此更好地探索我们的环境**。

这里同样还有**其他好奇心计算方法**。ML-Agents 使用了一种更高级的方法，称为随机网络蒸馏的好奇心。这超出了本教程的范围，但如果你感兴趣，[我写了一篇详细解释他的文章](https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938)。
