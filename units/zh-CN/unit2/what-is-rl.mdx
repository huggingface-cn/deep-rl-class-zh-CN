# 回顾: 什么是 RL?

在强化学习中，我们构建一个能**做智能决策**的智能体。例如，一个**学习玩电子游戏**的智能体，或一个能够通过决定**商品的购入种类和售出时间**从而**最大化收益**的交易智能体。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/rl-process.jpg" alt="RL process"/>

但是为了做出比较聪明的决策，我们的智能体需要从环境中学习，**通过试错的方法与环境交互**，并接受奖励（正向或负向）**作为唯一反馈**，以此进行学习。

智能体的目标是最大化累计期望奖励（基于奖励假设）

**智能体的决策过程称作策略π**：给定一个状态，一个策略将输出一个动作或一个动作的概率分布。也就是说，给定一个环境的观察，策略将会输出一个行动（或每一个动作的概率），智能体将会执行该动作。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/policy.jpg" alt="Policy"/>

我们的目标是找到一个最优的策略π*，也就是一个能够获得最好的累计期望奖励的策略。

找到这个最优策略（从而解决强化学习问题），**有两种主要的强化学习方法**：

- 基于策略的方法：**直接训练策略**，从而根据给定的状态来学习要执行的动作。
- 基于价值的方法：**训练一个价值函数**来学习**哪个状态更有价值**，并用这个价值函数**采取对应的动作**。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches.jpg" alt="Two RL approaches"/>

在本单元中，我们将**深入学习基于价值的方法**。