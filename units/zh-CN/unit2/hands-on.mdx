# å®è·µæ—¶é—´

      <CourseFloatingBanner classNames="absolute z-10 right-0 top-0"
      notebooks={[
        {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit2/unit2.ipynb"}
        ]}
        askForHelpUrl="http://hf.co/join/discord" />



ä¹‹å‰æˆ‘ä»¬å·²ç»å­¦ä¹ äº†Q-learningç®—æ³•ï¼Œç°åœ¨æˆ‘ä»¬è¦ä»å¤´å®ç°å®ƒï¼Œå¹¶åœ¨ä¸¤ä¸ªç¯å¢ƒä¸­è®­ç»ƒQ-learningæ™ºèƒ½ä½“ï¼š

1. [Frozen-Lake-v1ï¼ˆéæ»‘åŠ¨å’Œæ»‘åŠ¨ç‰ˆæœ¬ï¼‰](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/)â˜ƒï¸ï¼šæ™ºèƒ½ä½“éœ€è¦**ä»èµ·å§‹çŠ¶æ€ï¼ˆSï¼‰åˆ°è¾¾ç›®æ ‡çŠ¶æ€ï¼ˆGï¼‰**ï¼Œåªåœ¨å†°å†»çš„ç“·ç –ï¼ˆFï¼‰ä¸Šè¡Œèµ°ï¼Œé¿å…æ‰å…¥æ´ç©´ï¼ˆHï¼‰ã€‚
2. [è‡ªåŠ¨é©¾é©¶å‡ºç§Ÿè½¦](https://www.gymlibrary.dev/environments/toy_text/taxi/)ğŸš–ï¼šæ™ºèƒ½ä½“éœ€è¦**å­¦ä¼šåœ¨åŸå¸‚ä¸­å¯¼èˆª**ï¼Œä»¥ä¾¿å°†ä¹˜å®¢ä»Aç‚¹è¿è¾“åˆ°Bç‚¹ã€‚

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif" alt="Environments"/>

åœ¨[æ’è¡Œæ¦œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)ä¸­ä½ å¯ä»¥æ¯”è¾ƒè‡ªå·±å’Œå…¶ä»–åŒå­¦çš„ç»“æœï¼Œå¹¶ç›¸äº’äº¤æµæ¢è®¨æœ€å¥½çš„å®ç°æ–¹æ³•ä»¥æé«˜æ™ºèƒ½ä½“çš„åˆ†æ•°ã€‚è°å°†èµ¢å¾—è¯¥æŒ‘æˆ˜ï¼Ÿæ‹­ç›®ä»¥å¾…ï¼

ä¸ºäº†å®Œæˆè¿™ä¸ªå®è·µéƒ¨åˆ†çš„[è®¤è¯è¿‡ç¨‹](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)ï¼Œä½ éœ€è¦å°†ä½ è®­ç»ƒè¿‡çš„å‡ºç§Ÿè½¦æ¨¡å‹æ¨é€åˆ°Hubï¼Œå¹¶**è·å¾—>= 4.5çš„æˆç»©**ã€‚

ä½ å¯ä»¥åœ¨[æ’è¡Œæ¦œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)æ‰¾åˆ°ä½ çš„æ¨¡å‹å¹¶æŸ¥çœ‹æ¨¡å‹è¯„ä»·å€¼ï¼Œ**è¯„ä»·å€¼ = å¹³å‡å›æŠ¥ - å›æŠ¥çš„æ ‡å‡†å·®**

æœ‰å…³è®¤è¯è¿‡ç¨‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¯¥éƒ¨åˆ†ğŸ‘‰ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process

ä½ å¯ä»¥åœ¨è¯¥å¤„æ£€æŸ¥ä½ çš„è¿›åº¦ğŸ‘‰ https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course

**å¦‚æœè¦å¼€å§‹è¯¥å®è·µï¼Œè¯·å•å‡»â€œåœ¨Colabä¸­æ‰“å¼€â€æŒ‰é’®**ğŸ‘‡ï¼š

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit2/unit2.ipynb)


# Unit 2: åœ¨FrozenLake-v1 â›„ å’Œ Taxi-v3 ğŸš•ä¸­ä½¿ç”¨ Q-learning 

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/thumbnail.jpg" alt="Unit 2 Thumbnail">

åœ¨è¿™ä¸ªç¬”è®°æœ¬ä¸­ï¼Œ**ä½ å°†ä»å¤´ç¼–å†™ä½ çš„ç¬¬ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“**ï¼Œä½¿ç”¨Q-learningè®­ç»ƒæ™ºèƒ½ä½“åœ¨FrozenLakeâ„ï¸ä¸­ç©æ¸¸æˆï¼Œå¹¶å°†å…¶åˆ†äº«ç»™ç¤¾åŒºï¼Œå¯ä»¥å°è¯•ä¸åŒçš„é…ç½®è¿›è¡Œè®­ç»ƒã€‚

â¬‡ï¸ ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ï¼Œä½ å¯ä»¥åœ¨**å‡ åˆ†é’Ÿå†…å®ç°å®ƒ**ã€‚â¬‡ï¸

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif" alt="Environments"/>

### ğŸ® ç¯å¢ƒ:

- [FrozenLake-v1](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/)
- [Taxi-v3](https://www.gymlibrary.dev/environments/toy_text/taxi/)

### ğŸ“š RLåº“:

- Python and NumPy
- [Gym](https://www.gymlibrary.dev/)

æˆ‘ä»¬è‡´åŠ›äºæ”¹è¿›å®Œå–„è¯¥æ•™ç¨‹ï¼Œæ‰€ä»¥**å¦‚æœä½ åœ¨è¯¥æ•™ç¨‹ä¸­å‘ç°äº†ä¸€äº›é—®é¢˜**ï¼Œè¯·[åœ¨GitHub Repoä¸Šæå‡º](https://github.com/huggingface/deep-rl-class/issues)ã€‚

## æœ¬å•å…ƒçš„ç›®æ ‡ ğŸ†

åœ¨å•å…ƒç»“æŸæ—¶ï¼Œä½ å°†ï¼š

- èƒ½å¤Ÿä½¿ç”¨**Gym**ç¯å¢ƒåº“ã€‚
- èƒ½å¤Ÿä»å¤´ç¼–å†™ä¸€ä¸ªQ-learningæ™ºèƒ½ä½“ã€‚
- èƒ½å¤Ÿ**å°†ä½ çš„è®­ç»ƒè¿‡çš„æ™ºèƒ½ä½“åŠå…¶ä»£ç æ¨é€åˆ°Hub**ï¼Œå¹¶é™„ä¸Šç²¾ç¾çš„è§†é¢‘å›æ”¾å’Œè¯„ä¼°å¾—åˆ†ğŸ”¥ã€‚

## çŸ¥è¯†å‰æ ğŸ—ï¸

åœ¨æ·±å…¥äº†è§£ç¬”è®°æœ¬ä¹‹å‰ï¼Œä½ éœ€è¦ï¼š

ğŸ”² ğŸ“š **é€šè¿‡é˜…è¯»Unit 2å­¦ä¹ [Q-Learning](https://huggingface.co/deep-rl-course/unit2/introduction)** ğŸ¤—

## Q-learningçš„ç®€è¦å›é¡¾

- Q-learningç®—æ³•æ˜¯**ä¸€ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•**ï¼Œå…·æœ‰ä»¥ä¸‹ä¸»è¦ç‰¹ç‚¹ï¼š
  - å®ƒä¼šè®­ç»ƒä¸€ä¸ªQå‡½æ•°ï¼Œè¿™æ˜¯ä¸€ç§**åŠ¨ä½œä»·å€¼å‡½æ•°**ï¼Œå…¶å†…éƒ¨æœ‰ä¸€ä¸ªQè¡¨æ ¼ï¼Œç”¨äº**å­˜å‚¨æ‰€æœ‰çŠ¶æ€-åŠ¨ä½œå¯¹çš„å€¼**ã€‚
  - å½“ç»™å®šä¸€ä¸ªçŠ¶æ€å’ŒåŠ¨ä½œæ—¶ï¼ŒQå‡½æ•°**ä¼šåœ¨Qè¡¨æ ¼ä¸­æŸ¥æ‰¾ç›¸åº”çš„å€¼**ã€‚

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg" alt="Q function"  width="100%"/>

- åœ¨è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ä¸ª**æœ€ä¼˜çš„Qå‡½æ•°**ï¼Œ**ä»è€Œè·å¾—ä¸€ä¸ªæœ€ä¼˜çš„Qè¡¨æ ¼**ã€‚
- å½“æˆ‘ä»¬æ‹¥æœ‰ä¸€ä¸ª**æœ€ä¼˜çš„Qå‡½æ•°**æ—¶ï¼Œæˆ‘ä»¬å°±èƒ½å¾—åˆ°ä¸€ä¸ªæœ€ä¼˜ç­–ç•¥ï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“**åœ¨æ¯ä¸ªçŠ¶æ€ä¸‹åº”è¯¥é‡‡å–ä»€ä¹ˆæœ€ä½³åŠ¨ä½œã€‚**

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg" alt="Link value policy"  width="100%"/>

ç„¶è€Œï¼Œåœ¨ä¸€å¼€å§‹ï¼Œ**æˆ‘ä»¬çš„Qè¡¨æ ¼æ˜¯æ²¡ç”¨çš„**ï¼Œ**å› ä¸ºå®ƒä¸ºæ¯ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹æä¾›äº†ä»»æ„çš„å€¼ï¼ˆé€šå¸¸æˆ‘ä»¬ä¼šå°†Qè¡¨æ ¼åˆå§‹åŒ–ä¸ºå…¨é›¶å€¼ï¼‰**ã€‚ä½†éšç€æˆ‘ä»¬ä¸æ–­åœ°æ¢ç´¢ç¯å¢ƒå¹¶æ›´æ–°Qè¡¨æ ¼ï¼Œå®ƒå°†ä¸ºæˆ‘ä»¬æä¾›è¶Šæ¥è¶Šå¥½çš„è¿‘ä¼¼å€¼ã€‚

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg" alt="q-learning.jpeg" width="100%"/>

ä»¥ä¸‹æ˜¯Q-learningç®—æ³•çš„ä¼ªä»£ç ï¼š

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg" alt="Q-Learning" width="100%"/>



# è®©æˆ‘ä»¬å¼€å§‹ç¼–å†™ç¬¬ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ç®—æ³• ğŸš€

## å®‰è£…ä¾èµ–å¹¶åˆ›å»ºè™šæ‹Ÿæ˜¾ç¤º ğŸ”½

åœ¨ç¬”è®°ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆä¸€ä¸ªå›æ”¾è§†é¢‘ã€‚æ‰€ä»¥åœ¨Colabä¸­ï¼Œ**æˆ‘ä»¬éœ€è¦ä¸€ä¸ªè™šæ‹Ÿå±å¹•æ¥å‘ˆç°ç¯å¢ƒ**ï¼ˆä»è€Œå½•åˆ¶è§†é¢‘å¸§ï¼‰ã€‚

å› æ­¤ï¼Œä¸‹é¢çš„å•å…ƒæ ¼å°†å®‰è£…åº“å¹¶åˆ›å»ºå¹¶è¿è¡Œä¸€ä¸ªè™šæ‹Ÿå±å¹•ğŸ–¥

æˆ‘ä»¬å°†å®‰è£…å¤šä¸ªåº“ï¼š

- `gym`ï¼šåŒ…å«FrozenLake-v1â›„å’ŒTaxi-v3ğŸš•ç¯å¢ƒã€‚æˆ‘ä»¬ä½¿ç”¨`gym==0.24`ï¼Œå› ä¸ºå®ƒåŒ…å«ä¸€ä¸ªæ¼‚äº®çš„Taxi-v3 UIç‰ˆæœ¬ã€‚
- `pygame`ï¼šç”¨äºFrozenLake-v1å’ŒTaxi-v3çš„UIã€‚
- `numpy`ï¼šç”¨äºå¤„ç†æˆ‘ä»¬çš„Qè¡¨æ ¼ã€‚

Hugging Face Hub ğŸ¤— ä½œä¸ºä¸€ä¸ªä¸­å¿ƒå¹³å°ï¼Œä»»ä½•äººéƒ½å¯ä»¥åœ¨æ­¤å…±äº«å’Œæ¢ç´¢æ¨¡å‹å’Œæ•°æ®é›†ã€‚å®ƒå…·æœ‰ç‰ˆæœ¬æ§åˆ¶ã€åº¦é‡ã€å¯è§†åŒ–ç­‰åŠŸèƒ½ï¼Œä½¿ä½ å¯ä»¥è½»æ¾ä¸ä»–äººåˆä½œã€‚

ä½ å¯ä»¥åœ¨è¿™é‡ŒæŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚æœå®ƒä»¬ä½¿ç”¨Q-learningï¼‰ğŸ‘‰ https://huggingface.co/models?other=q-learning

```bash
pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt
```

```bash
sudo apt-get update
apt install python-opengl ffmpeg xvfb
pip3 install pyvirtualdisplay
```

ä¸ºäº†ç¡®ä¿èƒ½å¤Ÿä½¿ç”¨æ–°å®‰è£…çš„åº“ï¼Œ**æœ‰æ—¶æˆ‘ä»¬éœ€è¦é‡æ–°å¯åŠ¨ç¬”è®°æœ¬çš„è¿è¡Œæ—¶ç¯å¢ƒ**ã€‚ä¸‹ä¸€ä¸ªå•å…ƒæ ¼å°†å¼ºåˆ¶**è¿è¡Œæ—¶ç¯å¢ƒå´©æºƒï¼Œè¿™æ ·ä½ å°±éœ€è¦é‡æ–°è¿æ¥å¹¶ä»è¿™é‡Œå¼€å§‹è¿è¡Œä»£ç **ã€‚å¤šäºäº†è¿™ä¸ªæŠ€å·§ï¼Œ**æˆ‘ä»¬æ‰èƒ½è¿è¡Œæˆ‘ä»¬çš„è™šæ‹Ÿå±å¹•**ã€‚

```python
import os

os.kill(os.getpid(), 9)
```

```python
# Virtual display
from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()
```

## å¯¼å…¥åŒ… ğŸ“¦

é™¤äº†å®‰è£…çš„åº“ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨ï¼š

- `random`ï¼šç”Ÿæˆéšæœºæ•°ï¼ˆå¯¹äºepsilon-è´ªå¿ƒç­–ç•¥éå¸¸æœ‰ç”¨ï¼‰ã€‚
- `imageio`ï¼šç”Ÿæˆå›æ”¾è§†é¢‘ã€‚

```python
import numpy as np
import gym
import random
import imageio
import os

import pickle5 as pickle
from tqdm.notebook import tqdm
```

æ¥ä¸‹æ¥æˆ‘ä»¬æ­£å¼è¿›å…¥Q-learningç®—æ³•çš„ä»£ç éƒ¨åˆ† ğŸ”¥

# Part 1: Frozen Lake â›„ (éæ»‘åŠ¨ç‰ˆæœ¬)

## åˆ›å»ºå¹¶ç†è§£ [FrozenLake ç¯å¢ƒâ›„](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/)



ğŸ’¡ å¼€å§‹ä½¿ç”¨ç¯å¢ƒæ—¶ï¼ŒæŸ¥çœ‹å…¶æ–‡æ¡£æ˜¯ä¸ªå¥½ä¹ æƒ¯

ğŸ‘‰ https://www.gymlibrary.dev/environments/toy_text/frozen_lake/

---

æˆ‘ä»¬å°†ä½¿ç”¨Q-learningç®—æ³•æ¥è®­ç»ƒæ™ºèƒ½ä½“ï¼Œä½¿å…¶**ä»…åœ¨å†°å†»ç –å—ï¼ˆFï¼‰ä¸Šè¡Œèµ°ï¼Œå¹¶é¿å¼€æ´ç©´ï¼ˆHï¼‰ï¼Œä»èµ·å§‹çŠ¶æ€ï¼ˆSï¼‰å¯¼èˆªè‡³ç›®æ ‡çŠ¶æ€ï¼ˆGï¼‰**ã€‚

æˆ‘ä»¬æœ‰ä¸¤ç§è§„æ ¼çš„ç¯å¢ƒï¼š

- `map_name="4x4"`ï¼šä¸€ä¸ª4x4çš„ç½‘æ ¼ç‰ˆæœ¬
- `map_name="8x8"`ï¼šä¸€ä¸ª8x8çš„ç½‘æ ¼ç‰ˆæœ¬

ç¯å¢ƒæœ‰ä¸¤ç§æ¨¡å¼ï¼š

- `is_slippery=False`ï¼šç”±äºå†°å†»æ¹–é¢çš„éæ»‘åŠ¨æ€§è´¨ï¼Œæ™ºèƒ½ä½“æ€»æ˜¯æ²¿ç€**é¢„æœŸçš„æ–¹å‘ç§»åŠ¨**ï¼ˆç¡®å®šæ€§ï¼‰ã€‚
- `is_slippery=True`ï¼šç”±äºå†°å†»æ¹–é¢çš„æ»‘åŠ¨æ€§è´¨ï¼Œæ™ºèƒ½ä½“**å¯èƒ½ä¸ä¼šæ€»æ˜¯æ²¿ç€é¢„æœŸçš„æ–¹å‘ç§»åŠ¨**ï¼ˆéšæœºæ€§ï¼‰ã€‚

ç°åœ¨æˆ‘ä»¬å…ˆç”¨4x4çš„åœ°å›¾å’Œéæ»‘åŠ¨ç‰ˆæœ¬æ¥ç®€åŒ–é—®é¢˜ã€‚

```python
# ä½¿ç”¨æ­£ç¡®çš„å‚æ•°åˆ›å»ºFrozenLake-v1ç¯å¢ƒï¼Œä½¿ç”¨4x4åœ°å›¾å’Œéæ»‘åŠ¨ç‰ˆæœ¬
env = gym.make()  # TODO ä½¿ç”¨æ­£ç¡®çš„å‚æ•°
```

### ç­”æ¡ˆ

```python
env = gym.make("FrozenLake-v1", map_name="4x4", is_slippery=False)
```

ä½ å¯ä»¥åƒè¿™æ ·åˆ›å»ºè‡ªå·±çš„è‡ªå®šä¹‰ç½‘æ ¼ï¼š

```python
desc=["SFFF", "FHFH", "FFFH", "HFFG"]
gym.make('FrozenLake-v1', desc=desc, is_slippery=True)
```

ä½†æˆ‘ä»¬ç°åœ¨å°†ä½¿ç”¨é»˜è®¤çš„ç¯å¢ƒã€‚

### è®©æˆ‘ä»¬çœ‹çœ‹ç¯å¢ƒçš„æ ·å­ï¼š


```python
# æˆ‘ä»¬ç”¨gym.make("<name_of_the_environment>")åˆ›å»ºç¯å¢ƒ- is_slippery=Falseï¼šç”±äºå†°å†»æ¹–é¢çš„éæ»‘åŠ¨æ€§è´¨ï¼Œæ™ºèƒ½ä½“æ€»æ˜¯æ²¿ç€é¢„æœŸçš„æ–¹å‘ç§»åŠ¨ï¼ˆç¡®å®šæ€§ï¼‰
print("_____OBSERVATION SPACE_____ \n")
print("Observation Space", env.observation_space)
print("Sample observation", env.observation_space.sample())  # è·å¾—ä¸€ä¸ªéšæœºè§‚æµ‹å€¼
```

æˆ‘ä»¬é€šè¿‡`Observation Space Shape Discrete(16)`å¯ä»¥çœ‹åˆ°ï¼Œè§‚æµ‹å€¼æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œè¡¨ç¤º**æ™ºèƒ½ä½“å½“å‰ä½ç½®ä¸ºcurrent_row \* nrows + current_colï¼ˆå…¶ä¸­è¡Œå’Œåˆ—éƒ½ä»0å¼€å§‹ï¼‰**ã€‚

ä¾‹å¦‚ï¼Œ4x4åœ°å›¾ä¸­çš„ç›®æ ‡ä½ç½®å¯ä»¥æŒ‰ä»¥ä¸‹æ–¹å¼è®¡ç®—ï¼š3 * 4 + 3 = 15ã€‚å¯èƒ½çš„è§‚æµ‹å€¼æ•°é‡å–å†³äºåœ°å›¾çš„å¤§å°ã€‚**ä¾‹å¦‚ï¼Œ4x4åœ°å›¾æœ‰16ä¸ªå¯èƒ½çš„è§‚æµ‹å€¼ã€‚**

ä¾‹å¦‚ï¼Œè¿™æ˜¯ state = 0 çš„æ ·å­ï¼š

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png" alt="FrozenLake">

```python
print("\n _____ACTION SPACE_____ \n")
print("Action Space Shape", env.action_space.n)
print("Action Space Sample", env.action_space.sample())  # é‡‡å–ä¸€ä¸ªéšæœºåŠ¨ä½œ
```

åŠ¨ä½œç©ºé—´ï¼ˆæ™ºèƒ½ä½“å¯é‡‡å–çš„åŠ¨ä½œé›†åˆï¼‰æ˜¯ç¦»æ•£çš„ï¼Œæœ‰4ä¸ªå¯ç”¨åŠ¨ä½œğŸ®ï¼š

- 0ï¼šå‘å·¦èµ°
- 1ï¼šå‘ä¸‹èµ°
- 2ï¼šå‘å³èµ°
- 3ï¼šå‘ä¸Šèµ°

å¥–åŠ±å‡½æ•°ğŸ’°ï¼š

- åˆ°è¾¾ç›®æ ‡ï¼š+1
- åˆ°è¾¾æ´ç©´ï¼š0
- åˆ°è¾¾å†°å†»ï¼š0

## åˆ›å»ºå¹¶åˆå§‹åŒ– Qè¡¨æ ¼ ğŸ—„ï¸

(ğŸ‘€ ä»¥ä¸‹æ˜¯Q-learningç®—æ³•çš„ä¼ªä»£ç )

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg" alt="Q-Learning" width="100%"/>

ç°åœ¨æ˜¯åˆå§‹åŒ–æˆ‘ä»¬çš„Qè¡¨æ ¼çš„æ—¶å€™äº†ï¼ä¸ºäº†çŸ¥é“è¦ä½¿ç”¨å¤šå°‘è¡Œï¼ˆçŠ¶æ€ï¼‰å’Œåˆ—ï¼ˆåŠ¨ä½œï¼‰ï¼Œæˆ‘ä»¬éœ€è¦äº†è§£åŠ¨ä½œå’Œè§‚æµ‹ç©ºé—´ã€‚è™½ç„¶æˆ‘ä»¬ä¹‹å‰å·²ç»çŸ¥é“äº†åŠ¨ä½œå’Œè§‚æµ‹ç©ºé—´çš„æ•°å€¼ï¼Œä½†æ˜¯ä¸ºäº†ç®—æ³•èƒ½å¤Ÿé€‚ç”¨äºä¸åŒçš„ç¯å¢ƒï¼Œæˆ‘ä»¬åœ¨ç¨‹åºä¸­ä»¥å˜é‡çš„å½¢å¼å¯¹å®ƒä»¬è¿›è¡Œå­˜å‚¨ã€‚Gym ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§æ–¹æ³•ï¼š`env.action_space.n` å’Œ `env.observation_space.n`ã€‚


```python
state_space =
print("There are ", state_space, " possible states")

action_space =
print("There are ", action_space, " possible actions")
```

```python
# åˆ›å»ºä¸€ä¸ªå¤§å°ä¸ºï¼ˆstate_spaceï¼Œaction_spaceï¼‰çš„ Qè¡¨æ ¼ï¼Œå¹¶ä½¿ç”¨ np.zeros å°†æ¯ä¸ªå€¼åˆå§‹åŒ–ä¸º 0
def initialize_q_table(state_space, action_space):
  Qtable =
  return Qtable
```

```python
Qtable_frozenlake = initialize_q_table(state_space, action_space)
```

### ç­”æ¡ˆ

```python
state_space = env.observation_space.n
print("There are ", state_space, " possible states")

action_space = env.action_space.n
print("There are ", action_space, " possible actions")
```

```python
# åˆ›å»ºä¸€ä¸ªå¤§å°ä¸ºï¼ˆstate_spaceï¼Œaction_spaceï¼‰çš„ Qè¡¨æ ¼ï¼Œå¹¶ä½¿ç”¨ np.zeros å°†æ¯ä¸ªå€¼åˆå§‹åŒ–ä¸º 0
def initialize_q_table(state_space, action_space):
    Qtable = np.zeros((state_space, action_space))
    return Qtable
```

```python
Qtable_frozenlake = initialize_q_table(state_space, action_space)
```

## å®šä¹‰è´ªå¿ƒç­–ç•¥ ğŸ¤–

éœ€è¦æ³¨æ„çš„æ˜¯æˆ‘ä»¬æœ‰ä¸¤ä¸ªç­–ç•¥ï¼Œå› ä¸º Q-learning æ˜¯ä¸€ç§**å¼‚ç­–ç•¥**ç®—æ³•ï¼Œæ‰€ä»¥æˆ‘ä»¬**ä½¿ç”¨ä¸åŒçš„ç­–ç•¥æ¥æ›´æ–°è¡ŒåŠ¨å’Œä»·å€¼å‡½æ•°**ã€‚

- Epsilon è´ªå¿ƒç­–ç•¥ï¼ˆè¡ŒåŠ¨ç­–ç•¥ï¼‰
- è´ªå¿ƒç­–ç•¥ï¼ˆæ›´æ–°ç­–ç•¥ï¼‰

è´ªå¿ƒç­–ç•¥ä¹Ÿå°†æ˜¯ä½¿ç”¨ Q-learning ç®—æ³•è®­ç»ƒæ™ºèƒ½ä½“åçš„æœ€ç»ˆç­–ç•¥ï¼Œè´ªå¿ƒç­–ç•¥ç”¨äºä» Qè¡¨æ ¼ ä¸­é€‰æ‹©åŠ¨ä½œã€‚

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg" alt="Q-Learning" width="100%"/>


```python
def greedy_policy(Qtable, state):
  # åˆ©ç”¨ï¼šé€‰æ‹©å…·æœ‰æœ€é«˜çŠ¶æ€-åŠ¨ä½œä»·å€¼çš„åŠ¨ä½œ
  action =

  return action
```

#### ç­”æ¡ˆ

```python
def greedy_policy(Qtable, state):
    # åˆ©ç”¨ï¼šé€‰æ‹©å…·æœ‰æœ€é«˜çŠ¶æ€-åŠ¨ä½œä»·å€¼çš„åŠ¨ä½œ
    action = np.argmax(Qtable[state][:])

    return action
```

##å®šä¹‰ epsilon è´ªå¿ƒç­–ç•¥ ğŸ¤–

Epsilon è´ªå¿ƒç­–ç•¥æ˜¯å¤„ç†æ¢ç´¢å’Œåˆ©ç”¨ä¹‹é—´çš„æƒè¡¡é—®é¢˜çš„ä¸€ç§è®­ç»ƒç­–ç•¥ã€‚

Epsilon è´ªå¿ƒç­–ç•¥çš„æ€æƒ³æ˜¯ï¼š

- *æ¦‚ç‡ 1 â€” É›*ï¼šæ™ºèƒ½ä½“è¿›è¡Œ**åˆ©ç”¨**ï¼ˆå³æ™ºèƒ½ä½“é€‰æ‹©å…·æœ‰æœ€é«˜çŠ¶æ€-åŠ¨ä½œå¯¹å€¼çš„åŠ¨ä½œï¼‰ã€‚
- *æ¦‚ç‡ É›*ï¼š**æ™ºèƒ½ä½“è¿›è¡Œæ¢ç´¢**ï¼ˆå°è¯•éšæœºåŠ¨ä½œï¼‰ã€‚

éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œæˆ‘ä»¬é€æ¸**é™ä½ epsilon å€¼ï¼Œå› ä¸ºæ™ºèƒ½ä½“é€æ¸ä¸å†éœ€è¦æ¢ç´¢ï¼Œè€Œæ›´å¤šçš„éœ€è¦åˆ©ç”¨**ã€‚

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg" alt="Q-Learning" width="100%"/>


```python
def epsilon_greedy_policy(Qtable, state, epsilon):
  # åœ¨ 0 å’Œ 1 ä¹‹é—´éšæœºç”Ÿæˆä¸€ä¸ªæ•°å­—
  random_num =
  # å¦‚æœ random_num > epsilon --> åˆ©ç”¨
  if random_num > epsilon:
    # é‡‡å–ç»™å®šçŠ¶æ€ä¸‹æœ€é«˜å€¼çš„åŠ¨ä½œ
    # è¿™é‡Œå¯ä»¥ç”¨ np.argmax
    action =
  # å¦åˆ™ --> æ¢ç´¢
  else:
    action = # é‡‡å–ä¸€ä¸ªéšæœºåŠ¨ä½œ

  return action
```

#### ç­”æ¡ˆ

```python
def epsilon_greedy_policy(Qtable, state, epsilon):
    # åœ¨ 0 å’Œ 1 ä¹‹é—´éšæœºç”Ÿæˆä¸€ä¸ªæ•°å­—
    random_int = random.uniform(0, 1)
    # å¦‚æœ random_int > epsilon --> åˆ©ç”¨
    if random_int > epsilon:
        # é‡‡å–ç»™å®šçŠ¶æ€ä¸‹æœ€é«˜å€¼çš„åŠ¨ä½œ
        # è¿™é‡Œå¯ä»¥ç”¨ np.argmax
        action = greedy_policy(Qtable, state)
    # å¦åˆ™ --> æ¢ç´¢
    else:
        action = env.action_space.sample()

    return action
```

## å®šä¹‰è¶…å‚æ•° âš™ï¸

ä¸æ™ºèƒ½ä½“çš„æ¢ç´¢è¡ŒåŠ¨ç›¸å…³çš„è¶…å‚æ•°éå¸¸é‡è¦ï¼š

- æˆ‘ä»¬éœ€è¦ç¡®ä¿æ™ºèƒ½ä½“èƒ½å¤Ÿ**å……åˆ†åœ°æ¢ç´¢çŠ¶æ€ç©ºé—´**ä»¥å­¦ä¹ åˆ°ä¸€ä¸ªè¾ƒå¥½çš„å€¼è¿‘ä¼¼ã€‚ä¸ºäº†è¾¾åˆ°è¿™ä¸ªç›®æ ‡ï¼Œæˆ‘ä»¬éœ€è¦é€æ¸å‡å°epsilonã€‚
- ä½†æ˜¯å¦‚æœå°†epsilonå‡å°å¾—å¤ªå¿«ï¼ˆè¡°å‡ç‡è¿‡é«˜ï¼‰ï¼Œå°±ä¼š**å¢åŠ æ™ºèƒ½ä½“é™·å…¥å›°å¢ƒçš„é£é™©**ï¼Œå› ä¸ºå®ƒæ²¡æœ‰å……åˆ†æ¢ç´¢çŠ¶æ€ç©ºé—´ï¼Œæ‰€ä»¥æ— æ³•è§£å†³é—®é¢˜ã€‚

```python
# è®­ç»ƒå‚æ•°
n_training_episodes = 10000  # è®­ç»ƒå›åˆæ•°
learning_rate = 0.7  # å­¦ä¹ ç‡

# è¯„ä¼°å‚æ•°
n_eval_episodes = 100  # æµ‹è¯•å›åˆæ•°

# ç¯å¢ƒå‚æ•°
env_id = "FrozenLake-v1"  # ç¯å¢ƒåç§°
max_steps = 99  # æ¯æ¬¡å°è¯•çš„æœ€å¤§æ­¥æ•°
gamma = 0.95  # æŠ˜æ‰£ç‡
eval_seed = []  # ç¯å¢ƒçš„è¯„ä¼°ç§å­

# æ¢ç´¢å‚æ•°
max_epsilon = 1.0 # èµ·å§‹æ¢ç´¢æ¦‚ç‡
min_epsilon = 0.05 # æœ€å°æ¢ç´¢æ¦‚ç‡
decay_rate = 0.0005 # æ¢ç´¢æ¦‚ç‡çš„æŒ‡æ•°è¡°å‡é€Ÿç‡
```

## åˆ›å»ºè®­ç»ƒå‡½æ•°

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg" alt="Q-Learning" width="100%"/>

è®­ç»ƒçš„è¿‡ç¨‹ä»¥å¦‚ä¸‹æ–¹å¼è¿›è¡Œï¼š

```
åœ¨æ‰€æœ‰è®­ç»ƒæ¬¡æ•°çš„æ¯ä¸ªå¾ªç¯ä¸­ï¼š

å‡å°‘ epsilonï¼ˆå› ä¸ºæ™ºèƒ½ä½“é€æ¸ä¸å†éœ€è¦æ¢ç´¢ï¼‰
é‡ç½®ç¯å¢ƒ

    å¯¹äºæ¯ä¸ªæœ€å¤§å°è¯•æ­¥æ•°ï¼š
      ä½¿ç”¨ epsilon è´ªå¿ƒç­–ç•¥é€‰æ‹©åŠ¨ä½œ At
      æ‰§è¡ŒåŠ¨ä½œï¼ˆaï¼‰å¹¶è§‚å¯Ÿç»“æœçŠ¶æ€ï¼ˆs'ï¼‰å’Œå¥–åŠ±ï¼ˆrï¼‰
      ä½¿ç”¨è´å°”æ›¼æ–¹ç¨‹æ›´æ–° Q å€¼ Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]
      å¦‚æœå®Œæˆï¼Œç»“æŸæœ¬å›åˆ
      ä¸‹ä¸€ä¸ªçŠ¶æ€æ˜¯æ–°çŠ¶æ€
```

```python
def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):
  for episode in range(n_training_episodes):
    # å‡å° epsilonï¼ˆå› ä¸ºæ™ºèƒ½ä½“é€æ¸ä¸å†éœ€è¦æ¢ç´¢ï¼‰
    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)
    # é‡ç½®ç¯å¢ƒ
    state = env.reset()
    step = 0
    done = False

    # é‡å¤
    for step in range(max_steps):
      # ä½¿ç”¨ epsilon è´ªå¿ƒç­–ç•¥é€‰æ‹©åŠ¨ä½œ At
      action =

      # é‡‡å–åŠ¨ä½œ At å¹¶è§‚å¯Ÿ Rt+1 å’Œ St+1
      # é‡‡å–åŠ¨ä½œï¼ˆaï¼‰å¹¶è§‚å¯Ÿç»“æœçŠ¶æ€ï¼ˆs'ï¼‰å’Œå¥–åŠ±ï¼ˆrï¼‰
      new_state, reward, done, info =

      # æ›´æ–° Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]
      Qtable[state][action] =

      # å¦‚æœå®Œæˆï¼Œç»“æŸæœ¬æ¬¡å°è¯•
      if done:
        break

      # ä¸‹ä¸€ä¸ªçŠ¶æ€æ˜¯æ–°çŠ¶æ€
      state = new_state
  return Qtable
```

#### ç­”æ¡ˆ

```python
def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):
    for episode in tqdm(range(n_training_episodes)):
        # å‡å° epsilonï¼ˆå› ä¸ºæ™ºèƒ½ä½“é€æ¸ä¸å†éœ€è¦æ¢ç´¢ï¼‰
        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)
        # é‡ç½®ç¯å¢ƒ
        state = env.reset()
        step = 0
        done = False

        # é‡å¤
        for step in range(max_steps):
            # ä½¿ç”¨ epsilon è´ªå¿ƒç­–ç•¥é€‰æ‹©åŠ¨ä½œ At
            action = epsilon_greedy_policy(Qtable, state, epsilon)

            # é‡‡å–åŠ¨ä½œ At å¹¶è§‚å¯Ÿ Rt+1 å’Œ St+1
            # é‡‡å–åŠ¨ä½œï¼ˆaï¼‰å¹¶è§‚å¯Ÿç»“æœçŠ¶æ€ï¼ˆs'ï¼‰å’Œå¥–åŠ±ï¼ˆrï¼‰
            new_state, reward, done, info = env.step(action)

            # æ›´æ–° Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]
            Qtable[state][action] = Qtable[state][action] + learning_rate * (
                reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action]
            )

            # å¦‚æœå®Œæˆï¼Œç»“æŸæœ¬æ¬¡å°è¯•
            if done:
                break

            # ä¸‹ä¸€ä¸ªçŠ¶æ€æ˜¯æ–°çŠ¶æ€
            state = new_state
    return Qtable
```

## è®­ç»ƒ Q-learning æ™ºèƒ½ä½“ ğŸƒ

```python
Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)
```

## æŸ¥çœ‹Qè¡¨æ ¼ä¸­çš„å€¼ ğŸ‘€

```python
Qtable_frozenlake
```

## è¯„ä¼°å‡½æ•° ğŸ“

- å®šä¹‰æˆ‘ä»¬è¦ç”¨æ¥æµ‹è¯• Q-learning æ™ºèƒ½ä½“çš„è¯„ä¼°å‡½æ•°

```python
def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):
    """
    å¯¹æ™ºèƒ½ä½“è¿›è¡Œ n_eval_episodes è½®è¯„ä¼°ï¼Œè¿”å›å¹³å‡å¥–åŠ±å’Œå¥–åŠ±çš„æ ‡å‡†å·®ã€‚
    :param env: è¯„ä¼°ç¯å¢ƒ
    :param n_eval_episodes: è¯„ä¼°æ™ºèƒ½ä½“çš„è½®æ•°
    :param Q: Qè¡¨æ ¼
    :param seed: è¯„ä¼°ç§å­æ•°ç»„ï¼ˆç”¨äºtaxi-v3ï¼‰
	"""
    episode_rewards = []
    for episode in tqdm(range(n_eval_episodes)):
        if seed:
            state = env.reset(seed=seed[episode])
        else:
            state = env.reset()
        step = 0
        done = False
        total_rewards_ep = 0

        for step in range(max_steps):
            # åœ¨ç»™å®šçŠ¶æ€ä¸‹é€‰æ‹©å…·æœ‰æœ€å¤§é¢„æœŸæœªæ¥å¥–åŠ±çš„åŠ¨ä½œï¼ˆç´¢å¼•ï¼‰
            action = greedy_policy(Q, state)
            new_state, reward, done, info = env.step(action)
            total_rewards_ep += reward

            if done:
                break
            state = new_state
        episode_rewards.append(total_rewards_ep)
    mean_reward = np.mean(episode_rewards)
    std_reward = np.std(episode_rewards)

    return mean_reward, std_reward
```

## è¯„ä¼°Q-learningæ™ºèƒ½ä½“ ğŸ“ˆ

- é€šå¸¸åº”è¯¥å¾—åˆ°å¹³å‡å¥–åŠ±ä¸º1.0
- å› ä¸ºçŠ¶æ€ç©ºé—´éå¸¸å°ï¼ˆ16ï¼‰ï¼Œæ‰€ä»¥è¯¥**ç¯å¢ƒç›¸å¯¹ç®€å•**ï¼Œä½ å¯ä»¥å°è¯•ç”¨[æœ‰æ»‘åŠ¨ç‰ˆæœ¬](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/)æ›¿æ¢å®ƒï¼Œè¿™ä¼šå¼•å…¥éšæœºæ€§ï¼Œä½¿ç¯å¢ƒæ›´åŠ å¤æ‚ã€‚

```python
# è¯„ä¼°æ™ºèƒ½ä½“
mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)
print(f"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}")
```

## å°†æˆ‘ä»¬çš„è®­ç»ƒæ¨¡å‹å‘å¸ƒåˆ°Hub ğŸ”¥

å¦‚æœåœ¨è®­ç»ƒåçœ‹åˆ°äº†å¥½çš„ç»“æœï¼Œ**æˆ‘ä»¬å¯ä»¥ç”¨ä¸€è¡Œä»£ç å°†è®­ç»ƒæ¨¡å‹å‘å¸ƒåˆ°Hugging Face HubğŸ¤—**ã€‚

è¿™é‡Œæœ‰ä¸€ä¸ªæ¨¡å‹æ¦‚è¿°å¡çš„ä¾‹å­ï¼š

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/modelcard.png" alt="Model card" width="100%"/>

åœ¨åº•å±‚ï¼ŒHubä½¿ç”¨åŸºäºgitçš„å­˜å‚¨åº“ï¼ˆå¦‚æœä½ ä¸çŸ¥é“gitæ˜¯ä»€ä¹ˆï¼Œä¸ç”¨æ‹…å¿ƒï¼‰ï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥åœ¨å®éªŒå’Œæ”¹è¿›ä½ çš„æ™ºèƒ½ä½“æ—¶ï¼Œç”¨æ–°ç‰ˆæœ¬æ›´æ–°æ¨¡å‹ã€‚

#### è¯·å‹¿ä¿®æ”¹è¿™æ®µä»£ç 

```python
from huggingface_hub import HfApi, snapshot_download
from huggingface_hub.repocard import metadata_eval_result, metadata_save

from pathlib import Path
import datetime
import json
```

```python
def record_video(env, Qtable, out_directory, fps=1):
    """
    ç”Ÿæˆæ™ºèƒ½ä½“è¡¨ç°çš„å›æ”¾è§†é¢‘
    :param env
    :param Qtable: æˆ‘ä»¬æ™ºèƒ½ä½“çš„Qè¡¨æ ¼
    :param out_directory
    :param fps: æ¯ç§’å¸§æ•°ï¼ˆå¯¹äºtaxi-v3å’Œfrozenlake-v1ï¼Œæˆ‘ä»¬ä½¿ç”¨1ï¼‰
    """
    images = []
    done = False
    state = env.reset(seed=random.randint(0, 500))
    img = env.render(mode="rgb_array")
    images.append(img)
    while not done:
        # åœ¨ç»™å®šçŠ¶æ€ä¸‹é€‰æ‹©å…·æœ‰æœ€å¤§é¢„æœŸæœªæ¥å¥–åŠ±çš„åŠ¨ä½œï¼ˆç´¢å¼•ï¼‰
        action = np.argmax(Qtable[state][:])
        state, reward, done, info = env.step(action)  # ç›´æ¥å°†next_state = stateç”¨äºè®°å½•é€»è¾‘
        img = env.render(mode="rgb_array")
        images.append(img)
    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)
```

```python
def push_to_hub(repo_id, model, env, video_fps=1, local_repo_path="hub"):
    """
    è¯„ä¼°ã€ç”Ÿæˆè§†é¢‘å¹¶å°†æ¨¡å‹ä¸Šä¼ åˆ°Hugging Face Hubã€‚
    è¯¥æ–¹æ³•å®Œæˆæ•´ä¸ªæµç¨‹ï¼š
    - å®ƒè¯„ä¼°æ¨¡å‹
    - å®ƒç”Ÿæˆæ¨¡å‹æ¦‚è¿°å¡
    - å®ƒç”Ÿæˆæ™ºèƒ½ä½“çš„å›æ”¾è§†é¢‘
    - å®ƒå°†æ‰€æœ‰å†…å®¹æ¨é€åˆ°Hub

    :param repo_id: Hugging Face Hubä¸­çš„æ¨¡å‹å­˜å‚¨åº“ID
    :param env
    :param video_fps: ä»¥å¤šå°‘å¸§æ¯ç§’å½•åˆ¶æˆ‘ä»¬çš„è§†é¢‘å›æ”¾
    (å¯¹äºtaxi-v3å’Œfrozenlake-v1ï¼Œæˆ‘ä»¬ä½¿ç”¨1)
    :param local_repo_path: æœ¬åœ°å­˜å‚¨åº“çš„ä½ç½®
    """
    _, repo_name = repo_id.split("/")

    eval_env = env
    api = HfApi()

    # ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºä»“åº“
    repo_url = api.create_repo(
        repo_id=repo_id,
        exist_ok=True,
    )

    # ç¬¬äºŒæ­¥ï¼šä¸‹è½½æ–‡ä»¶
    repo_local_path = Path(snapshot_download(repo_id=repo_id))

    #ç¬¬ä¸‰æ­¥ï¼šä¿å­˜æ¨¡å‹
    if env.spec.kwargs.get("map_name"):
        model["map_name"] = env.spec.kwargs.get("map_name")
        if env.spec.kwargs.get("is_slippery", "") == False:
            model["slippery"] = False

    # å°†æ¨¡å‹å­˜å‚¨ä¸ºPickleæ–‡ä»¶
    with open((repo_local_path) / "q-learning.pkl", "wb") as f:
        pickle.dump(model, f)

    # ç¬¬å››æ­¥ï¼šè¯„ä¼°æ¨¡å‹å¹¶æ„å»ºåŒ…å«è¯„ä¼°æŒ‡æ ‡çš„JSONæ–‡ä»¶
    mean_reward, std_reward = evaluate_agent(
        eval_env, model["max_steps"], model["n_eval_episodes"], model["qtable"], model["eval_seed"]
    )

    evaluate_data = {
        "env_id": model["env_id"],
        "mean_reward": mean_reward,
        "n_eval_episodes": model["n_eval_episodes"],
        "eval_datetime": datetime.datetime.now().isoformat(),
    }

    # ç¼–å†™ä¸€ä¸ªåä¸º "results.json" çš„JSONæ–‡ä»¶ï¼Œå…¶ä¸­å°†åŒ…å«è¯„ä¼°ç»“æœ
    with open(repo_local_path / "results.json", "w") as outfile:
        json.dump(evaluate_data, outfile)

    # ç¬¬äº”æ­¥ï¼šåˆ›å»ºæ¨¡å‹æ¦‚è¿°æ¦‚è¿°å¡
    env_name = model["env_id"]
    if env.spec.kwargs.get("map_name"):
        env_name += "-" + env.spec.kwargs.get("map_name")

    if env.spec.kwargs.get("is_slippery", "") == False:
        env_name += "-" + "no_slippery"

    metadata = {}
    metadata["tags"] = [env_name, "q-learning", "reinforcement-learning", "custom-implementation"]

    # æ·»åŠ æŒ‡æ ‡
    eval = metadata_eval_result(
        model_pretty_name=repo_name,
        task_pretty_name="reinforcement-learning",
        task_id="reinforcement-learning",
        metrics_pretty_name="mean_reward",
        metrics_id="mean_reward",
        metrics_value=f"{mean_reward:.2f} +/- {std_reward:.2f}",
        dataset_pretty_name=env_name,
        dataset_id=env_name,
    )

    # åˆå¹¶ä¸¤ä¸ªå­—å…¸
    metadata = {**metadata, **eval}

    model_card = f"""
    # **Q-learning** æ™ºèƒ½ä½“ç© **{env_id}**
    è¿™æ˜¯ä¸€ä¸ªå—è¿‡è®­ç»ƒçš„**Q-learning**æ™ºèƒ½ä½“ç© **{env_id}** çš„æ¨¡å‹ã€‚

    ## ç”¨æ³•

    ```python

    model = load_from_hub(repo_id="{repo_id}", filename="q-learning.pkl")

    # ä¸è¦å¿˜è®°æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ é¢å¤–çš„å±æ€§ (is_slippery=Falseç­‰)
    env = gym.make(model["env_id"])
```

    """
    
    evaluate_agent(env, model["max_steps"], model["n_eval_episodes"], model["qtable"], model["eval_seed"])
    
    readme_path = repo_local_path / "README.md"
    readme = ""
    print(readme_path.exists())
    if readme_path.exists():
        with readme_path.open("r", encoding="utf8") as f:
            readme = f.read()
    else:
        readme = model_card
    
    with readme_path.open("w", encoding="utf-8") as f:
        f.write(readme)
    
    # å°†æŒ‡æ ‡ä¿å­˜åˆ°Readmeå…ƒæ•°æ®
    metadata_save(readme_path, metadata)
    
    # ç¬¬å…­æ­¥ï¼šå½•åˆ¶è§†é¢‘
    video_path = repo_local_path / "replay.mp4"
    record_video(env, model["qtable"], video_path, video_fps)
    
    # ç¬¬ä¸ƒæ­¥. å°†æ‰€æœ‰å†…å®¹æ¨é€åˆ°Hub
    api.upload_folder(
        repo_id=repo_id,
        folder_path=repo_local_path,
        path_in_repo=".",
    )
    
    print("Your model is pushed to the Hub. You can view your model here: ", repo_url)

```
### .

é€šè¿‡ä½¿ç”¨ push_to_hubï¼Œä½ å¯ä»¥è¯„ä¼°ã€å½•åˆ¶å›æ”¾ã€ç”Ÿæˆæ™ºèƒ½ä½“çš„æ¨¡å‹å¡ç‰‡å¹¶å°†å…¶æ¨é€åˆ°Hubã€‚

è¿™æ ·ï¼š

- å¯ä»¥å±•ç¤ºä½ çš„ä½œå“ ğŸ”¥
- å¯ä»¥æŸ¥çœ‹æ™ºèƒ½ä½“çš„æ¸¸æˆè¿‡ç¨‹ ğŸ‘€
- å¯ä»¥ä¸ç¤¾åŒºåˆ†äº«å…¶ä»–äººå¯ä»¥ä½¿ç”¨çš„æ™ºèƒ½ä½“ ğŸ’¾
- å¯ä»¥è®¿é—®æ’è¡Œæ¦œğŸ†ï¼ŒæŸ¥çœ‹ä½ çš„æ™ºèƒ½ä½“ä¸åŒå­¦ç›¸æ¯”è¡¨ç°å¦‚ä½• ğŸ‘‰ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard


è¦ä¸ç¤¾åŒºå…±äº«ä½ çš„æ¨¡å‹ï¼Œè¿˜éœ€éµå¾ªä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

1ï¸âƒ£ï¼ˆå¦‚æœè¿˜æ²¡æœ‰å®Œæˆï¼‰åˆ›å»ºHFå¸æˆ· â¡ https://huggingface.co/join

2ï¸âƒ£ ç™»å½•åï¼Œä½ éœ€è¦ä»Hugging Faceç½‘ç«™å­˜å‚¨ä½ çš„è®¤è¯ä»¤ç‰Œã€‚

åˆ›å»ºä¸€ä¸ªæ–°ä»¤ç‰Œï¼ˆhttps://huggingface.co/settings/tokensï¼‰**å…·æœ‰å†™æƒé™**

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg" alt="Create HF Token">


```python
from huggingface_hub import notebook_login

notebook_login()
```

å¦‚æœä½ ä¸æƒ³ä½¿ç”¨Google Colabæˆ–Jupyter Notebookï¼Œå¯ä»¥ä½¿ç”¨æ­¤å‘½ä»¤ä»£æ›¿ï¼š`huggingface-cli login`ï¼ˆæˆ–`login`ï¼‰

3ï¸âƒ£ ç°åœ¨æˆ‘ä»¬å‡†å¤‡ä½¿ç”¨`push_to_hub()`å‡½æ•°å°†è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“æ¨é€åˆ°ğŸ¤—HubğŸ”¥

- é¦–å…ˆåˆ›å»º**åŒ…å«è¶…å‚æ•°å’ŒQ_tableçš„æ¨¡å‹å­—å…¸**ã€‚

```python
model = {
    "env_id": env_id,
    "max_steps": max_steps,
    "n_training_episodes": n_training_episodes,
    "n_eval_episodes": n_eval_episodes,
    "eval_seed": eval_seed,
    "learning_rate": learning_rate,
    "gamma": gamma,
    "max_epsilon": max_epsilon,
    "min_epsilon": min_epsilon,
    "decay_rate": decay_rate,
    "qtable": Qtable_frozenlake,
}
```

å¡«å†™`push_to_hub`å‡½æ•°ï¼š

- `repo_id`ï¼šå°†åˆ›å»º/æ›´æ–°çš„Hugging Face Hubå­˜å‚¨åº“çš„åç§°` (repo_id = {username}/{repo_name})` 

  ğŸ’¡ ä¸€ä¸ªå¥½çš„`repo_id`æ˜¯`{username}/q-{env_id}`

- `model`ï¼šæ¨¡å‹å­—å…¸ï¼ŒåŒ…å«è¶…å‚æ•°å’ŒQtable

- `env`ï¼šç¯å¢ƒ

- `commit_message`ï¼šæäº¤ä¿¡æ¯

```python
model
```

```python
username = ""  # å¡«å†™ä½ çš„ç”¨æˆ·å
repo_name = "q-FrozenLake-v1-4x4-noSlippery"
push_to_hub(repo_id=f"{username}/{repo_name}", model=model, env=env)
```

æ­å–œğŸ¥³ä½ åˆšåˆšä»é›¶å¼€å§‹å®ç°ã€è®­ç»ƒå¹¶ä¸Šä¼ äº†ä½ çš„ç¬¬ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ã€‚ 

FrozenLake-v1 æ— æ»‘åŠ¨ç‰ˆ æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„ç¯å¢ƒï¼Œè®©æˆ‘ä»¬å°è¯•ä¸€ä¸ªæ›´éš¾çš„ç¯å¢ƒğŸ”¥ã€‚

# ç¬¬äºŒéƒ¨åˆ†ï¼šTaxi-v3 ç¯å¢ƒğŸš–

## åˆ›å»ºå¹¶ç†è§£ [Taxi-v3 ç¯å¢ƒğŸš•](https://www.gymlibrary.dev/environments/toy_text/taxi/)

------

ğŸ’¡ å¼€å§‹ä½¿ç”¨ç¯å¢ƒæ—¶ï¼ŒæŸ¥çœ‹å…¶æ–‡æ¡£æ˜¯ä¸ªå¥½ä¹ æƒ¯

ğŸ‘‰ https://www.gymlibrary.dev/environments/toy_text/taxi/

---

åœ¨`Taxi-v3`ğŸš•ä¸­ï¼Œç½‘æ ¼ç¯å¢ƒä¸­æœ‰å››ä¸ªæŒ‡å®šä½ç½®ï¼Œåˆ†åˆ«ä¸ºR(ed)ã€G(reen)ã€Y(ellow)å’ŒB(lue)ã€‚

å½“å›åˆå¼€å§‹æ—¶ï¼Œ**å‡ºç§Ÿè½¦éšæœºå‡ºç°åœ¨ä¸€ä¸ªæ–¹æ ¼ä¸­**ï¼Œä¹˜å®¢ä½äºä¸€ä¸ªéšæœºä½ç½®ã€‚å‡ºç§Ÿè½¦é©¶å‘ä¹˜å®¢æ‰€åœ¨ä½ç½®ï¼Œ**æ¥è½½ä¹˜å®¢**ï¼Œé©¶å‘ä¹˜å®¢çš„ç›®çš„åœ°ï¼ˆå¦å¤–å››ä¸ªæŒ‡å®šä½ç½®ä¸­çš„ä¸€ä¸ªï¼‰ï¼Œç„¶å**æ”¾ä¸‹ä¹˜å®¢**ã€‚ä¸€æ—¦ä¹˜å®¢è¢«æ”¾ä¸‹ï¼Œå›åˆç»“æŸã€‚


<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi.png" alt="Taxi">


```python
env = gym.make("Taxi-v3")
```

There are **500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger** (including the case when the passenger is in the taxi), and **4 destination locations.**

è¯¥ç¯å¢ƒæœ‰**500ä¸ªç¦»æ•£çŠ¶æ€ï¼Œå› ä¸ºæœ‰25ä¸ªå‡ºç§Ÿè½¦ä½ç½®ï¼Œ5ä¸ªå¯èƒ½çš„ä¹˜å®¢ä½ç½®**ï¼ˆåŒ…æ‹¬ä¹˜å®¢åœ¨å‡ºç§Ÿè½¦å†…çš„æƒ…å†µï¼‰ï¼Œä»¥åŠ**4ä¸ªç›®çš„åœ°ä½ç½®ã€‚**


```python
state_space = env.observation_space.n
print("There are ", state_space, " possible states")
```

```python
action_space = env.action_space.n
print("There are ", action_space, " possible actions")
```

åŠ¨ä½œç©ºé—´ï¼ˆæ™ºèƒ½ä½“å¯ä»¥é‡‡å–çš„å¯èƒ½åŠ¨ä½œé›†åˆï¼‰æ˜¯ç¦»æ•£çš„ï¼Œæœ‰**6ä¸ªå¯ç”¨åŠ¨ä½œğŸ®**ï¼š

- 0ï¼šå‘å—ç§»åŠ¨
- 1ï¼šå‘åŒ—ç§»åŠ¨
- 2ï¼šå‘ä¸œç§»åŠ¨
- 3ï¼šå‘è¥¿ç§»åŠ¨
- 4ï¼šæ¥è½½ä¹˜å®¢
- 5ï¼šæ”¾ä¸‹ä¹˜å®¢

å¥–åŠ±å‡½æ•°ğŸ’°ï¼š

- æ¯æ­¥-1ï¼Œé™¤éè§¦å‘å…¶ä»–å¥–åŠ±ã€‚
- é€è¾¾ä¹˜å®¢+20ã€‚
- éæ³•æ‰§è¡Œâ€œæ¥è½½â€å’Œâ€œæ”¾ä¸‹â€åŠ¨ä½œ-10ã€‚

```python
# åˆ›å»ºå…·æœ‰state_sizeè¡Œå’Œaction_sizeåˆ—ï¼ˆ500x6ï¼‰çš„Qè¡¨æ ¼
Qtable_taxi = initialize_q_table(state_space, action_space)
print(Qtable_taxi)
print("Q-table shape: ", Qtable_taxi.shape)
```

## å®šä¹‰è¶…å‚æ•° âš™ï¸

âš  è¯·å‹¿ä¿®æ”¹EVAL_SEEDï¼ševal_seedæ•°ç»„**å…è®¸æˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„å‡ºç§Ÿè½¦èµ·å§‹ä½ç½®è¯„ä¼°æ¯ä¸ªåŒå­¦çš„æ™ºèƒ½ä½“**

```python
# è®­ç»ƒè¶…å‚æ•°
n_training_episodes = 25000  # è®­ç»ƒå›åˆæ•°
learning_rate = 0.7  # å­¦ä¹ ç‡

# è¯„ä¼°å‚æ•°
n_eval_episodes = 100  # è¯„ä¼°å›åˆæ•°

# è¯·å‹¿ä¿®æ”¹EVAL_SEED
eval_seed = [
    16,
    54,
    165,
    177,
    191,
    191,
    120,
    80,
    149,
    178,
    48,
    38,
    6,
    125,
    174,
    73,
    50,
    172,
    100,
    148,
    146,
    6,
    25,
    40,
    68,
    148,
    49,
    167,
    9,
    97,
    164,
    176,
    61,
    7,
    54,
    55,
    161,
    131,
    184,
    51,
    170,
    12,
    120,
    113,
    95,
    126,
    51,
    98,
    36,
    135,
    54,
    82,
    45,
    95,
    89,
    59,
    95,
    124,
    9,
    113,
    58,
    85,
    51,
    134,
    121,
    169,
    105,
    21,
    30,
    11,
    50,
    65,
    12,
    43,
    82,
    145,
    152,
    97,
    106,
    55,
    31,
    85,
    38,
    112,
    102,
    168,
    123,
    97,
    21,
    83,
    158,
    26,
    80,
    63,
    5,
    81,
    32,
    11,
    28,
    148,
]  # è¯„ä¼°ç§å­ï¼Œè¿™ç¡®ä¿äº†æ‰€æœ‰åŒå­¦çš„æ™ºèƒ½ä½“éƒ½åœ¨ç›¸åŒçš„å‡ºç§Ÿè½¦èµ·å§‹ä½ç½®ä¸Šè¿›è¡Œè®­ç»ƒ
# æ¯ä¸ªç§å­éƒ½æœ‰ä¸€ä¸ªç‰¹å®šçš„èµ·å§‹çŠ¶æ€

# ç¯å¢ƒå‚æ•°
env_id = "Taxi-v3" # ç¯å¢ƒåç§°
max_steps = 99 # æ¯ä¸ªå›åˆçš„æœ€å¤§æ­¥æ•°
gamma = 0.95 # æŠ˜æ‰£ç‡

# æ¢ç´¢å‚æ•°
max_epsilon = 1.0 # èµ·å§‹æ¢ç´¢æ¦‚ç‡
min_epsilon = 0.05 # æœ€å°æ¢ç´¢æ¦‚ç‡
decay_rate = 0.005 # æ¢ç´¢æ¦‚ç‡çš„æŒ‡æ•°è¡°å‡ç‡
```

## è®­ç»ƒ Q-learning æ™ºèƒ½ä½“ ğŸƒ

```python
Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)
Qtable_taxi
```

## åˆ›å»ºä¸€ä¸ªæ¨¡å‹å­—å…¸ ğŸ’¾ å¹¶å°†è®­ç»ƒå¥½çš„æ¨¡å‹å‘å¸ƒåˆ°Hub ğŸ”¥

- æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ¨¡å‹å­—å…¸ï¼Œå…¶ä¸­å°†åŒ…å«æ‰€æœ‰å¯å¤ç°çš„è®­ç»ƒè¶…å‚æ•°å’ŒQè¡¨æ ¼ã€‚


```python
model = {
    "env_id": env_id,
    "max_steps": max_steps,
    "n_training_episodes": n_training_episodes,
    "n_eval_episodes": n_eval_episodes,
    "eval_seed": eval_seed,
    "learning_rate": learning_rate,
    "gamma": gamma,
    "max_epsilon": max_epsilon,
    "min_epsilon": min_epsilon,
    "decay_rate": decay_rate,
    "qtable": Qtable_taxi,
}
```

```python
username = ""  # å¡«å†™ä½ çš„ç”¨æˆ·å
repo_name = ""
push_to_hub(repo_id=f"{username}/{repo_name}", model=model, env=env)
```

ç°åœ¨å·²ç»å‘å¸ƒåˆ°Hubä¸Šï¼Œä½ å¯ä»¥æŸ¥çœ‹æ’è¡Œæ¦œ ğŸ† ğŸ‘‰ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard ä¸åŒå­¦ä»¬æ¯”è¾ƒTaxi-v3çš„ç»“æœã€‚

âš  è‹¥è¦æŸ¥çœ‹ä½ çš„æ’åï¼Œä½ éœ€è¦åœ¨æ’è¡Œæ¦œé¡µé¢åº•éƒ¨**ç‚¹å‡»åˆ·æ–°** âš 

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi-leaderboard.png" alt="Taxi Leaderboard">

# ç¬¬ä¸‰éƒ¨åˆ†ï¼šä»HubåŠ è½½æ¨¡å‹ ğŸ”½

é€šè¿‡Hugging Face Hub ğŸ¤—ä½ å¯ä»¥è½»æ¾åœ°åŠ è½½ç¤¾åŒºçš„å¼ºå¤§æ¨¡å‹ã€‚

ä»HubåŠ è½½ä¿å­˜çš„æ¨¡å‹éå¸¸ç®€å•ï¼š

1. å‰å¾€ https://huggingface.co/models?other=q-learning æŸ¥çœ‹æ‰€æœ‰q-learningå·²ä¿å­˜æ¨¡å‹çš„åˆ—è¡¨ã€‚
2. é€‰æ‹©ä¸€ä¸ªå¹¶å¤åˆ¶å…¶repo_id

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/copy-id.png" alt="Copy id">

3. ç„¶åæˆ‘ä»¬åªéœ€ä½¿ç”¨ `load_from_hub`ï¼Œå‚æ•°ä¸ºï¼š

- repo_id

- filenameï¼šå­˜å‚¨åœ¨repoä¸­çš„å·²ä¿å­˜æ¨¡å‹æ–‡ä»¶åã€‚ ä»Hugging Face Hubä¸‹è½½æ¨¡å‹ã€‚ 

  :param repo_id: æ¥è‡ªHugging Face Hubçš„æ¨¡å‹å­˜å‚¨åº“çš„ID 

  :param filename: å­˜å‚¨åº“ä¸­çš„æ¨¡å‹zipæ–‡ä»¶çš„åç§°

#### è¯·å‹¿ä¿®æ”¹è¯¥éƒ¨åˆ†ä»£ç 

```python
from urllib.error import HTTPError

from huggingface_hub import hf_hub_download


def load_from_hub(repo_id: str, filename: str) -> str:
    """
    ä»Hugging Face Hubä¸‹è½½æ¨¡å‹
    :param repo_id: æ¥è‡ªHugging Face Hubçš„æ¨¡å‹å­˜å‚¨åº“çš„ID
    :param filename: å­˜å‚¨åº“ä¸­çš„æ¨¡å‹zipæ–‡ä»¶çš„åç§°
    """
    # ä»Hubä¸­è·å–æ¨¡å‹ï¼Œä¸‹è½½å¹¶å°†æ¨¡å‹ç¼“å­˜åˆ°æœ¬åœ°ç£ç›˜ä¸­
    pickle_model = hf_hub_download(repo_id=repo_id, filename=filename)

    with open(pickle_model, "rb") as f:
        downloaded_model_file = pickle.load(f)

    return downloaded_model_file
```

### .

```python
model = load_from_hub(repo_id="ThomasSimonini/q-Taxi-v3", filename="q-learning.pkl")  # å°è¯•ä½¿ç”¨å¦ä¸€ä¸ªæ¨¡å‹

print(model)
env = gym.make(model["env_id"])

evaluate_agent(env, model["max_steps"], model["n_eval_episodes"], model["qtable"], model["eval_seed"])
```

```python
model = load_from_hub(
    repo_id="ThomasSimonini/q-FrozenLake-v1-no-slippery", filename="q-learning.pkl"
)  # å°è¯•ä½¿ç”¨å¦ä¸€ä¸ªæ¨¡å‹

env = gym.make(model["env_id"], is_slippery=False)

evaluate_agent(env, model["max_steps"], model["n_eval_episodes"], model["qtable"], model["eval_seed"])
```

## é¢å¤–æŒ‘æˆ˜ ğŸ†

æœ€å¥½çš„å­¦ä¹ æ–¹æ³•å°±æ˜¯**è‡ªå·±å»å°è¯•**ï¼ç›®å‰çš„æ™ºèƒ½ä½“è¡¨ç°å¹¶ä¸ç†æƒ³ï¼Œä½ å¯ä»¥å°è¯•è®©å®ƒè®­ç»ƒæ›´å¤šæ­¥ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨1,000,000æ­¥çš„è®­ç»ƒä¸­ï¼Œæ™ºèƒ½ä½“èƒ½å–å¾—å¾ˆå¥½çš„æˆæœï¼

åœ¨[æ’è¡Œæ¦œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)ä¸Šï¼Œä½ å¯ä»¥çœ‹åˆ°ä½ çš„æ™ºèƒ½ä½“æ’åï¼Œä½ èƒ½ç™»ä¸Šæ¦œé¦–å—ï¼Ÿ

ä»¥ä¸‹æ˜¯ä¸€äº›å»ºè®®ï¼š

- è®­ç»ƒæ›´å¤šæ­¥éª¤
- è§‚å¯Ÿå…¶ä»–åŒå­¦çš„æ¨¡å‹ï¼Œå°è¯•ä¸åŒçš„è¶…å‚æ•°
- åœ¨Hubä¸Š**å‘å¸ƒä½ æ–°è®­ç»ƒçš„æ¨¡å‹** ğŸ”¥

å¦‚æœè§‰å¾—åœ¨å†°é¢ä¸Šè¡Œèµ°å’Œé©¾é©¶å‡ºç§Ÿè½¦å¤ªæ— èŠäº†ï¼Œå¯ä»¥å°è¯•**æ›´æ¢ç¯å¢ƒ**ï¼Œå¦‚ä½¿ç”¨FrozenLake-v1æ»‘åŠ¨ç‰ˆï¼Œé€šè¿‡æŸ¥é˜…[gymæ–‡æ¡£](https://www.gymlibrary.dev/)äº†è§£å®ƒä»¬æ˜¯å¦‚ä½•ä½¿ç”¨çš„ï¼Œå¹¶äº«å—å…¶å¸¦æ¥çš„æ•ˆæœå§ğŸ‰ã€‚

_____________________________________________________________________

æ­å–œğŸ¥³ï¼Œä½ åˆšåˆšå®ç°ã€è®­ç»ƒå¹¶ä¸Šä¼ äº†ä½ çš„ç¬¬ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ã€‚

ç†è§£Q-learningå¯¹äºé¢†ä¼šåŸºäºä»·å€¼çš„æ–¹æ³•éå¸¸é‡è¦ã€‚

åœ¨æ¥ä¸‹æ¥çš„å•å…ƒä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ æ·±åº¦Qå­¦ä¹ ã€‚æˆ‘ä»¬ä¼šå‘ç°ï¼Œåˆ›å»ºå’Œæ›´æ–°Qè¡¨æ ¼çš„ç¡®æ˜¯ä¸ªå¥½ç­–ç•¥ï¼Œ**ä½†è¿™ç§æ–¹æ³•å¹¶ä¸å…·å¤‡æ‰©å±•æ€§**ã€‚

ä¾‹å¦‚ï¼Œå‡è®¾ä½ åˆ›å»ºäº†ä¸€ä¸ªèƒ½ç©ã€Šæ¯ç­æˆ˜å£«ã€‹çš„æ™ºèƒ½ä½“ã€‚

<img src="https://vizdoom.cs.put.edu.pl/user/pages/01.tutorial/basic.png" alt="Doom"/>

æ¯ç­æˆ˜å£«æ˜¯ä¸€ä¸ªåºå¤§çš„ç¯å¢ƒï¼Œæ‹¥æœ‰å¤§é‡çš„çŠ¶æ€ç©ºé—´ï¼ˆæ•°ç™¾ä¸‡ä¸ªä¸åŒçŠ¶æ€ï¼‰ã€‚ä¸ºè¿™æ ·çš„ç¯å¢ƒåˆ›å»ºå’Œæ›´æ–°Qè¡¨æ ¼å¹¶ä¸é«˜æ•ˆã€‚

æ­£å› å¦‚æ­¤ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€å•å…ƒå­¦ä¹ æ·±åº¦Qå­¦ä¹ ã€‚è¿™æ˜¯ä¸€ç§ç®—æ³•ï¼Œ**å®ƒåˆ©ç”¨ç¥ç»ç½‘ç»œåœ¨ç»™å®šçŠ¶æ€æ—¶è¿‘ä¼¼è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„ä¸åŒQå€¼**ã€‚

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif" alt="Environments"/>

æœŸå¾…åœ¨ç¬¬ä¸‰å•å…ƒä¸ä½ ç›¸è§ï¼ğŸ”¥

## ç»§ç»­å­¦ä¹ ï¼Œä¿æŒå“è¶Š ğŸ¤—